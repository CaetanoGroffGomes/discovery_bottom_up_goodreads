{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8071d5f0",
   "metadata": {},
   "source": [
    "# EDA ‚Äî `book1-100.csv`\n",
    "\n",
    "An√°lise explorat√≥ria dos dados de `book1-100.csv`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff5b25fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:03.908590Z",
     "start_time": "2025-10-27T18:45:03.906092Z"
    }
   },
   "source": [
    "# CONFIG\n",
    "import pandas as pd\n",
    "\n",
    "# CAMINHO DO ARQUIVO\n",
    "path = r\"C:\\Users\\USER\\PycharmProjects\\JupyterProject\\data\\book1-100k.csv\""
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "e3bf8834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:04.589450Z",
     "start_time": "2025-10-27T18:45:03.931292Z"
    }
   },
   "source": [
    "# LEITURA ROBUSTA\n",
    "try:\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Arquivo n√£o encontrado em {path}. Ajuste a vari√°vel 'path' acima.\") from e\n",
    "\n",
    "# PADRONIZA√á√ÉO NOMES COLUNAS\n",
    "df.columns = [c.strip().replace(\"  \", \" \").replace(\" \", \"_\").lower() for c in df.columns]\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head(10)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (58292, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id                                               name ratingdist1  \\\n",
       "0   1  Harry Potter and the Half-Blood Prince (Harry ...      1:9896   \n",
       "1   2  Harry Potter and the Order of the Phoenix (Har...     1:12455   \n",
       "2   3  Harry Potter and the Sorcerer's Stone (Harry P...    1:108202   \n",
       "3   4  Harry Potter and the Chamber of Secrets (Harry...     1:11896   \n",
       "4   5  Harry Potter and the Prisoner of Azkaban (Harr...     1:10128   \n",
       "5   6  Harry Potter and the Goblet of Fire (Harry Pot...      1:9419   \n",
       "6   8  Harry Potter Boxed Set, Books 1-5 (Harry Potte...       1:402   \n",
       "7   9  Unauthorized Harry Potter Book Seven News: \"Ha...         1:0   \n",
       "8  10       Harry Potter Collection (Harry Potter, #1-6)       1:257   \n",
       "9  12  The Ultimate Hitchhiker's Guide: Five Complete...      1:3443   \n",
       "\n",
       "   pagesnumber ratingdist4 ratingdisttotal  publishmonth  publishday  \\\n",
       "0          652    4:556485   total:2298124            16           9   \n",
       "1          870    4:604283   total:2358637             1           9   \n",
       "2          309   4:1513191   total:6587388             1          11   \n",
       "3          352    4:706082   total:2560657             1          11   \n",
       "4          435    4:630534   total:2610317             1           5   \n",
       "5          734    4:606800   total:2431085            28           9   \n",
       "6         2690      4:4650     total:43968            13           9   \n",
       "7          152         4:7        total:28            26           4   \n",
       "8         3342      4:4358     total:30313            12           9   \n",
       "9          815     4:75683    total:274268             1          11   \n",
       "\n",
       "         publisher  countsofreview  publishyear language  \\\n",
       "0  Scholastic Inc.           28062         2006      eng   \n",
       "1  Scholastic Inc.           29770         2004      eng   \n",
       "2   Scholastic Inc           75911         2003      eng   \n",
       "3       Scholastic             244         2003      eng   \n",
       "4  Scholastic Inc.           37093         2004      eng   \n",
       "5       Scholastic           31978         2002      eng   \n",
       "6       Scholastic             166         2004      eng   \n",
       "7     Nimble Books               1         2005    en-US   \n",
       "8       Scholastic             809         2005      eng   \n",
       "9   Gramercy Books             255         2005      eng   \n",
       "\n",
       "                  authors  rating ratingdist2 ratingdist5        isbn  \\\n",
       "0            J.K. Rowling    4.57     2:25317   5:1546466         NaN   \n",
       "1            J.K. Rowling    4.50     2:37005   5:1493113  0439358078   \n",
       "2            J.K. Rowling    4.47    2:130310   5:4268227         NaN   \n",
       "3            J.K. Rowling    4.42     2:49353   5:1504505  0439554896   \n",
       "4            J.K. Rowling    4.57     2:24849   5:1749958  043965548X   \n",
       "5            J.K. Rowling    4.56     2:24282   5:1612165         NaN   \n",
       "6            J.K. Rowling    4.78       2:283     5:37432  0439682584   \n",
       "7  W. Frederick Zimmerman    3.79         2:5        5:10  0976540606   \n",
       "8            J.K. Rowling    4.73       2:218     5:24406  0439827604   \n",
       "9           Douglas Adams    4.37      2:7613    5:157499  0517226952   \n",
       "\n",
       "  ratingdist3  \n",
       "0    3:159960  \n",
       "1    3:211781  \n",
       "2    3:567458  \n",
       "3    3:288821  \n",
       "4    3:194848  \n",
       "5    3:178419  \n",
       "6      3:1201  \n",
       "7         3:6  \n",
       "8      3:1074  \n",
       "9     3:30030  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>ratingdist1</th>\n",
       "      <th>pagesnumber</th>\n",
       "      <th>ratingdist4</th>\n",
       "      <th>ratingdisttotal</th>\n",
       "      <th>publishmonth</th>\n",
       "      <th>publishday</th>\n",
       "      <th>publisher</th>\n",
       "      <th>countsofreview</th>\n",
       "      <th>publishyear</th>\n",
       "      <th>language</th>\n",
       "      <th>authors</th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingdist2</th>\n",
       "      <th>ratingdist5</th>\n",
       "      <th>isbn</th>\n",
       "      <th>ratingdist3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>1:9896</td>\n",
       "      <td>652</td>\n",
       "      <td>4:556485</td>\n",
       "      <td>total:2298124</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>28062</td>\n",
       "      <td>2006</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2:25317</td>\n",
       "      <td>5:1546466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:159960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>1:12455</td>\n",
       "      <td>870</td>\n",
       "      <td>4:604283</td>\n",
       "      <td>total:2358637</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>29770</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2:37005</td>\n",
       "      <td>5:1493113</td>\n",
       "      <td>0439358078</td>\n",
       "      <td>3:211781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>1:108202</td>\n",
       "      <td>309</td>\n",
       "      <td>4:1513191</td>\n",
       "      <td>total:6587388</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Scholastic Inc</td>\n",
       "      <td>75911</td>\n",
       "      <td>2003</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2:130310</td>\n",
       "      <td>5:4268227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:567458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>1:11896</td>\n",
       "      <td>352</td>\n",
       "      <td>4:706082</td>\n",
       "      <td>total:2560657</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>244</td>\n",
       "      <td>2003</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2:49353</td>\n",
       "      <td>5:1504505</td>\n",
       "      <td>0439554896</td>\n",
       "      <td>3:288821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>1:10128</td>\n",
       "      <td>435</td>\n",
       "      <td>4:630534</td>\n",
       "      <td>total:2610317</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>37093</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2:24849</td>\n",
       "      <td>5:1749958</td>\n",
       "      <td>043965548X</td>\n",
       "      <td>3:194848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>1:9419</td>\n",
       "      <td>734</td>\n",
       "      <td>4:606800</td>\n",
       "      <td>total:2431085</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>31978</td>\n",
       "      <td>2002</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.56</td>\n",
       "      <td>2:24282</td>\n",
       "      <td>5:1612165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:178419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>1:402</td>\n",
       "      <td>2690</td>\n",
       "      <td>4:4650</td>\n",
       "      <td>total:43968</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>166</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.78</td>\n",
       "      <td>2:283</td>\n",
       "      <td>5:37432</td>\n",
       "      <td>0439682584</td>\n",
       "      <td>3:1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Unauthorized Harry Potter Book Seven News: \"Ha...</td>\n",
       "      <td>1:0</td>\n",
       "      <td>152</td>\n",
       "      <td>4:7</td>\n",
       "      <td>total:28</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>Nimble Books</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>en-US</td>\n",
       "      <td>W. Frederick Zimmerman</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2:5</td>\n",
       "      <td>5:10</td>\n",
       "      <td>0976540606</td>\n",
       "      <td>3:6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Harry Potter Collection (Harry Potter, #1-6)</td>\n",
       "      <td>1:257</td>\n",
       "      <td>3342</td>\n",
       "      <td>4:4358</td>\n",
       "      <td>total:30313</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>809</td>\n",
       "      <td>2005</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2:218</td>\n",
       "      <td>5:24406</td>\n",
       "      <td>0439827604</td>\n",
       "      <td>3:1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>1:3443</td>\n",
       "      <td>815</td>\n",
       "      <td>4:75683</td>\n",
       "      <td>total:274268</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Gramercy Books</td>\n",
       "      <td>255</td>\n",
       "      <td>2005</td>\n",
       "      <td>eng</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>4.37</td>\n",
       "      <td>2:7613</td>\n",
       "      <td>5:157499</td>\n",
       "      <td>0517226952</td>\n",
       "      <td>3:30030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Limpeza do dataset",
   "id": "a4552e4fc9cb2e05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### An√°lise de nulls, uniques e tipo",
   "id": "edbb91cec7eda12e"
  },
  {
   "cell_type": "code",
   "id": "18516917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:05.026357Z",
     "start_time": "2025-10-27T18:45:04.611066Z"
    }
   },
   "source": [
    "profile = []\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    profile.append({\n",
    "        \"column\": c,\n",
    "        \"dtype\": str(s.dtype),\n",
    "        \"missing_%\": round(s.isna().mean()*100, 2),\n",
    "        \"unique_n\": int(s.nunique(dropna=True)),\n",
    "        \"example_values\": \", \".join(map(str, s.dropna().astype(str).unique()[:5]))\n",
    "    })\n",
    "import pandas as pd\n",
    "pd.DataFrame(profile).sort_values(\"missing_%\", ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             column    dtype  missing_%  unique_n  \\\n",
       "11         language   object      65.19        36   \n",
       "16             isbn   object       0.94     57552   \n",
       "8         publisher   object       0.85      7520   \n",
       "0                id    int64       0.00     58097   \n",
       "1              name   object       0.00     57510   \n",
       "2       ratingdist1   object       0.00      2378   \n",
       "5   ratingdisttotal   object       0.00     11464   \n",
       "6      publishmonth    int64       0.00        31   \n",
       "4       ratingdist4   object       0.00      7994   \n",
       "3       pagesnumber    int64       0.00      1340   \n",
       "9    countsofreview    int64       0.00      2163   \n",
       "7        publishday    int64       0.00        12   \n",
       "12          authors   object       0.00     28633   \n",
       "10      publishyear    int64       0.00       105   \n",
       "13           rating  float64       0.00       267   \n",
       "14      ratingdist2   object       0.00      3718   \n",
       "15      ratingdist5   object       0.00      7964   \n",
       "17      ratingdist3   object       0.00      6620   \n",
       "\n",
       "                                       example_values  \n",
       "11                          eng, en-US, fre, spa, mul  \n",
       "16  0439358078, 0439554896, 043965548X, 0439682584...  \n",
       "8   Scholastic Inc., Scholastic Inc, Scholastic, N...  \n",
       "0                                       1, 2, 3, 4, 5  \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...  \n",
       "2         1:9896, 1:12455, 1:108202, 1:11896, 1:10128  \n",
       "5   total:2298124, total:2358637, total:6587388, t...  \n",
       "6                                   16, 1, 28, 13, 26  \n",
       "4   4:556485, 4:604283, 4:1513191, 4:706082, 4:630534  \n",
       "3                             652, 870, 309, 352, 435  \n",
       "9                     28062, 29770, 75911, 244, 37093  \n",
       "7                                      9, 11, 5, 4, 8  \n",
       "12  J.K. Rowling, W. Frederick Zimmerman, Douglas ...  \n",
       "10                       2006, 2004, 2003, 2002, 2005  \n",
       "13          4.57, 4.5, 4.47, 4.42, 4.5600000000000005  \n",
       "14       2:25317, 2:37005, 2:130310, 2:49353, 2:24849  \n",
       "15  5:1546466, 5:1493113, 5:4268227, 5:1504505, 5:...  \n",
       "17   3:159960, 3:211781, 3:567458, 3:288821, 3:194848  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>unique_n</th>\n",
       "      <th>example_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>language</td>\n",
       "      <td>object</td>\n",
       "      <td>65.19</td>\n",
       "      <td>36</td>\n",
       "      <td>eng, en-US, fre, spa, mul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>isbn</td>\n",
       "      <td>object</td>\n",
       "      <td>0.94</td>\n",
       "      <td>57552</td>\n",
       "      <td>0439358078, 0439554896, 043965548X, 0439682584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>publisher</td>\n",
       "      <td>object</td>\n",
       "      <td>0.85</td>\n",
       "      <td>7520</td>\n",
       "      <td>Scholastic Inc., Scholastic Inc, Scholastic, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58097</td>\n",
       "      <td>1, 2, 3, 4, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>57510</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ratingdist1</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2378</td>\n",
       "      <td>1:9896, 1:12455, 1:108202, 1:11896, 1:10128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ratingdisttotal</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11464</td>\n",
       "      <td>total:2298124, total:2358637, total:6587388, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>publishmonth</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31</td>\n",
       "      <td>16, 1, 28, 13, 26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratingdist4</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7994</td>\n",
       "      <td>4:556485, 4:604283, 4:1513191, 4:706082, 4:630534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pagesnumber</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1340</td>\n",
       "      <td>652, 870, 309, 352, 435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>countsofreview</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2163</td>\n",
       "      <td>28062, 29770, 75911, 244, 37093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>publishday</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>9, 11, 5, 4, 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>authors</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28633</td>\n",
       "      <td>J.K. Rowling, W. Frederick Zimmerman, Douglas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>publishyear</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>105</td>\n",
       "      <td>2006, 2004, 2003, 2002, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rating</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>267</td>\n",
       "      <td>4.57, 4.5, 4.47, 4.42, 4.5600000000000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ratingdist2</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3718</td>\n",
       "      <td>2:25317, 2:37005, 2:130310, 2:49353, 2:24849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ratingdist5</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7964</td>\n",
       "      <td>5:1546466, 5:1493113, 5:4268227, 5:1504505, 5:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratingdist3</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6620</td>\n",
       "      <td>3:159960, 3:211781, 3:567458, 3:288821, 3:194848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Avaliar livros com ano > 2020",
   "id": "85cee9981d6500e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:05.074079Z",
     "start_time": "2025-10-27T18:45:05.035211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DEFINIR NOMES DAS COLUNAS\n",
    "year_col = \"publishyear\"\n",
    "title_col = \"name\"\n",
    "author_col = \"authors\"\n",
    "\n",
    "# FILTRAR > 2020\n",
    "invalidYear = df[df[year_col] > 2020].copy()\n",
    "validYear = df[df[year_col] <= 2020].copy()\n",
    "\n",
    "# MONTAR COLUNAS\n",
    "cols_to_show = [title_col, author_col, year_col]\n",
    "\n",
    "print(f\"T√≠tulos inv√°lidos (> 2020): {len(invalidYear)}\")\n",
    "\n",
    "# EXIBIR INV√ÅLIDOS\n",
    "if len(invalidYear) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è LIVROS COM ANO INV√ÅLIDO (> 2020):\")\n",
    "    display(invalidYear[cols_to_show].sort_values(year_col, ascending=False))\n",
    "else:\n",
    "    print(\"‚úÖ Nenhum livro com ano > 2020\")\n",
    "\n",
    "# EXIBIR AMOSTRA\n",
    "print(f\"\\nT√≠tulos v√°lidos (‚â§ 2020): {len(validYear)}\")"
   ],
   "id": "95fd789fee3862d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√≠tulos inv√°lidos (> 2020): 1\n",
      "\n",
      "‚ö†Ô∏è LIVROS COM ANO INV√ÅLIDO (> 2020):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                 name           authors  \\\n",
       "25597  The Water Babies: A Fairy Tale for a Land Baby  Charles Kingsley   \n",
       "\n",
       "       publishyear  \n",
       "25597         3002  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>authors</th>\n",
       "      <th>publishyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25597</th>\n",
       "      <td>The Water Babies: A Fairy Tale for a Land Baby</td>\n",
       "      <td>Charles Kingsley</td>\n",
       "      <td>3002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T√≠tulos v√°lidos (‚â§ 2020): 58291\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Valida√ß√£o e Normaliza√ß√£o de ISBN (ISBN-10 / ISBN-13)",
   "id": "a35cf03d356722ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:06.353024Z",
     "start_time": "2025-10-27T18:45:05.084058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Valida ISBNs e detecta duplicatas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# 0. CONFIGURA√á√ÉO E CARREGAMENTO\n",
    "# =====================================================\n",
    "\n",
    "# Caminhos\n",
    "INPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR = Path(\"exports/duplicatas\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìö VALIDA√á√ÉO E AN√ÅLISE DE ISBN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÇ Lendo dados de: {INPUT_DIR}\")\n",
    "print(f\"üìÅ Salvando em: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Carregar dataset limpo\n",
    "clean_file = INPUT_DIR / \"book_data_clean.csv\"\n",
    "\n",
    "if not clean_file.exists():\n",
    "    print(f\"‚ùå Erro: Arquivo n√£o encontrado: {clean_file}\")\n",
    "    print(f\"   Execute primeiro o script de limpeza de dados!\")\n",
    "\n",
    "    # Tentar carregar do dataset original\n",
    "    print(f\"\\n‚ö†Ô∏è  Tentando carregar dataset original...\")\n",
    "    original_file = Path(\"data/book1-100k.csv\")\n",
    "\n",
    "    if original_file.exists():\n",
    "        print(f\"‚úÖ Dataset original encontrado: {original_file}\")\n",
    "        df = pd.read_csv(original_file, engine=\"python\", on_bad_lines=\"skip\")\n",
    "        # Padronizar nomes de colunas\n",
    "        df.columns = [c.strip().replace(\"  \", \" \").replace(\" \", \"_\").lower() for c in df.columns]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Nenhum dataset encontrado! Execute a limpeza de dados primeiro.\")\n",
    "else:\n",
    "    df = pd.read_csv(clean_file)\n",
    "    print(f\"‚úÖ Dataset limpo carregado: {len(df):,} livros\")\n",
    "\n",
    "# Nome da coluna\n",
    "isbn_col = \"isbn\"\n",
    "\n",
    "# Verificar se coluna existe\n",
    "if isbn_col not in df.columns:\n",
    "    print(f\"‚ùå Erro: Coluna '{isbn_col}' n√£o encontrada!\")\n",
    "    print(f\"   Colunas dispon√≠veis: {list(df.columns)}\")\n",
    "    raise ValueError(f\"Coluna '{isbn_col}' n√£o encontrada no dataset\")\n",
    "\n",
    "print(f\"‚úÖ Coluna '{isbn_col}' encontrada\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. FUN√á√ïES DE VALIDA√á√ÉO\n",
    "# =====================================================\n",
    "\n",
    "def clean_isbn(s):\n",
    "    \"\"\"Remove caracteres n√£o num√©ricos do ISBN\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[^0-9X]\", \"\", s)  # Remove tudo exceto n√∫meros e X\n",
    "    return s if s else None\n",
    "\n",
    "\n",
    "def is_valid_isbn10(isbn):\n",
    "    \"\"\"Valida ISBN-10 usando algoritmo de check digit\"\"\"\n",
    "    if not isbn or len(isbn) != 10:\n",
    "        return False\n",
    "\n",
    "    total = 0\n",
    "    for i, ch in enumerate(isbn[:9]):\n",
    "        if not ch.isdigit():\n",
    "            return False\n",
    "        total += (10 - i) * int(ch)\n",
    "\n",
    "    # √öltimo caractere pode ser X (vale 10)\n",
    "    check = isbn[-1]\n",
    "    if check == 'X':\n",
    "        total += 10\n",
    "    elif check.isdigit():\n",
    "        total += int(check)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    return total % 11 == 0\n",
    "\n",
    "\n",
    "def is_valid_isbn13(isbn):\n",
    "    \"\"\"Valida ISBN-13 usando algoritmo de check digit\"\"\"\n",
    "    if not isbn or len(isbn) != 13 or not isbn.isdigit():\n",
    "        return False\n",
    "\n",
    "    # Calcula check digit\n",
    "    total = 0\n",
    "    for i, ch in enumerate(isbn[:12]):\n",
    "        weight = 1 if i % 2 == 0 else 3\n",
    "        total += int(ch) * weight\n",
    "\n",
    "    check_digit = (10 - (total % 10)) % 10\n",
    "    return check_digit == int(isbn[-1])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. AN√ÅLISE DO DATASET\n",
    "# =====================================================\n",
    "\n",
    "print(\"üîç Analisando ISBNs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Limpar ISBNs\n",
    "df['isbn_clean'] = df[isbn_col].apply(clean_isbn)\n",
    "\n",
    "# Detectar tipo de ISBN\n",
    "df['isbn_length'] = df['isbn_clean'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# Validar\n",
    "df['is_isbn10'] = df['isbn_clean'].apply(\n",
    "    lambda x: is_valid_isbn10(x) if x and len(x) == 10 else False\n",
    ")\n",
    "df['is_isbn13'] = df['isbn_clean'].apply(\n",
    "    lambda x: is_valid_isbn13(x) if x and len(x) == 13 else False\n",
    ")\n",
    "\n",
    "# ISBNs v√°lidos (10 ou 13)\n",
    "df['is_valid'] = df['is_isbn10'] | df['is_isbn13']\n",
    "\n",
    "# =====================================================\n",
    "# 3. ESTAT√çSTICAS\n",
    "# =====================================================\n",
    "\n",
    "total = len(df)\n",
    "com_isbn = df['isbn_clean'].notna().sum()\n",
    "isbn10_count = (df['isbn_length'] == 10).sum()\n",
    "isbn13_count = (df['isbn_length'] == 13).sum()\n",
    "isbn10_valid = df['is_isbn10'].sum()\n",
    "isbn13_valid = df['is_isbn13'].sum()\n",
    "validos = df['is_valid'].sum()\n",
    "invalidos = com_isbn - validos\n",
    "\n",
    "print(f\"\\nüìä Resumo:\")\n",
    "print(f\"   Total de livros: {total:,}\")\n",
    "print(f\"   Com ISBN (n√£o nulo): {com_isbn:,} ({com_isbn/total*100:.1f}%)\")\n",
    "print(f\"   Sem ISBN: {total - com_isbn:,} ({(total-com_isbn)/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìè Tipos encontrados:\")\n",
    "print(f\"   ISBN-10: {isbn10_count:,} ({isbn10_valid:,} v√°lidos)\")\n",
    "print(f\"   ISBN-13: {isbn13_count:,} ({isbn13_valid:,} v√°lidos)\")\n",
    "print(f\"   Outros tamanhos: {com_isbn - isbn10_count - isbn13_count:,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Valida√ß√£o:\")\n",
    "print(f\"   ISBNs v√°lidos (10 ou 13): {validos:,} ({validos/com_isbn*100:.1f}%)\")\n",
    "print(f\"   ISBNs inv√°lidos: {invalidos:,} ({invalidos/com_isbn*100:.1f}%)\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. DETECTAR DUPLICATAS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç AN√ÅLISE DE DUPLICATAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Contar duplicatas (usando ISBN limpo)\n",
    "df_com_isbn = df[df['isbn_clean'].notna()]\n",
    "duplicatas = df_com_isbn['isbn_clean'].value_counts()\n",
    "duplicatas = duplicatas[duplicatas > 1]\n",
    "\n",
    "print(f\"\\nISBNs duplicados: {len(duplicatas):,}\")\n",
    "print(f\"Total de ocorr√™ncias duplicadas: {duplicatas.sum():,}\")\n",
    "\n",
    "if len(duplicatas) > 0:\n",
    "    # Criar DataFrame de duplicatas\n",
    "    df_dupes = df[df['isbn_clean'].isin(duplicatas.index)].copy()\n",
    "    df_dupes = df_dupes.sort_values('isbn_clean')\n",
    "\n",
    "    print(f\"\\nüìã Top 10 ISBNs mais duplicados:\")\n",
    "    for isbn, count in duplicatas.head(10).items():\n",
    "        print(f\"   {isbn}: {count} ocorr√™ncias\")\n",
    "\n",
    "    # Exibir tabela de duplicatas\n",
    "    print(f\"\\nüìö Tabela de livros duplicados (primeiros 30):\")\n",
    "    cols_mostrar = ['isbn_clean', 'isbn_length', 'is_valid', 'name', 'authors']\n",
    "    cols_mostrar = [c for c in cols_mostrar if c in df_dupes.columns]\n",
    "    if len(cols_mostrar) > 0:\n",
    "        print(df_dupes[cols_mostrar].head(30).to_string())\n",
    "else:\n",
    "    print(\"\\n‚úÖ Nenhuma duplicata encontrada!\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. AMOSTRAS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã AMOSTRAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ISBNs inv√°lidos\n",
    "invalidos_df = df[df['isbn_clean'].notna() & ~df['is_valid']]\n",
    "if len(invalidos_df) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ISBNs inv√°lidos ({len(invalidos_df)} encontrados):\")\n",
    "    cols = ['isbn', 'isbn_clean', 'isbn_length', 'name']\n",
    "    cols = [c for c in cols if c in invalidos_df.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(invalidos_df[cols].head(10).to_string())\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos os ISBNs s√£o v√°lidos!\")\n",
    "\n",
    "# ISBNs v√°lidos por tipo\n",
    "validos_10 = df[df['is_isbn10']]\n",
    "validos_13 = df[df['is_isbn13']]\n",
    "\n",
    "if len(validos_10) > 0:\n",
    "    print(f\"\\n‚úÖ ISBNs-10 v√°lidos ({len(validos_10)} encontrados):\")\n",
    "    cols = ['isbn_clean', 'name', 'authors']\n",
    "    cols = [c for c in cols if c in validos_10.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(validos_10[cols].head(5).to_string())\n",
    "\n",
    "if len(validos_13) > 0:\n",
    "    print(f\"\\n‚úÖ ISBNs-13 v√°lidos ({len(validos_13)} encontrados):\")\n",
    "    cols = ['isbn_clean', 'name', 'authors']\n",
    "    cols = [c for c in cols if c in validos_13.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(validos_13[cols].head(5).to_string())\n",
    "\n",
    "# =====================================================\n",
    "# 6. EXPORTAR\n",
    "# =====================================================\n",
    "\n",
    "def exportar_resultados():\n",
    "    \"\"\"Exporta relat√≥rios de ISBN para pasta exports/duplicatas\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üíæ EXPORTANDO RESULTADOS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Relat√≥rio completo\n",
    "    cols_export = ['id', 'name', 'authors', 'isbn', 'isbn_clean',\n",
    "                   'isbn_length', 'is_isbn10', 'is_isbn13', 'is_valid']\n",
    "    cols_export = [c for c in cols_export if c in df.columns]\n",
    "\n",
    "    complete_file = OUTPUT_DIR / 'isbn_relatorio_completo.csv'\n",
    "    df[cols_export].to_csv(complete_file, index=False)\n",
    "    print(f\"‚úÖ Relat√≥rio completo salvo: {complete_file}\")\n",
    "    print(f\"   {len(df):,} registros salvos\")\n",
    "\n",
    "    # Apenas duplicatas\n",
    "    if len(duplicatas) > 0:\n",
    "        dupes_file = OUTPUT_DIR / 'isbn_duplicatas.csv'\n",
    "        df_dupes.to_csv(dupes_file, index=False)\n",
    "        print(f\"‚úÖ Duplicatas salvas: {dupes_file}\")\n",
    "        print(f\"   {len(df_dupes):,} registros duplicados salvos\")\n",
    "\n",
    "    # Apenas inv√°lidos\n",
    "    if len(invalidos_df) > 0:\n",
    "        invalid_file = OUTPUT_DIR / 'isbn_invalidos.csv'\n",
    "        invalidos_df.to_csv(invalid_file, index=False)\n",
    "        print(f\"‚úÖ Inv√°lidos salvos: {invalid_file}\")\n",
    "        print(f\"   {len(invalidos_df):,} registros inv√°lidos salvos\")\n",
    "\n",
    "    # Estat√≠sticas resumidas\n",
    "    stats = {\n",
    "        \"M√©trica\": [\n",
    "            \"Total de livros\",\n",
    "            \"Com ISBN (n√£o nulo)\",\n",
    "            \"ISBNs v√°lidos\",\n",
    "            \"ISBNs inv√°lidos\",\n",
    "            \"ISBN-10 v√°lidos\",\n",
    "            \"ISBN-13 v√°lidos\",\n",
    "            \"ISBNs duplicados\",\n",
    "            \"Total de duplicatas\"\n",
    "        ],\n",
    "        \"Valor\": [\n",
    "            len(df),\n",
    "            df['isbn_clean'].notna().sum(),\n",
    "            df['is_valid'].sum(),\n",
    "            (df['isbn_clean'].notna() & ~df['is_valid']).sum(),\n",
    "            df['is_isbn10'].sum(),\n",
    "            df['is_isbn13'].sum(),\n",
    "            len(duplicatas),\n",
    "            duplicatas.sum() if len(duplicatas) > 0 else 0\n",
    "        ]\n",
    "    }\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_file = OUTPUT_DIR / 'isbn_statistics.csv'\n",
    "    stats_df.to_csv(stats_file, index=False)\n",
    "    print(f\"‚úÖ Estat√≠sticas salvas: {stats_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üìÅ Todos os arquivos salvos em: {OUTPUT_DIR.absolute()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Executar exporta√ß√£o\n",
    "exportar_resultados()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ An√°lise conclu√≠da!\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "f30b4920f3657677",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìö VALIDA√á√ÉO E AN√ÅLISE DE ISBN\n",
      "======================================================================\n",
      "üìÇ Lendo dados de: exports\\clean_data\n",
      "üìÅ Salvando em: exports\\duplicatas\n",
      "\n",
      "‚ùå Erro: Arquivo n√£o encontrado: exports\\clean_data\\book_data_clean.csv\n",
      "   Execute primeiro o script de limpeza de dados!\n",
      "\n",
      "‚ö†Ô∏è  Tentando carregar dataset original...\n",
      "‚úÖ Dataset original encontrado: data\\book1-100k.csv\n",
      "‚úÖ Coluna 'isbn' encontrada\n",
      "\n",
      "üîç Analisando ISBNs...\n",
      "======================================================================\n",
      "\n",
      "üìä Resumo:\n",
      "   Total de livros: 58,292\n",
      "   Com ISBN (n√£o nulo): 57,746 (99.1%)\n",
      "   Sem ISBN: 546 (0.9%)\n",
      "\n",
      "üìè Tipos encontrados:\n",
      "   ISBN-10: 57,738 (57,730 v√°lidos)\n",
      "   ISBN-13: 0 (0 v√°lidos)\n",
      "   Outros tamanhos: 8\n",
      "\n",
      "‚úÖ Valida√ß√£o:\n",
      "   ISBNs v√°lidos (10 ou 13): 57,730 (100.0%)\n",
      "   ISBNs inv√°lidos: 16 (0.0%)\n",
      "\n",
      "======================================================================\n",
      "üîç AN√ÅLISE DE DUPLICATAS\n",
      "======================================================================\n",
      "\n",
      "ISBNs duplicados: 194\n",
      "Total de ocorr√™ncias duplicadas: 388\n",
      "\n",
      "üìã Top 10 ISBNs mais duplicados:\n",
      "   038072023X: 2 ocorr√™ncias\n",
      "   0380715732: 2 ocorr√™ncias\n",
      "   0380725355: 2 ocorr√™ncias\n",
      "   006103097X: 2 ocorr√™ncias\n",
      "   0380763621: 2 ocorr√™ncias\n",
      "   0380720248: 2 ocorr√™ncias\n",
      "   0380709937: 2 ocorr√™ncias\n",
      "   038076363X: 2 ocorr√™ncias\n",
      "   0380715740: 2 ocorr√™ncias\n",
      "   0380709945: 2 ocorr√™ncias\n",
      "\n",
      "üìö Tabela de livros duplicados (primeiros 30):\n",
      "       isbn_clean  isbn_length  is_valid                                                                                                                                                                     name               authors\n",
      "49794  006053818X           10      True  Everyone Comes to Elaine's: Forty Years of Movie Stars, All-Stars, Literary Lions, Financial Scions, Top Cops, Politicians, and Power Brokers at the Legendary Hot Spot         A.E. Hotchner\n",
      "49877  006053818X           10      True  Everyone Comes to Elaine's: Forty Years of Movie Stars, All-Stars, Literary Lions, Financial Scions, Top Cops, Politicians, and Power Brokers at the Legendary Hot Spot         A.E. Hotchner\n",
      "44115  0060610352           10      True                                                                                   The God We Never Knew: Beyond Dogmatic Religion to a More Authentic Contemporary Faith        Marcus J. Borg\n",
      "44225  0060610352           10      True                                                                                   The God We Never Knew: Beyond Dogmatic Religion to a More Authentic Contemporary Faith        Marcus J. Borg\n",
      "44140  0060611391           10      True                                                                                                                                      Wishful Thinking: A Theological ABC    Frederick Buechner\n",
      "44255  0060611391           10      True                                                                                                                                      Wishful Thinking: A Theological ABC    Frederick Buechner\n",
      "44137  0060611413           10      True                                                                                                                                                       Peculiar Treasures    Frederick Buechner\n",
      "44251  0060611413           10      True                                                                                                                                                       Peculiar Treasures    Frederick Buechner\n",
      "44254  0060611561           10      True                                                                                                         Telling the Truth: The Gospel as Tragedy, Comedy, and Fairy Tale    Frederick Buechner\n",
      "44139  0060611561           10      True                                                                                                         Telling the Truth: The Gospel as Tragedy, Comedy, and Fairy Tale    Frederick Buechner\n",
      "44256  006061160X           10      True                                                                                                                                                        The Wizard's Tide    Frederick Buechner\n",
      "44141  006061160X           10      True                                                                                                                                                        The Wizard's Tide    Frederick Buechner\n",
      "44134  0060611626           10      True                                                                                                                                                                   Godric    Frederick Buechner\n",
      "44248  0060611626           10      True                                                                                                                                                                   Godric    Frederick Buechner\n",
      "44250  006061174X           10      True                                                                                                                                                   The Magnificent Defeat    Frederick Buechner\n",
      "44136  006061174X           10      True                                                                                                                                                   The Magnificent Defeat    Frederick Buechner\n",
      "44249  0060611758           10      True                                                                                                                                                       The Hungering Dark    Frederick Buechner\n",
      "44135  0060611758           10      True                                                                                                                                                       The Hungering Dark    Frederick Buechner\n",
      "44247  0060611782           10      True                                                                                                                                                                  Brendan    Frederick Buechner\n",
      "44133  0060611782           10      True                                                                                                                                                                  Brendan    Frederick Buechner\n",
      "44313  0060616598           10      True                                                                                                                                                The Birth of Christianity  John Dominic Crossan\n",
      "44189  0060616598           10      True                                                                                                                                                The Birth of Christianity  John Dominic Crossan\n",
      "49814  0060724552           10      True                                                                                                                                Party Princess (The Princess Diaries, #7)             Meg Cabot\n",
      "49897  0060724552           10      True                                                                                                                                Party Princess (The Princess Diaries, #7)             Meg Cabot\n",
      "49893  0060724560           10      True                                                                                                                         Princess on the Brink (The Princess Diaries, #8)             Meg Cabot\n",
      "49810  0060724560           10      True                                                                                                                         Princess on the Brink (The Princess Diaries, #8)             Meg Cabot\n",
      "49809  0060898488           10      True                                                                                                                                                        Portrait in Sepia        Isabel Allende\n",
      "49892  0060898488           10      True                                                                                                                                                        Portrait in Sepia        Isabel Allende\n",
      "49790  0060905859           10      True                                                                                                                                          Karl Marx: His Life and Thought        David McLellan\n",
      "49873  0060905859           10      True                                                                                                                                          Karl Marx: His Life and Thought        David McLellan\n",
      "\n",
      "======================================================================\n",
      "üìã AMOSTRAS\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  ISBNs inv√°lidos (16 encontrados):\n",
      "             isbn  isbn_clean  isbn_length                                                            name\n",
      "2228   0312349486  0312349486           10                              Twelve Sharp (Stephanie Plum, #12)\n",
      "3602    188098510   188098510            9                     Art to Choke Hearts and Pissing in the Gene\n",
      "6952    084386874   084386874            9                                        Rejoice (Redemption, #4)\n",
      "7774   189708210x   189708210            9           Bookclub in a Box Discusses the Novel The Corrections\n",
      "11400  043938950x   043938950            9                           Getting the Girl (Wolfe Brothers, #3)\n",
      "14674  043985623x   043985623            9                                                       Clockwork\n",
      "17659  1591854135  1591854135           10  The Bait Of Satan: Living Free from the Deadly Trap of Offense\n",
      "22269  9781903254  9781903254           10            The Gospel of Filth: A Bible of Decadence & Darkness\n",
      "25177  4490249512  4490249512           10                     The Currents of Space (Galactic Empire, #2)\n",
      "28674  8486478698  8486478698           10             Las Cenizas de Angela (Angela's Ashes): Una Memoria\n",
      "\n",
      "‚úÖ ISBNs-10 v√°lidos (57730 encontrados):\n",
      "   isbn_clean                                                                                     name                 authors\n",
      "1  0439358078                             Harry Potter and the Order of the Phoenix (Harry Potter, #5)            J.K. Rowling\n",
      "3  0439554896                               Harry Potter and the Chamber of Secrets (Harry Potter, #2)            J.K. Rowling\n",
      "4  043965548X                              Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)            J.K. Rowling\n",
      "6  0439682584                                   Harry Potter Boxed Set, Books 1-5 (Harry Potter, #1-5)            J.K. Rowling\n",
      "7  0976540606  Unauthorized Harry Potter Book Seven News: \"Half-Blood Prince\" Analysis and Speculation  W. Frederick Zimmerman\n",
      "\n",
      "======================================================================\n",
      "üíæ EXPORTANDO RESULTADOS\n",
      "======================================================================\n",
      "‚úÖ Relat√≥rio completo salvo: exports\\duplicatas\\isbn_relatorio_completo.csv\n",
      "   58,292 registros salvos\n",
      "‚úÖ Duplicatas salvas: exports\\duplicatas\\isbn_duplicatas.csv\n",
      "   388 registros duplicados salvos\n",
      "‚úÖ Inv√°lidos salvos: exports\\duplicatas\\isbn_invalidos.csv\n",
      "   16 registros inv√°lidos salvos\n",
      "‚úÖ Estat√≠sticas salvas: exports\\duplicatas\\isbn_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "üìÅ Todos os arquivos salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\duplicatas\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ An√°lise conclu√≠da!\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detec√ß√£o de duplicatas de t√≠tulos com embeddings simples (TF-IDF n-gramas)",
   "id": "bb64c38064557765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:46:58.302868Z",
     "start_time": "2025-10-27T18:45:06.362169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "import os, re, unicodedata, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Colunas candidatas\n",
    "title_candidates = [\"title\", \"book_title\", \"name\"]\n",
    "id_candidates    = [\"bookid\", \"book_id\", \"id\"]\n",
    "\n",
    "# Criar pasta para exporta√ß√µes\n",
    "OUTPUT_DIR = Path(\"exports/duplicatas\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Arquivo:\", path)\n",
    "print(f\"Pasta de exporta√ß√£o: {OUTPUT_DIR}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 2. LEITURA E PREPARA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Erro ao ler {path}: {e}\")\n",
    "\n",
    "# Padroniza nomes de colunas\n",
    "df.columns = [c.strip().lower().replace(\"  \", \" \").replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "def pick(cands, cols): return next((c for c in cands if c in cols), None)\n",
    "\n",
    "title_col = pick(title_candidates, df.columns)\n",
    "id_col = pick(id_candidates, df.columns)\n",
    "\n",
    "if title_col is None:\n",
    "    raise ValueError(\"Nenhuma coluna de t√≠tulo encontrada.\")\n",
    "if id_col is None:\n",
    "    id_col = \"__row_id__\"\n",
    "    df[id_col] = np.arange(len(df))\n",
    "\n",
    "print(f\"‚úÖ Coluna de t√≠tulo: '{title_col}'\")\n",
    "print(f\"‚úÖ Coluna de ID: '{id_col}'\")\n",
    "print(f\"üìä Total de livros: {len(df):,}\")\n",
    "print()\n",
    "\n",
    "# Normaliza√ß√£o de texto\n",
    "def normalize_text(t):\n",
    "    t = str(t).strip().lower()\n",
    "    t = unicodedata.normalize(\"NFKD\", t).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df[\"__title_norm__\"] = df[title_col].astype(str).apply(normalize_text)\n",
    "print(\"üìù Exemplo de t√≠tulos normalizados:\")\n",
    "display(df[[title_col, \"__title_norm__\"]].head(10))\n",
    "\n",
    "# ============================================================\n",
    "# 3. VETORIZA√á√ÉO TF-IDF + SIMILARIDADE COSSENO\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"\\nüîç Calculando similaridades...\")\n",
    "corpus = df[\"__title_norm__\"].fillna(\"\")\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "n_neighbors = 10 if len(df) <= 20000 else 5\n",
    "nn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "nn.fit(X)\n",
    "\n",
    "distances, indices = nn.kneighbors(X, n_neighbors=n_neighbors, return_distance=True)\n",
    "print(f\"‚úÖ Matriz de vizinhos calculada: {distances.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. MONTAR PARES COM SIMILARIDADE\n",
    "# ============================================================\n",
    "\n",
    "similarity_threshold = 0.15  # dist√¢ncia ‚â§ 0.15 ‚Üí similaridade ‚â• 0.85\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(df)):\n",
    "    for k in range(1, indices.shape[1]):\n",
    "        j = indices[i, k]\n",
    "        d = distances[i, k]\n",
    "        if np.isfinite(d) and d <= similarity_threshold:\n",
    "            a, b = int(df.iloc[i][id_col]), int(df.iloc[j][id_col])\n",
    "            if a != b:\n",
    "                i_, j_ = sorted([a, b])\n",
    "                pairs.append((i_, j_, float(1 - d)))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[f\"{id_col}_a\", f\"{id_col}_b\", \"cosine_similarity\"]).drop_duplicates()\n",
    "print(f\"\\n‚úÖ Total de pares similares encontrados: {len(pairs_df):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. MOSTRAR T√çTULOS SEMELHANTES\n",
    "# ============================================================\n",
    "\n",
    "left  = df[[id_col, \"__title_norm__\", title_col]].rename(columns={id_col: f\"{id_col}_a\", \"__title_norm__\":\"title_norm_a\", title_col:\"title_a\"})\n",
    "right = df[[id_col, \"__title_norm__\", title_col]].rename(columns={id_col: f\"{id_col}_b\", \"__title_norm__\":\"title_norm_b\", title_col:\"title_b\"})\n",
    "\n",
    "pairs_details = pairs_df.merge(left, on=f\"{id_col}_a\").merge(right, on=f\"{id_col}_b\")\n",
    "\n",
    "print(\"\\nüìã Amostra de pares similares:\")\n",
    "display(pairs_details.head(30))\n",
    "\n",
    "# ============================================================\n",
    "# 6. CLUSTERS DE DUPLICATAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüîó Agrupando duplicatas em clusters...\")\n",
    "\n",
    "parent = {}\n",
    "def find(x):\n",
    "    parent.setdefault(x, x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(a, b):\n",
    "    ra, rb = find(a), find(b)\n",
    "    if ra != rb:\n",
    "        parent[rb] = ra\n",
    "\n",
    "for _, r in pairs_df.iterrows():\n",
    "    union(r[f\"{id_col}_a\"], r[f\"{id_col}_b\"])\n",
    "\n",
    "clusters = {}\n",
    "for node in df[id_col]:\n",
    "    root = find(node)\n",
    "    clusters.setdefault(root, []).append(node)\n",
    "\n",
    "clusters_df = pd.DataFrame(\n",
    "    [{\"cluster_id\": k, \"size\": len(v), \"members\": v} for k,v in clusters.items() if len(v)>1]\n",
    ").sort_values(\"size\", ascending=False)\n",
    "\n",
    "print(f\"‚úÖ Clusters detectados: {len(clusters_df):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. ESTAT√çSTICAS DETALHADAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä ESTAT√çSTICAS DE DUPLICATAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "total_livros = len(df)\n",
    "livros_com_duplicatas = sum([len(members) for members in clusters.values() if len(members) > 1])\n",
    "livros_unicos = total_livros - livros_com_duplicatas\n",
    "\n",
    "print(f\"\\nüìö Vis√£o Geral:\")\n",
    "print(f\"   Total de livros no dataset: {total_livros:,}\")\n",
    "print(f\"   Livros √∫nicos (sem duplicatas): {livros_unicos:,} ({livros_unicos/total_livros*100:.1f}%)\")\n",
    "print(f\"   Livros com duplicatas: {livros_com_duplicatas:,} ({livros_com_duplicatas/total_livros*100:.1f}%)\")\n",
    "print(f\"   Total de clusters de duplicatas: {len(clusters_df):,}\")\n",
    "\n",
    "# Estat√≠sticas de clusters\n",
    "print(f\"\\nüîó Clusters:\")\n",
    "print(f\"   Maior cluster: {clusters_df['size'].max()} livros\")\n",
    "print(f\"   Menor cluster: {clusters_df['size'].min()} livros\")\n",
    "print(f\"   M√©dia de livros por cluster: {clusters_df['size'].mean():.1f}\")\n",
    "print(f\"   Mediana: {clusters_df['size'].median():.0f}\")\n",
    "\n",
    "# Distribui√ß√£o de tamanhos\n",
    "print(f\"\\nüìä Distribui√ß√£o de Tamanhos de Clusters:\")\n",
    "size_dist = clusters_df['size'].value_counts().sort_index()\n",
    "for size, count in size_dist.head(10).items():\n",
    "    print(f\"   {size} duplicatas: {count:,} clusters\")\n",
    "\n",
    "# Top clusters com t√≠tulos\n",
    "print(f\"\\nüèÜ TOP 10 MAIORES CLUSTERS DE DUPLICATAS:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in clusters_df.head(10).iterrows():\n",
    "    cluster_ids = row['members']\n",
    "    cluster_books = df[df[id_col].isin(cluster_ids)]\n",
    "\n",
    "    # Pegar t√≠tulos do cluster\n",
    "    titles = cluster_books[title_col].unique()\n",
    "    main_title = titles[0] if len(titles) > 0 else \"N/A\"\n",
    "    if len(main_title) > 60:\n",
    "        main_title = main_title[:60] + \"...\"\n",
    "\n",
    "    print(f\"#{idx+1:2d} | {row['size']:3d} livros | Cluster {row['cluster_id']:6.0f}\")\n",
    "    print(f\"     T√≠tulo: {main_title}\")\n",
    "\n",
    "    # Mostrar varia√ß√µes de t√≠tulo se houver\n",
    "    if len(titles) > 1:\n",
    "        print(f\"     Varia√ß√µes: {len(titles)} t√≠tulos diferentes no cluster\")\n",
    "    print()\n",
    "\n",
    "# Estat√≠sticas de similaridade\n",
    "print(f\"üìà Similaridade dos Pares:\")\n",
    "print(f\"   Total de pares encontrados: {len(pairs_df):,}\")\n",
    "print(f\"   Similaridade m√©dia: {pairs_df['cosine_similarity'].mean():.3f}\")\n",
    "print(f\"   Similaridade m√≠nima: {pairs_df['cosine_similarity'].min():.3f}\")\n",
    "print(f\"   Similaridade m√°xima: {pairs_df['cosine_similarity'].max():.3f}\")\n",
    "\n",
    "# Distribui√ß√£o de similaridade\n",
    "print(f\"\\nüìä Distribui√ß√£o de Similaridade:\")\n",
    "bins = [0.85, 0.90, 0.95, 0.98, 1.00]\n",
    "labels = ['0.85-0.90', '0.90-0.95', '0.95-0.98', '0.98-1.00']\n",
    "pairs_df['sim_range'] = pd.cut(pairs_df['cosine_similarity'], bins=bins, labels=labels, include_lowest=True)\n",
    "sim_dist = pairs_df['sim_range'].value_counts().sort_index()\n",
    "for range_label, count in sim_dist.items():\n",
    "    pct = count / len(pairs_df) * 100\n",
    "    print(f\"   {range_label}: {count:,} pares ({pct:.1f}%)\")\n",
    "\n",
    "# Pares mais similares\n",
    "print(f\"\\nüéØ TOP 10 PARES MAIS SIMILARES:\")\n",
    "print(\"-\" * 70)\n",
    "top_similar = pairs_details.nlargest(10, 'cosine_similarity')\n",
    "for i, (idx, row) in enumerate(top_similar.iterrows(), 1):\n",
    "    title_a = row['title_a']\n",
    "    title_b = row['title_b']\n",
    "\n",
    "    # Truncar se muito longo\n",
    "    if len(title_a) > 50:\n",
    "        title_a = title_a[:50] + \"...\"\n",
    "    if len(title_b) > 50:\n",
    "        title_b = title_b[:50] + \"...\"\n",
    "\n",
    "    print(f\"#{i:2d} | Similaridade: {row['cosine_similarity']:.4f}\")\n",
    "    print(f\"     A: {title_a}\")\n",
    "    print(f\"     B: {title_b}\")\n",
    "    print()\n",
    "\n",
    "# An√°lise de autores em duplicatas (se coluna existir)\n",
    "if 'authors' in df.columns:\n",
    "    print(f\"üë§ An√°lise de Autores em Duplicatas:\")\n",
    "    duplicated_ids = [id for members in clusters.values() if len(members) > 1 for id in members]\n",
    "    duplicated_books = df[df[id_col].isin(duplicated_ids)]\n",
    "\n",
    "    # Autores com mais duplicatas\n",
    "    author_dupes = duplicated_books['authors'].value_counts().head(10)\n",
    "    print(f\"\\n   Top 10 autores com mais duplicatas:\")\n",
    "    for author, count in author_dupes.items():\n",
    "        author_display = author[:40] + \"...\" if len(str(author)) > 40 else author\n",
    "        print(f\"   {author_display}: {count} livros duplicados\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 8. EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ EXPORTANDO RESULTADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Exportar pares\n",
    "pairs_file = OUTPUT_DIR / \"title_near_duplicates_pairs.csv\"\n",
    "pairs_details.to_csv(pairs_file, index=False)\n",
    "print(f\"‚úÖ Pares exportados: {pairs_file}\")\n",
    "print(f\"   {len(pairs_details):,} pares salvos\")\n",
    "\n",
    "# Exportar clusters\n",
    "clusters_file = OUTPUT_DIR / \"title_near_duplicates_clusters.csv\"\n",
    "clusters_df.to_csv(clusters_file, index=False)\n",
    "print(f\"‚úÖ Clusters exportados: {clusters_file}\")\n",
    "print(f\"   {len(clusters_df):,} clusters salvos\")\n",
    "\n",
    "# Exportar livros duplicados com detalhes\n",
    "duplicated_ids = [id for members in clusters.values() if len(members) > 1 for id in members]\n",
    "duplicated_books = df[df[id_col].isin(duplicated_ids)].copy()\n",
    "\n",
    "# Adicionar informa√ß√£o do cluster\n",
    "id_to_cluster = {}\n",
    "for cluster_id, members in clusters.items():\n",
    "    if len(members) > 1:\n",
    "        for member_id in members:\n",
    "            id_to_cluster[member_id] = cluster_id\n",
    "\n",
    "duplicated_books['cluster_id'] = duplicated_books[id_col].map(id_to_cluster)\n",
    "duplicated_books['cluster_size'] = duplicated_books['cluster_id'].map(\n",
    "    {k: len(v) for k, v in clusters.items()}\n",
    ")\n",
    "\n",
    "duplicated_books_sorted = duplicated_books.sort_values(['cluster_size', 'cluster_id'], ascending=[False, True])\n",
    "duplicated_file = OUTPUT_DIR / \"books_with_duplicates.csv\"\n",
    "duplicated_books_sorted.to_csv(duplicated_file, index=False)\n",
    "print(f\"‚úÖ Livros duplicados exportados: {duplicated_file}\")\n",
    "print(f\"   {len(duplicated_books_sorted):,} livros com duplicatas salvos\")\n",
    "\n",
    "# Exportar resumo estat√≠stico\n",
    "stats = {\n",
    "    \"M√©trica\": [\n",
    "        \"Total de livros\",\n",
    "        \"Livros √∫nicos\",\n",
    "        \"Livros com duplicatas\",\n",
    "        \"% com duplicatas\",\n",
    "        \"Total de clusters\",\n",
    "        \"Maior cluster\",\n",
    "        \"M√©dia de livros por cluster\",\n",
    "        \"Total de pares similares\",\n",
    "        \"Similaridade m√©dia\"\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        total_livros,\n",
    "        livros_unicos,\n",
    "        livros_com_duplicatas,\n",
    "        f\"{livros_com_duplicatas/total_livros*100:.1f}%\",\n",
    "        len(clusters_df),\n",
    "        clusters_df['size'].max(),\n",
    "        f\"{clusters_df['size'].mean():.1f}\",\n",
    "        len(pairs_df),\n",
    "        f\"{pairs_df['cosine_similarity'].mean():.3f}\"\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_file = OUTPUT_DIR / \"duplicates_statistics.csv\"\n",
    "stats_df.to_csv(stats_file, index=False)\n",
    "print(f\"‚úÖ Estat√≠sticas exportadas: {stats_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ AN√ÅLISE CONCLU√çDA!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìÅ Todos os arquivos foram salvos em: {OUTPUT_DIR.absolute()}\")"
   ],
   "id": "8fe428b6af003ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\data\\book1-100k.csv\n",
      "Pasta de exporta√ß√£o: exports\\duplicatas\n",
      "\n",
      "‚úÖ Coluna de t√≠tulo: 'name'\n",
      "‚úÖ Coluna de ID: 'id'\n",
      "üìä Total de livros: 58,292\n",
      "\n",
      "üìù Exemplo de t√≠tulos normalizados:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                name  \\\n",
       "0  Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1  Harry Potter and the Order of the Phoenix (Har...   \n",
       "2  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "3  Harry Potter and the Chamber of Secrets (Harry...   \n",
       "4  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "5  Harry Potter and the Goblet of Fire (Harry Pot...   \n",
       "6  Harry Potter Boxed Set, Books 1-5 (Harry Potte...   \n",
       "7  Unauthorized Harry Potter Book Seven News: \"Ha...   \n",
       "8       Harry Potter Collection (Harry Potter, #1-6)   \n",
       "9  The Ultimate Hitchhiker's Guide: Five Complete...   \n",
       "\n",
       "                                      __title_norm__  \n",
       "0  harry potter and the half blood prince harry p...  \n",
       "1  harry potter and the order of the phoenix harr...  \n",
       "2  harry potter and the sorcerer s stone harry po...  \n",
       "3  harry potter and the chamber of secrets harry ...  \n",
       "4  harry potter and the prisoner of azkaban harry...  \n",
       "5  harry potter and the goblet of fire harry pott...  \n",
       "6  harry potter boxed set books 1 5 harry potter 1 5  \n",
       "7  unauthorized harry potter book seven news half...  \n",
       "8           harry potter collection harry potter 1 6  \n",
       "9  the ultimate hitchhiker s guide five complete ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>__title_norm__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>harry potter and the sorcerer s stone harry po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>harry potter and the chamber of secrets harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>harry potter and the prisoner of azkaban harry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>harry potter and the goblet of fire harry pott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>harry potter boxed set books 1 5 harry potter 1 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Unauthorized Harry Potter Book Seven News: \"Ha...</td>\n",
       "      <td>unauthorized harry potter book seven news half...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Harry Potter Collection (Harry Potter, #1-6)</td>\n",
       "      <td>harry potter collection harry potter 1 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>the ultimate hitchhiker s guide five complete ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Calculando similaridades...\n",
      "‚úÖ Matriz de vizinhos calculada: (58292, 5)\n",
      "\n",
      "‚úÖ Total de pares similares encontrados: 3,074\n",
      "\n",
      "üìã Amostra de pares similares:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    id_a   id_b  cosine_similarity  \\\n",
       "0      1  70367           1.000000   \n",
       "1      1  93124           1.000000   \n",
       "2      2  77522           1.000000   \n",
       "3      2  70356           0.875292   \n",
       "4      3  77523           0.902182   \n",
       "5     12     18           0.863741   \n",
       "6     13     14           0.943693   \n",
       "7     13     18           0.930903   \n",
       "8     13  79260           0.886023   \n",
       "9     13  79259           0.886023   \n",
       "10    14  17059           0.940345   \n",
       "11    14  79259           0.940345   \n",
       "12    14  79260           0.940345   \n",
       "13    18  17059           0.867549   \n",
       "14    18  79260           0.867549   \n",
       "15    18  79259           0.867549   \n",
       "16    28  10540           1.000000   \n",
       "17    31  15247           0.910844   \n",
       "18    34  92671           1.000000   \n",
       "19    34    119           0.908473   \n",
       "20    53  52539           1.000000   \n",
       "21    53  69941           0.978680   \n",
       "22    58  28807           0.901075   \n",
       "23    59  77354           1.000000   \n",
       "24    61  28807           0.873253   \n",
       "25    74     82           0.932489   \n",
       "26    82  19895           0.931149   \n",
       "27    98   6930           0.908762   \n",
       "28   103  42432           1.000000   \n",
       "29   105  53767           1.000000   \n",
       "\n",
       "                                         title_norm_a  \\\n",
       "0   harry potter and the half blood prince harry p...   \n",
       "1   harry potter and the half blood prince harry p...   \n",
       "2   harry potter and the order of the phoenix harr...   \n",
       "3   harry potter and the order of the phoenix harr...   \n",
       "4   harry potter and the sorcerer s stone harry po...   \n",
       "5   the ultimate hitchhiker s guide five complete ...   \n",
       "6   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "7   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "8   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "9   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "10  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "11  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "12  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "13  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "14  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "15  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "16                          notes from a small island   \n",
       "17    the lord of the rings the lord of the rings 1 3   \n",
       "18  the fellowship of the ring the lord of the rin...   \n",
       "19  the fellowship of the ring the lord of the rin...   \n",
       "20  guts the true stories behind hatchet and the b...   \n",
       "21  guts the true stories behind hatchet and the b...   \n",
       "22                            changeling changeling 1   \n",
       "23                                 the changeling sea   \n",
       "24                                     the changeling   \n",
       "25        the john mcphee reader john mcphee reader 1   \n",
       "26  the second john mcphee reader john mcphee read...   \n",
       "27       what to expect the first year what to expect   \n",
       "28              god emperor of dune dune chronicles 4   \n",
       "29                chapterhouse dune dune chronicles 6   \n",
       "\n",
       "                                              title_a  \\\n",
       "0   Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "2   Harry Potter and the Order of the Phoenix (Har...   \n",
       "3   Harry Potter and the Order of the Phoenix (Har...   \n",
       "4   Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "5   The Ultimate Hitchhiker's Guide: Five Complete...   \n",
       "6   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "7   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "8   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "9   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "10  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "11  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "12  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "13  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "14  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "15  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "16                          Notes from a Small Island   \n",
       "17  The Lord of the Rings (The Lord of the Rings, ...   \n",
       "18  The Fellowship of the Ring (The Lord of the Ri...   \n",
       "19  The Fellowship of the Ring (The Lord of the Ri...   \n",
       "20  Guts: The True Stories behind Hatchet and the ...   \n",
       "21  Guts: The True Stories behind Hatchet and the ...   \n",
       "22                        Changeling (Changeling, #1)   \n",
       "23                                 The Changeling Sea   \n",
       "24                                     The Changeling   \n",
       "25    The John McPhee Reader (John McPhee Reader, #1)   \n",
       "26  The Second John McPhee Reader (John McPhee Rea...   \n",
       "27     What to Expect the First Year (What to Expect)   \n",
       "28          God Emperor of Dune (Dune Chronicles, #4)   \n",
       "29            Chapterhouse: Dune (Dune Chronicles #6)   \n",
       "\n",
       "                                         title_norm_b  \\\n",
       "0   harry potter and the half blood prince harry p...   \n",
       "1   harry potter and the half blood prince harry p...   \n",
       "2   harry potter and the order of the phoenix harr...   \n",
       "3           harry potter and the order of the phoenix   \n",
       "4               harry potter and the sorcerer s stone   \n",
       "5   the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "6   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "7   the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "8   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "9   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "10  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "11  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "12  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "13  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "14  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "15  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "16                          notes from a small island   \n",
       "17                              the lord of the rings   \n",
       "18  the fellowship of the ring the lord of the rin...   \n",
       "19  the lord of the rings the art of the fellowshi...   \n",
       "20  guts the true stories behind hatchet and the b...   \n",
       "21  guts the true stories behind hatchet and the b...   \n",
       "22                                         changeling   \n",
       "23                                 the changeling sea   \n",
       "24                                         changeling   \n",
       "25  the second john mcphee reader john mcphee read...   \n",
       "26                      the second john mcphee reader   \n",
       "27                      what to expect the first year   \n",
       "28              god emperor of dune dune chronicles 4   \n",
       "29                chapterhouse dune dune chronicles 6   \n",
       "\n",
       "                                              title_b  \n",
       "0   Harry Potter and the Half-blood Prince (Harry ...  \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...  \n",
       "2   Harry Potter and the Order of the Phoenix (Har...  \n",
       "3           Harry Potter and the Order of the Phoenix  \n",
       "4               Harry Potter and the Sorcerer's Stone  \n",
       "5   The Ultimate Hitchhiker's Guide (Hitchhiker's ...  \n",
       "6   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "7   The Ultimate Hitchhiker's Guide (Hitchhiker's ...  \n",
       "8   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "9   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "10  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "11  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "12  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "13  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "14  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "15  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "16                          Notes From A Small Island  \n",
       "17                              The Lord of the Rings  \n",
       "18  The Fellowship of the Ring (The Lord of the Ri...  \n",
       "19  The Lord of the Rings: The Art of the Fellowsh...  \n",
       "20  Guts: The True Stories Behind Hatchet and the ...  \n",
       "21  Guts: The True Stories Behind Hatchet and the ...  \n",
       "22                                         Changeling  \n",
       "23                                 The Changeling Sea  \n",
       "24                                         Changeling  \n",
       "25  The Second John McPhee Reader (John McPhee Rea...  \n",
       "26                      The Second John McPhee Reader  \n",
       "27                      What to Expect the First Year  \n",
       "28           God Emperor of Dune (Dune Chronicles #4)  \n",
       "29            Chapterhouse Dune (Dune Chronicles, #6)  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>title_norm_a</th>\n",
       "      <th>title_a</th>\n",
       "      <th>title_norm_b</th>\n",
       "      <th>title_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>70367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>93124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>77522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>70356</td>\n",
       "      <td>0.875292</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>77523</td>\n",
       "      <td>0.902182</td>\n",
       "      <td>harry potter and the sorcerer s stone harry po...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>harry potter and the sorcerer s stone</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0.863741</td>\n",
       "      <td>the ultimate hitchhiker s guide five complete ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0.943693</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0.930903</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.886023</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.886023</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>17059</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>17059</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>10540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>notes from a small island</td>\n",
       "      <td>Notes from a Small Island</td>\n",
       "      <td>notes from a small island</td>\n",
       "      <td>Notes From A Small Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31</td>\n",
       "      <td>15247</td>\n",
       "      <td>0.910844</td>\n",
       "      <td>the lord of the rings the lord of the rings 1 3</td>\n",
       "      <td>The Lord of the Rings (The Lord of the Rings, ...</td>\n",
       "      <td>the lord of the rings</td>\n",
       "      <td>The Lord of the Rings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34</td>\n",
       "      <td>92671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34</td>\n",
       "      <td>119</td>\n",
       "      <td>0.908473</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>the lord of the rings the art of the fellowshi...</td>\n",
       "      <td>The Lord of the Rings: The Art of the Fellowsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>53</td>\n",
       "      <td>52539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories behind Hatchet and the ...</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories Behind Hatchet and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53</td>\n",
       "      <td>69941</td>\n",
       "      <td>0.978680</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories behind Hatchet and the ...</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories Behind Hatchet and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>58</td>\n",
       "      <td>28807</td>\n",
       "      <td>0.901075</td>\n",
       "      <td>changeling changeling 1</td>\n",
       "      <td>Changeling (Changeling, #1)</td>\n",
       "      <td>changeling</td>\n",
       "      <td>Changeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59</td>\n",
       "      <td>77354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>the changeling sea</td>\n",
       "      <td>The Changeling Sea</td>\n",
       "      <td>the changeling sea</td>\n",
       "      <td>The Changeling Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61</td>\n",
       "      <td>28807</td>\n",
       "      <td>0.873253</td>\n",
       "      <td>the changeling</td>\n",
       "      <td>The Changeling</td>\n",
       "      <td>changeling</td>\n",
       "      <td>Changeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>74</td>\n",
       "      <td>82</td>\n",
       "      <td>0.932489</td>\n",
       "      <td>the john mcphee reader john mcphee reader 1</td>\n",
       "      <td>The John McPhee Reader (John McPhee Reader, #1)</td>\n",
       "      <td>the second john mcphee reader john mcphee read...</td>\n",
       "      <td>The Second John McPhee Reader (John McPhee Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82</td>\n",
       "      <td>19895</td>\n",
       "      <td>0.931149</td>\n",
       "      <td>the second john mcphee reader john mcphee read...</td>\n",
       "      <td>The Second John McPhee Reader (John McPhee Rea...</td>\n",
       "      <td>the second john mcphee reader</td>\n",
       "      <td>The Second John McPhee Reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>98</td>\n",
       "      <td>6930</td>\n",
       "      <td>0.908762</td>\n",
       "      <td>what to expect the first year what to expect</td>\n",
       "      <td>What to Expect the First Year (What to Expect)</td>\n",
       "      <td>what to expect the first year</td>\n",
       "      <td>What to Expect the First Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>103</td>\n",
       "      <td>42432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>god emperor of dune dune chronicles 4</td>\n",
       "      <td>God Emperor of Dune (Dune Chronicles, #4)</td>\n",
       "      <td>god emperor of dune dune chronicles 4</td>\n",
       "      <td>God Emperor of Dune (Dune Chronicles #4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>105</td>\n",
       "      <td>53767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>chapterhouse dune dune chronicles 6</td>\n",
       "      <td>Chapterhouse: Dune (Dune Chronicles #6)</td>\n",
       "      <td>chapterhouse dune dune chronicles 6</td>\n",
       "      <td>Chapterhouse Dune (Dune Chronicles, #6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Agrupando duplicatas em clusters...\n",
      "‚úÖ Clusters detectados: 2,060\n",
      "\n",
      "======================================================================\n",
      "üìä ESTAT√çSTICAS DE DUPLICATAS\n",
      "======================================================================\n",
      "\n",
      "üìö Vis√£o Geral:\n",
      "   Total de livros no dataset: 58,292\n",
      "   Livros √∫nicos (sem duplicatas): 53,629 (92.0%)\n",
      "   Livros com duplicatas: 4,663 (8.0%)\n",
      "   Total de clusters de duplicatas: 2,060\n",
      "\n",
      "üîó Clusters:\n",
      "   Maior cluster: 19 livros\n",
      "   Menor cluster: 2 livros\n",
      "   M√©dia de livros por cluster: 2.3\n",
      "   Mediana: 2\n",
      "\n",
      "üìä Distribui√ß√£o de Tamanhos de Clusters:\n",
      "   2 duplicatas: 1,753 clusters\n",
      "   3 duplicatas: 208 clusters\n",
      "   4 duplicatas: 52 clusters\n",
      "   5 duplicatas: 19 clusters\n",
      "   6 duplicatas: 9 clusters\n",
      "   7 duplicatas: 5 clusters\n",
      "   8 duplicatas: 5 clusters\n",
      "   9 duplicatas: 4 clusters\n",
      "   10 duplicatas: 2 clusters\n",
      "   11 duplicatas: 1 clusters\n",
      "\n",
      "üèÜ TOP 10 MAIORES CLUSTERS DE DUPLICATAS:\n",
      "----------------------------------------------------------------------\n",
      "#65 |  19 livros | Cluster    866\n",
      "     T√≠tulo: Fullmetal Alchemist, Vol. 9 (Fullmetal Alchemist, #9)\n",
      "     Varia√ß√µes: 19 t√≠tulos diferentes no cluster\n",
      "\n",
      "#1658 |  15 livros | Cluster  74949\n",
      "     T√≠tulo: Angel Sanctuary, Vol. 14\n",
      "     Varia√ß√µes: 15 t√≠tulos diferentes no cluster\n",
      "\n",
      "#854 |  11 livros | Cluster  23708\n",
      "     T√≠tulo: Maison Ikkoku, Volume 1 (Maison Ikkoku, #1)\n",
      "     Varia√ß√µes: 11 t√≠tulos diferentes no cluster\n",
      "\n",
      "#565 |  10 livros | Cluster  13026\n",
      "     T√≠tulo: Alice In Wonderland: Including Alice's Adventures In Wonderl...\n",
      "     Varia√ß√µes: 8 t√≠tulos diferentes no cluster\n",
      "\n",
      "#151 |  10 livros | Cluster  13731\n",
      "     T√≠tulo: Bleach, Volume 15\n",
      "     Varia√ß√µes: 10 t√≠tulos diferentes no cluster\n",
      "\n",
      "#584 |   9 livros | Cluster  13568\n",
      "     T√≠tulo: Tsubasa: RESERVoir CHRoNiCLE, Vol. 11\n",
      "     Varia√ß√µes: 9 t√≠tulos diferentes no cluster\n",
      "\n",
      "#1283 |   9 livros | Cluster  69547\n",
      "     T√≠tulo: The Best American Poetry 2006\n",
      "     Varia√ß√µes: 9 t√≠tulos diferentes no cluster\n",
      "\n",
      "#653 |   9 livros | Cluster  15592\n",
      "     T√≠tulo: McSweeney's #12\n",
      "     Varia√ß√µes: 9 t√≠tulos diferentes no cluster\n",
      "\n",
      "# 4 |   9 livros | Cluster  24956\n",
      "     T√≠tulo: The Ultimate Hitchhiker's Guide: Five Complete Novels and On...\n",
      "     Varia√ß√µes: 8 t√≠tulos diferentes no cluster\n",
      "\n",
      "#705 |   8 livros | Cluster  17434\n",
      "     T√≠tulo: Physics for Scientists and Engineers with Modern Physics: Vo...\n",
      "     Varia√ß√µes: 8 t√≠tulos diferentes no cluster\n",
      "\n",
      "üìà Similaridade dos Pares:\n",
      "   Total de pares encontrados: 3,074\n",
      "   Similaridade m√©dia: 0.940\n",
      "   Similaridade m√≠nima: 0.850\n",
      "   Similaridade m√°xima: 1.000\n",
      "\n",
      "üìä Distribui√ß√£o de Similaridade:\n",
      "   0.85-0.90: 860 pares (28.0%)\n",
      "   0.90-0.95: 838 pares (27.3%)\n",
      "   0.95-0.98: 269 pares (8.8%)\n",
      "   0.98-1.00: 1,107 pares (36.0%)\n",
      "\n",
      "üéØ TOP 10 PARES MAIS SIMILARES:\n",
      "----------------------------------------------------------------------\n",
      "# 1 | Similaridade: 1.0000\n",
      "     A: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "     B: Harry Potter and the Half-blood Prince (Harry Pott...\n",
      "\n",
      "# 2 | Similaridade: 1.0000\n",
      "     A: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "     B: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "\n",
      "# 3 | Similaridade: 1.0000\n",
      "     A: Notes from a Small Island\n",
      "     B: Notes From A Small Island\n",
      "\n",
      "# 4 | Similaridade: 1.0000\n",
      "     A: The Fellowship of the Ring (The Lord of the Rings,...\n",
      "     B: The Fellowship of the Ring (The Lord of the Rings,...\n",
      "\n",
      "# 5 | Similaridade: 1.0000\n",
      "     A: The Changeling Sea\n",
      "     B: The Changeling Sea\n",
      "\n",
      "# 6 | Similaridade: 1.0000\n",
      "     A: God Emperor of Dune (Dune Chronicles, #4)\n",
      "     B: God Emperor of Dune (Dune Chronicles #4)\n",
      "\n",
      "# 7 | Similaridade: 1.0000\n",
      "     A: Heretics of Dune (Dune Chronicles, #5)\n",
      "     B: Heretics of Dune (Dune Chronicles #5)\n",
      "\n",
      "# 8 | Similaridade: 1.0000\n",
      "     A: Treasure Island\n",
      "     B: Treasure Island\n",
      "\n",
      "# 9 | Similaridade: 1.0000\n",
      "     A: Stranger in a Strange Land\n",
      "     B: Stranger in a Strange Land\n",
      "\n",
      "#10 | Similaridade: 1.0000\n",
      "     A: Time Enough for Love\n",
      "     B: Time Enough for Love\n",
      "\n",
      "üë§ An√°lise de Autores em Duplicatas:\n",
      "\n",
      "   Top 10 autores com mais duplicatas:\n",
      "   Rumiko Takahashi: 44 livros duplicados\n",
      "   J.R.R. Tolkien: 42 livros duplicados\n",
      "   William Shakespeare: 36 livros duplicados\n",
      "   John Grisham: 34 livros duplicados\n",
      "   Lawrence Block: 28 livros duplicados\n",
      "   James Patterson: 27 livros duplicados\n",
      "   Orson Scott Card: 26 livros duplicados\n",
      "   Alexander McCall Smith: 23 livros duplicados\n",
      "   Dave Eggers: 23 livros duplicados\n",
      "   Colin Dexter: 22 livros duplicados\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "üíæ EXPORTANDO RESULTADOS\n",
      "======================================================================\n",
      "‚úÖ Pares exportados: exports\\duplicatas\\title_near_duplicates_pairs.csv\n",
      "   3,094 pares salvos\n",
      "‚úÖ Clusters exportados: exports\\duplicatas\\title_near_duplicates_clusters.csv\n",
      "   2,060 clusters salvos\n",
      "‚úÖ Livros duplicados exportados: exports\\duplicatas\\books_with_duplicates.csv\n",
      "   4,663 livros com duplicatas salvos\n",
      "‚úÖ Estat√≠sticas exportadas: exports\\duplicatas\\duplicates_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "‚úÖ AN√ÅLISE CONCLU√çDA!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Todos os arquivos foram salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\duplicatas\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cria√ß√£o de Dataset Limpo",
   "id": "f02fd67d4bc05111"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:47:02.246242Z",
     "start_time": "2025-10-27T18:46:58.341797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Remove duplicatas de ISBN, t√≠tulos e anos inv√°lidos\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "# Criar pasta para datasets limpos\n",
    "OUTPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üßπ LIMPEZA DE DATASET - REMO√á√ÉO DE DUPLICATAS E ANOS INV√ÅLIDOS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÅ Pasta de sa√≠da: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS ORIGINAIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìÇ Carregando dataset original...\")\n",
    "# O DataFrame df j√° deve estar carregado no notebook\n",
    "# Se n√£o estiver, descomente a linha abaixo:\n",
    "# df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "df_original = df.copy()  # Fazer c√≥pia para preservar original\n",
    "print(f\"‚úÖ Dataset carregado: {len(df_original):,} livros\")\n",
    "print(f\"   Colunas: {list(df_original.columns)}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. IDENTIFICAR REGISTROS A REMOVER\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç Identificando registros para remo√ß√£o...\\n\")\n",
    "\n",
    "# Inicializar m√°scaras\n",
    "ids_to_remove = set()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.1. ANOS INV√ÅLIDOS (> 2020)\n",
    "# -----------------------------\n",
    "year_col = \"publishyear\"\n",
    "\n",
    "if year_col in df_original.columns:\n",
    "    invalid_years_mask = df_original[year_col] > 2020\n",
    "    invalid_years_count = invalid_years_mask.sum()\n",
    "    invalid_years_ids = df_original[invalid_years_mask]['id'].tolist()\n",
    "    ids_to_remove.update(invalid_years_ids)\n",
    "\n",
    "    print(f\"üìÖ Anos Inv√°lidos (> 2020):\")\n",
    "    print(f\"   Encontrados: {invalid_years_count:,} livros\")\n",
    "\n",
    "    if invalid_years_count > 0:\n",
    "        years_dist = df_original[invalid_years_mask][year_col].value_counts().sort_index(ascending=False)\n",
    "        print(f\"   Distribui√ß√£o:\")\n",
    "        for year, count in years_dist.head(5).items():\n",
    "            print(f\"      {int(year)}: {count:,} livros\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Coluna '{year_col}' n√£o encontrada - pulando valida√ß√£o de anos\")\n",
    "    invalid_years_count = 0\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.2. DUPLICATAS DE ISBN\n",
    "# -----------------------------\n",
    "isbn_col = \"isbn\"\n",
    "\n",
    "if isbn_col in df_original.columns:\n",
    "    # Considerar apenas ISBNs n√£o nulos\n",
    "    df_with_isbn = df_original[df_original[isbn_col].notna()]\n",
    "\n",
    "    # Encontrar ISBNs duplicados\n",
    "    isbn_counts = df_with_isbn[isbn_col].value_counts()\n",
    "    duplicate_isbns = isbn_counts[isbn_counts > 1].index.tolist()\n",
    "\n",
    "    # Para cada ISBN duplicado, manter apenas o primeiro registro\n",
    "    isbn_duplicate_ids = []\n",
    "    for isbn in duplicate_isbns:\n",
    "        duplicate_records = df_original[df_original[isbn_col] == isbn]\n",
    "        # Manter o primeiro, remover os demais\n",
    "        ids_to_keep_one = duplicate_records['id'].iloc[0]\n",
    "        ids_to_remove_from_isbn = duplicate_records['id'].iloc[1:].tolist()\n",
    "        isbn_duplicate_ids.extend(ids_to_remove_from_isbn)\n",
    "\n",
    "    ids_to_remove.update(isbn_duplicate_ids)\n",
    "\n",
    "    print(f\"üìö Duplicatas de ISBN:\")\n",
    "    print(f\"   ISBNs duplicados: {len(duplicate_isbns):,}\")\n",
    "    print(f\"   Registros a remover: {len(isbn_duplicate_ids):,}\")\n",
    "    print(f\"   Registros √∫nicos mantidos: {len(duplicate_isbns):,}\")\n",
    "\n",
    "    if len(duplicate_isbns) > 0:\n",
    "        print(f\"   Top 5 ISBNs mais duplicados:\")\n",
    "        for isbn, count in isbn_counts.head(5).items():\n",
    "            if count > 1:\n",
    "                print(f\"      {isbn}: {count} ocorr√™ncias\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Coluna '{isbn_col}' n√£o encontrada - pulando valida√ß√£o de ISBN\")\n",
    "    len_isbn_duplicate_ids = 0\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.3. DUPLICATAS DE T√çTULO\n",
    "# -----------------------------\n",
    "# Nota: Assumindo que voc√™ j√° rodou a an√°lise de duplicatas de t√≠tulo\n",
    "# e tem os clusters salvos. Vamos carregar se existir.\n",
    "\n",
    "title_clusters_file = Path(\"exports/duplicatas/title_near_duplicates_clusters.csv\")\n",
    "\n",
    "if title_clusters_file.exists():\n",
    "    print(f\"üìñ Duplicatas de T√≠tulo:\")\n",
    "    clusters_df = pd.read_csv(title_clusters_file)\n",
    "\n",
    "    # Para cada cluster, manter apenas o primeiro ID\n",
    "    title_duplicate_ids = []\n",
    "    for _, row in clusters_df.iterrows():\n",
    "        members = eval(row['members'])  # Converter string de lista para lista\n",
    "        # Manter o primeiro, remover os demais\n",
    "        title_duplicate_ids.extend(members[1:])\n",
    "\n",
    "    ids_to_remove.update(title_duplicate_ids)\n",
    "\n",
    "    print(f\"   Clusters encontrados: {len(clusters_df):,}\")\n",
    "    print(f\"   Registros a remover: {len(title_duplicate_ids):,}\")\n",
    "    print(f\"   Registros √∫nicos mantidos: {len(clusters_df):,}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Arquivo de clusters n√£o encontrado: {title_clusters_file}\")\n",
    "    print(f\"   Execute a an√°lise de duplicatas de t√≠tulo primeiro\")\n",
    "    print(f\"   Pulando remo√ß√£o de duplicatas de t√≠tulo...\")\n",
    "    title_duplicate_ids = []\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 4. REMOVER REGISTROS E GERAR DATASET LIMPO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üßπ GERANDO DATASET LIMPO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Criar m√°scara de IDs a manter\n",
    "df_clean = df_original[~df_original['id'].isin(ids_to_remove)].copy()\n",
    "\n",
    "print(f\"\\nüìä Resumo da Limpeza:\")\n",
    "print(f\"   Dataset original: {len(df_original):,} livros\")\n",
    "print(f\"   \")\n",
    "print(f\"   Removidos por:\")\n",
    "print(f\"      Anos inv√°lidos: {invalid_years_count:,}\")\n",
    "print(f\"      Duplicatas de ISBN: {len(isbn_duplicate_ids) if 'isbn_duplicate_ids' in locals() else 0:,}\")\n",
    "print(f\"      Duplicatas de t√≠tulo: {len(title_duplicate_ids):,}\")\n",
    "print(f\"   \")\n",
    "print(f\"   Total de IDs √∫nicos a remover: {len(ids_to_remove):,}\")\n",
    "print(f\"   Dataset limpo: {len(df_clean):,} livros\")\n",
    "print(f\"   \")\n",
    "print(f\"   Taxa de remo√ß√£o: {len(ids_to_remove)/len(df_original)*100:.2f}%\")\n",
    "print(f\"   Taxa de reten√ß√£o: {len(df_clean)/len(df_original)*100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. VALIDA√á√ïES DO DATASET LIMPO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n‚úÖ Valida√ß√µes do Dataset Limpo:\")\n",
    "\n",
    "# Validar anos\n",
    "if year_col in df_clean.columns:\n",
    "    max_year = df_clean[year_col].max()\n",
    "    min_year = df_clean[year_col].min()\n",
    "    print(f\"   Anos: {min_year:.0f} - {max_year:.0f}\")\n",
    "    invalid_after = (df_clean[year_col] > 2020).sum()\n",
    "    print(f\"   Anos > 2020: {invalid_after} (deve ser 0)\")\n",
    "\n",
    "# Validar ISBNs duplicados\n",
    "if isbn_col in df_clean.columns:\n",
    "    isbn_dupes_after = df_clean[isbn_col].duplicated().sum()\n",
    "    print(f\"   ISBNs duplicados: {isbn_dupes_after} (deve ser 0)\")\n",
    "\n",
    "# Validar IDs √∫nicos\n",
    "id_dupes = df_clean['id'].duplicated().sum()\n",
    "print(f\"   IDs duplicados: {id_dupes} (deve ser 0)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 6. EXPORTAR DATASETS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ EXPORTANDO DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dataset limpo principal\n",
    "clean_file = OUTPUT_DIR / \"book_data_clean.csv\"\n",
    "df_clean.to_csv(clean_file, index=False)\n",
    "print(f\"\\n‚úÖ Dataset limpo salvo: {clean_file}\")\n",
    "print(f\"   {len(df_clean):,} livros | {len(df_clean.columns)} colunas\")\n",
    "\n",
    "# Dataset de registros removidos (para auditoria)\n",
    "df_removed = df_original[df_original['id'].isin(ids_to_remove)].copy()\n",
    "\n",
    "# Adicionar coluna indicando o motivo da remo√ß√£o\n",
    "df_removed['removal_reason'] = ''\n",
    "\n",
    "for idx, row in df_removed.iterrows():\n",
    "    reasons = []\n",
    "\n",
    "    if year_col in df_removed.columns and row[year_col] > 2020:\n",
    "        reasons.append('invalid_year')\n",
    "\n",
    "    if isbn_col in df_removed.columns and 'isbn_duplicate_ids' in locals():\n",
    "        if row['id'] in isbn_duplicate_ids:\n",
    "            reasons.append('duplicate_isbn')\n",
    "\n",
    "    if row['id'] in title_duplicate_ids:\n",
    "        reasons.append('duplicate_title')\n",
    "\n",
    "    df_removed.at[idx, 'removal_reason'] = '; '.join(reasons)\n",
    "\n",
    "removed_file = OUTPUT_DIR / \"book_data_removed.csv\"\n",
    "df_removed.to_csv(removed_file, index=False)\n",
    "print(f\"‚úÖ Registros removidos salvos: {removed_file}\")\n",
    "print(f\"   {len(df_removed):,} livros | Para auditoria\")\n",
    "\n",
    "# Estat√≠sticas da limpeza\n",
    "stats = {\n",
    "    \"M√©trica\": [\n",
    "        \"Dataset Original\",\n",
    "        \"Anos Inv√°lidos Removidos\",\n",
    "        \"Duplicatas ISBN Removidas\",\n",
    "        \"Duplicatas T√≠tulo Removidas\",\n",
    "        \"Total Removido (IDs √∫nicos)\",\n",
    "        \"Dataset Limpo\",\n",
    "        \"Taxa de Reten√ß√£o (%)\"\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        len(df_original),\n",
    "        invalid_years_count,\n",
    "        len(isbn_duplicate_ids) if 'isbn_duplicate_ids' in locals() else 0,\n",
    "        len(title_duplicate_ids),\n",
    "        len(ids_to_remove),\n",
    "        len(df_clean),\n",
    "        f\"{len(df_clean)/len(df_original)*100:.2f}\"\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_file = OUTPUT_DIR / \"cleaning_statistics.csv\"\n",
    "stats_df.to_csv(stats_file, index=False)\n",
    "print(f\"‚úÖ Estat√≠sticas salvas: {stats_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. COMPARA√á√ÉO ANTES/DEPOIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä COMPARA√á√ÉO ANTES vs DEPOIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìà Estat√≠sticas Gerais:\")\n",
    "print(f\"   {'M√©trica':<30} {'Antes':>12} {'Depois':>12} {'Diferen√ßa':>12}\")\n",
    "print(f\"   {'-'*30} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "\n",
    "metrics = [\n",
    "    ('Total de livros', len(df_original), len(df_clean)),\n",
    "    ('Colunas', len(df_original.columns), len(df_clean.columns)),\n",
    "]\n",
    "\n",
    "if year_col in df_original.columns:\n",
    "    metrics.append(('Ano m√©dio', df_original[year_col].mean(), df_clean[year_col].mean()))\n",
    "    metrics.append(('Ano m√≠nimo', df_original[year_col].min(), df_clean[year_col].min()))\n",
    "    metrics.append(('Ano m√°ximo', df_original[year_col].max(), df_clean[year_col].max()))\n",
    "\n",
    "if isbn_col in df_original.columns:\n",
    "    metrics.append(('ISBNs √∫nicos', df_original[isbn_col].nunique(), df_clean[isbn_col].nunique()))\n",
    "    metrics.append(('ISBNs nulos', df_original[isbn_col].isna().sum(), df_clean[isbn_col].isna().sum()))\n",
    "\n",
    "for metric, before, after in metrics:\n",
    "    diff = after - before\n",
    "    if isinstance(before, float) and isinstance(after, float):\n",
    "        print(f\"   {metric:<30} {before:>12.1f} {after:>12.1f} {diff:>+12.1f}\")\n",
    "    else:\n",
    "        print(f\"   {metric:<30} {before:>12,} {after:>12,} {diff:>+12,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ LIMPEZA CONCLU√çDA!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìÅ Arquivos salvos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\nVoc√™ pode usar o dataset limpo com:\")\n",
    "print(f\"   df_clean = pd.read_csv('{clean_file}')\")"
   ],
   "id": "56be59727f1e099a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üßπ LIMPEZA DE DATASET - REMO√á√ÉO DE DUPLICATAS E ANOS INV√ÅLIDOS\n",
      "======================================================================\n",
      "üìÅ Pasta de sa√≠da: exports\\clean_data\n",
      "\n",
      "üìÇ Carregando dataset original...\n",
      "‚úÖ Dataset carregado: 58,292 livros\n",
      "   Colunas: ['id', 'name', 'ratingdist1', 'pagesnumber', 'ratingdist4', 'ratingdisttotal', 'publishmonth', 'publishday', 'publisher', 'countsofreview', 'publishyear', 'language', 'authors', 'rating', 'ratingdist2', 'ratingdist5', 'isbn', 'ratingdist3', '__title_norm__']\n",
      "\n",
      "üîç Identificando registros para remo√ß√£o...\n",
      "\n",
      "üìÖ Anos Inv√°lidos (> 2020):\n",
      "   Encontrados: 1 livros\n",
      "   Distribui√ß√£o:\n",
      "      3002: 1 livros\n",
      "\n",
      "üìö Duplicatas de ISBN:\n",
      "   ISBNs duplicados: 194\n",
      "   Registros a remover: 194\n",
      "   Registros √∫nicos mantidos: 194\n",
      "   Top 5 ISBNs mais duplicados:\n",
      "      038072023X: 2 ocorr√™ncias\n",
      "      0380715732: 2 ocorr√™ncias\n",
      "      0380725355: 2 ocorr√™ncias\n",
      "      006103097X: 2 ocorr√™ncias\n",
      "      0380763621: 2 ocorr√™ncias\n",
      "\n",
      "üìñ Duplicatas de T√≠tulo:\n",
      "   Clusters encontrados: 2,060\n",
      "   Registros a remover: 2,603\n",
      "   Registros √∫nicos mantidos: 2,060\n",
      "\n",
      "======================================================================\n",
      "üßπ GERANDO DATASET LIMPO\n",
      "======================================================================\n",
      "\n",
      "üìä Resumo da Limpeza:\n",
      "   Dataset original: 58,292 livros\n",
      "   \n",
      "   Removidos por:\n",
      "      Anos inv√°lidos: 1\n",
      "      Duplicatas de ISBN: 194\n",
      "      Duplicatas de t√≠tulo: 2,603\n",
      "   \n",
      "   Total de IDs √∫nicos a remover: 2,595\n",
      "   Dataset limpo: 55,502 livros\n",
      "   \n",
      "   Taxa de remo√ß√£o: 4.45%\n",
      "   Taxa de reten√ß√£o: 95.21%\n",
      "\n",
      "‚úÖ Valida√ß√µes do Dataset Limpo:\n",
      "   Anos: 162 - 2020\n",
      "   Anos > 2020: 0 (deve ser 0)\n",
      "   ISBNs duplicados: 500 (deve ser 0)\n",
      "   IDs duplicados: 0 (deve ser 0)\n",
      "\n",
      "======================================================================\n",
      "üíæ EXPORTANDO DATASETS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Dataset limpo salvo: exports\\clean_data\\book_data_clean.csv\n",
      "   55,502 livros | 19 colunas\n",
      "‚úÖ Registros removidos salvos: exports\\clean_data\\book_data_removed.csv\n",
      "   2,790 livros | Para auditoria\n",
      "‚úÖ Estat√≠sticas salvas: exports\\clean_data\\cleaning_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "üìä COMPARA√á√ÉO ANTES vs DEPOIS\n",
      "======================================================================\n",
      "\n",
      "üìà Estat√≠sticas Gerais:\n",
      "   M√©trica                               Antes       Depois    Diferen√ßa\n",
      "   ------------------------------ ------------ ------------ ------------\n",
      "   Total de livros                      58,292       55,502       -2,790\n",
      "   Colunas                                  19           19           +0\n",
      "   Ano m√©dio                            1999.5       1999.6         +0.0\n",
      "   Ano m√≠nimo                              162          162           +0\n",
      "   Ano m√°ximo                            3,002        2,020         -982\n",
      "   ISBNs √∫nicos                         57,552       55,001       -2,551\n",
      "   ISBNs nulos                             546          501          -45\n",
      "\n",
      "======================================================================\n",
      "‚úÖ LIMPEZA CONCLU√çDA!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Arquivos salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\clean_data\n",
      "\n",
      "Voc√™ pode usar o dataset limpo com:\n",
      "   df_clean = pd.read_csv('exports\\clean_data\\book_data_clean.csv')\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## POC de cluster por similaridade de T√≠tulo, Autor, Ano de publica√ß√£o, Editora e l√≠ngua",
   "id": "2bdff6122d787e05"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-27T18:47:02.252651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Agrupa livros similares baseado em: t√≠tulo, autor, l√≠ngua, ano, publisher\n",
    "Usa K-Means, DBSCAN e Hierarchical Clustering\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar visualiza√ß√£o\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "# Pastas\n",
    "INPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR = Path(\"exports/clustering\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ü§ñ POC: CLUSTERING DE LIVROS SIMILARES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÇ Dataset: {INPUT_DIR}/book_data_clean.csv\")\n",
    "print(f\"üìÅ Sa√≠da: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS LIMPOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìÇ Carregando dataset limpo...\")\n",
    "clean_file = INPUT_DIR / \"book_data_clean.csv\"\n",
    "\n",
    "if not clean_file.exists():\n",
    "    print(f\"‚ùå Erro: Arquivo n√£o encontrado: {clean_file}\")\n",
    "    print(f\"   Execute primeiro o script de limpeza de dados!\")\n",
    "    raise FileNotFoundError(f\"Dataset limpo n√£o encontrado: {clean_file}\")\n",
    "\n",
    "df = pd.read_csv(clean_file)\n",
    "print(f\"‚úÖ Dataset carregado: {len(df):,} livros\")\n",
    "print(f\"   Colunas: {len(df.columns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. PREPARA√á√ÉO DOS DADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîß PREPARA√á√ÉO DOS DADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Selecionar colunas relevantes\n",
    "features_cols = {\n",
    "    'title': 'name',\n",
    "    'author': 'authors',\n",
    "    'language': 'language',\n",
    "    'year': 'publishyear',\n",
    "    'publisher': 'publisher'\n",
    "}\n",
    "\n",
    "# Verificar quais colunas existem\n",
    "available_cols = {}\n",
    "for key, col in features_cols.items():\n",
    "    if col in df.columns:\n",
    "        available_cols[key] = col\n",
    "        print(f\"‚úÖ {key}: '{col}'\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {key}: '{col}' n√£o encontrada\")\n",
    "\n",
    "# Criar DataFrame de trabalho com colunas dispon√≠veis\n",
    "df_work = df[list(available_cols.values()) + ['id']].copy()\n",
    "df_work = df_work.rename(columns={v: k for k, v in available_cols.items()})\n",
    "\n",
    "# Remover linhas com muitos valores nulos\n",
    "print(f\"\\nüìä Valores nulos antes da limpeza:\")\n",
    "null_counts = df_work.isnull().sum()\n",
    "for col, count in null_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {col}: {count:,} ({count/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "# Remover linhas onde t√≠tulo ou autor est√£o nulos\n",
    "df_work = df_work.dropna(subset=['title', 'author'])\n",
    "print(f\"\\n‚úÖ Ap√≥s remover nulos em t√≠tulo/autor: {len(df_work):,} livros\")\n",
    "\n",
    "# Preencher valores nulos restantes\n",
    "if 'language' in df_work.columns:\n",
    "    df_work['language'] = df_work['language'].fillna('unknown')\n",
    "if 'year' in df_work.columns:\n",
    "    df_work['year'] = df_work['year'].fillna(df_work['year'].median())\n",
    "if 'publisher' in df_work.columns:\n",
    "    df_work['publisher'] = df_work['publisher'].fillna('unknown')\n",
    "\n",
    "print(f\"‚úÖ Dataset final para clustering: {len(df_work):,} livros\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. ENGENHARIA DE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üî® ENGENHARIA DE FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 4.1 T√çTULO - TF-IDF\n",
    "print(\"\\nüìù Processando t√≠tulos com TF-IDF...\")\n",
    "title_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    lowercase=True,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "title_features = title_vectorizer.fit_transform(df_work['title'].astype(str))\n",
    "print(f\"‚úÖ T√≠tulo: {title_features.shape[1]} features\")\n",
    "\n",
    "# 4.2 AUTOR - TF-IDF\n",
    "print(\"üë§ Processando autores com TF-IDF...\")\n",
    "author_vectorizer = TfidfVectorizer(\n",
    "    max_features=50,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=2,\n",
    "    lowercase=True\n",
    ")\n",
    "author_features = author_vectorizer.fit_transform(df_work['author'].astype(str))\n",
    "print(f\"‚úÖ Autor: {author_features.shape[1]} features\")\n",
    "\n",
    "# 4.3 L√çNGUA - One-Hot Encoding\n",
    "if 'language' in df_work.columns:\n",
    "    print(\"üåç Processando l√≠nguas...\")\n",
    "    le_lang = LabelEncoder()\n",
    "    df_work['language_encoded'] = le_lang.fit_transform(df_work['language'].astype(str))\n",
    "    language_features = pd.get_dummies(df_work['language'], prefix='lang').values\n",
    "    print(f\"‚úÖ L√≠ngua: {language_features.shape[1]} features ({len(le_lang.classes_)} idiomas)\")\n",
    "else:\n",
    "    language_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.4 ANO - Normalizado\n",
    "if 'year' in df_work.columns:\n",
    "    print(\"üìÖ Processando anos...\")\n",
    "    scaler_year = StandardScaler()\n",
    "    year_features = scaler_year.fit_transform(df_work[['year']])\n",
    "    print(f\"‚úÖ Ano: normalizado ({df_work['year'].min():.0f} - {df_work['year'].max():.0f})\")\n",
    "else:\n",
    "    year_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.5 PUBLISHER - TF-IDF\n",
    "if 'publisher' in df_work.columns:\n",
    "    print(\"üè¢ Processando publishers...\")\n",
    "    publisher_vectorizer = TfidfVectorizer(\n",
    "        max_features=30,\n",
    "        min_df=3,\n",
    "        lowercase=True\n",
    "    )\n",
    "    publisher_features = publisher_vectorizer.fit_transform(df_work['publisher'].astype(str))\n",
    "    print(f\"‚úÖ Publisher: {publisher_features.shape[1]} features\")\n",
    "else:\n",
    "    publisher_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.6 COMBINAR TODAS AS FEATURES\n",
    "print(\"\\nüîó Combinando features...\")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Converter arrays densos para sparse\n",
    "language_sparse = csr_matrix(language_features)\n",
    "year_sparse = csr_matrix(year_features)\n",
    "\n",
    "# Combinar com pesos\n",
    "features_combined = hstack([\n",
    "    title_features * 3.0,        # Peso maior para t√≠tulo\n",
    "    author_features * 2.0,       # Peso m√©dio-alto para autor\n",
    "    language_sparse * 1.0,       # Peso normal para l√≠ngua\n",
    "    year_sparse * 0.5,           # Peso menor para ano\n",
    "    publisher_features * 0.5     # Peso menor para publisher\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Features combinadas: {features_combined.shape}\")\n",
    "print(f\"   Total de features: {features_combined.shape[1]}\")\n",
    "\n",
    "# Converter para array denso para clustering\n",
    "X = features_combined.toarray()\n",
    "\n",
    "# ============================================================\n",
    "# 5. REDU√á√ÉO DE DIMENSIONALIDADE (PCA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìâ REDU√á√ÉO DE DIMENSIONALIDADE (PCA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PCA para visualiza√ß√£o (2D e 3D)\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X)\n",
    "print(f\"‚úÖ PCA 2D: vari√¢ncia explicada = {pca_2d.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "print(f\"‚úÖ PCA 3D: vari√¢ncia explicada = {pca_3d.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# PCA para clustering (manter 95% da vari√¢ncia)\n",
    "pca_full = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca_full.fit_transform(X)\n",
    "print(f\"‚úÖ PCA clustering: {X_pca.shape[1]} componentes (95% vari√¢ncia)\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. DETERMINA√á√ÉO DO N√öMERO IDEAL DE CLUSTERS (ELBOW METHOD)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä DETERMINA√á√ÉO DO N√öMERO IDEAL DE CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüîç Testando diferentes n√∫meros de clusters (K-Means)...\")\n",
    "k_range = range(3, 15)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "db_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_pca, labels))\n",
    "    db_scores.append(davies_bouldin_score(X_pca, labels))\n",
    "\n",
    "    print(f\"   K={k:2d} | In√©rcia: {kmeans.inertia_:,.0f} | Silhouette: {silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "# Plotar curvas de avalia√ß√£o\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('In√©rcia', fontsize=12)\n",
    "axes[0].set_title('M√©todo do Cotovelo (Elbow Method)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score por K', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Threshold 0.5')\n",
    "axes[1].legend()\n",
    "\n",
    "# Davies-Bouldin score (menor √© melhor)\n",
    "axes[2].plot(k_range, db_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)\n",
    "axes[2].set_ylabel('Davies-Bouldin Score', fontsize=12)\n",
    "axes[2].set_title('Davies-Bouldin Score por K', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n‚úÖ Gr√°fico salvo: {OUTPUT_DIR / 'elbow_analysis.png'}\")\n",
    "\n",
    "# Sugerir melhor K\n",
    "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "best_k_db = k_range[np.argmin(db_scores)]\n",
    "print(f\"\\nüí° Sugest√µes de K:\")\n",
    "print(f\"   Melhor Silhouette Score: K={best_k_silhouette}\")\n",
    "print(f\"   Melhor Davies-Bouldin: K={best_k_db}\")\n",
    "\n",
    "# Usar K sugerido\n",
    "optimal_k = best_k_silhouette\n",
    "print(f\"\\n‚úÖ K escolhido para an√°lise: {optimal_k}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. APLICAR ALGORITMOS DE CLUSTERING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ü§ñ APLICANDO ALGORITMOS DE CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 7.1 K-MEANS\n",
    "print(f\"\\n1Ô∏è‚É£ K-Means (K={optimal_k})...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "df_work['cluster_kmeans'] = kmeans.fit_predict(X_pca)\n",
    "silhouette_kmeans = silhouette_score(X_pca, df_work['cluster_kmeans'])\n",
    "db_kmeans = davies_bouldin_score(X_pca, df_work['cluster_kmeans'])\n",
    "ch_kmeans = calinski_harabasz_score(X_pca, df_work['cluster_kmeans'])\n",
    "\n",
    "print(f\"   ‚úÖ Silhouette Score: {silhouette_kmeans:.3f}\")\n",
    "print(f\"   ‚úÖ Davies-Bouldin Score: {db_kmeans:.3f}\")\n",
    "print(f\"   ‚úÖ Calinski-Harabasz Score: {ch_kmeans:.1f}\")\n",
    "\n",
    "# 7.2 DBSCAN\n",
    "print(f\"\\n2Ô∏è‚É£ DBSCAN...\")\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=5, metric='euclidean')\n",
    "df_work['cluster_dbscan'] = dbscan.fit_predict(X_pca)\n",
    "n_clusters_dbscan = len(set(df_work['cluster_dbscan'])) - (1 if -1 in df_work['cluster_dbscan'] else 0)\n",
    "n_noise_dbscan = list(df_work['cluster_dbscan']).count(-1)\n",
    "\n",
    "print(f\"   ‚úÖ Clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"   ‚úÖ Ru√≠do (outliers): {n_noise_dbscan} livros ({n_noise_dbscan/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters_dbscan > 1:\n",
    "    mask = df_work['cluster_dbscan'] != -1\n",
    "    silhouette_dbscan = silhouette_score(X_pca[mask], df_work[mask]['cluster_dbscan'])\n",
    "    print(f\"   ‚úÖ Silhouette Score: {silhouette_dbscan:.3f}\")\n",
    "\n",
    "# 7.3 HIERARCHICAL CLUSTERING\n",
    "print(f\"\\n3Ô∏è‚É£ Hierarchical Clustering (K={optimal_k})...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "df_work['cluster_hierarchical'] = hierarchical.fit_predict(X_pca)\n",
    "silhouette_hier = silhouette_score(X_pca, df_work['cluster_hierarchical'])\n",
    "db_hier = davies_bouldin_score(X_pca, df_work['cluster_hierarchical'])\n",
    "ch_hier = calinski_harabasz_score(X_pca, df_work['cluster_hierarchical'])\n",
    "\n",
    "print(f\"   ‚úÖ Silhouette Score: {silhouette_hier:.3f}\")\n",
    "print(f\"   ‚úÖ Davies-Bouldin Score: {db_hier:.3f}\")\n",
    "print(f\"   ‚úÖ Calinski-Harabasz Score: {ch_hier:.1f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. AN√ÅLISE DOS CLUSTERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä AN√ÅLISE DOS CLUSTERS (K-MEANS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estat√≠sticas por cluster\n",
    "print(f\"\\nüìà Distribui√ß√£o de livros por cluster:\")\n",
    "cluster_counts = df_work['cluster_kmeans'].value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    pct = count / len(df_work) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} livros ({pct:.1f}%)\")\n",
    "\n",
    "# Caracter√≠sticas de cada cluster\n",
    "print(f\"\\nüîç Caracter√≠sticas dos Clusters:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "\n",
    "    print(f\"\\nüè∑Ô∏è  CLUSTER {cluster} ({len(cluster_data):,} livros)\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "\n",
    "    # Top autores\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_authors = cluster_data['author'].value_counts().head(3)\n",
    "        print(f\"   üìö Top autores:\")\n",
    "        for author, count in top_authors.items():\n",
    "            author_display = author[:40] + \"...\" if len(str(author)) > 40 else author\n",
    "            print(f\"      ‚Ä¢ {author_display}: {count} livros\")\n",
    "\n",
    "    # L√≠nguas\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_langs = cluster_data['language'].value_counts().head(3)\n",
    "        print(f\"   üåç L√≠nguas:\")\n",
    "        for lang, count in top_langs.items():\n",
    "            print(f\"      ‚Ä¢ {lang}: {count} livros ({count/len(cluster_data)*100:.1f}%)\")\n",
    "\n",
    "    # Anos\n",
    "    if 'year' in cluster_data.columns:\n",
    "        print(f\"   üìÖ Anos: {cluster_data['year'].min():.0f} - {cluster_data['year'].max():.0f}\")\n",
    "        print(f\"      M√©dia: {cluster_data['year'].mean():.1f}\")\n",
    "\n",
    "    # Top publishers\n",
    "    if 'publisher' in cluster_data.columns:\n",
    "        top_pubs = cluster_data['publisher'].value_counts().head(3)\n",
    "        print(f\"   üè¢ Top publishers:\")\n",
    "        for pub, count in top_pubs.items():\n",
    "            pub_display = pub[:40] + \"...\" if len(str(pub)) > 40 else pub\n",
    "            print(f\"      ‚Ä¢ {pub_display}: {count} livros\")\n",
    "\n",
    "    # Exemplos de t√≠tulos\n",
    "    print(f\"   üìñ Exemplos de t√≠tulos:\")\n",
    "    sample_titles = cluster_data['title'].sample(min(3, len(cluster_data)), random_state=42)\n",
    "    for title in sample_titles:\n",
    "        title_display = title[:60] + \"...\" if len(str(title)) > 60 else title\n",
    "        print(f\"      ‚Ä¢ {title_display}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. VISUALIZA√á√ïES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä GERANDO VISUALIZA√á√ïES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 9.1 Scatter plot 2D - K-Means\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_kmeans'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 0].set_title(f'K-Means Clustering (K={optimal_k})', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# DBSCAN\n",
    "scatter2 = axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_dbscan'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 1].set_title(f'DBSCAN Clustering ({n_clusters_dbscan} clusters)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster')\n",
    "\n",
    "# Hierarchical\n",
    "scatter3 = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_hierarchical'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1, 0].set_title(f'Hierarchical Clustering (K={optimal_k})', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter3, ax=axes[1, 0], label='Cluster')\n",
    "\n",
    "# Distribui√ß√£o de tamanhos dos clusters\n",
    "cluster_sizes = df_work['cluster_kmeans'].value_counts().sort_index()\n",
    "axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values, color='steelblue', alpha=0.7)\n",
    "axes[1, 1].set_title('Distribui√ß√£o de Livros por Cluster (K-Means)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Cluster', fontsize=11)\n",
    "axes[1, 1].set_ylabel('N√∫mero de Livros', fontsize=11)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, v in enumerate(cluster_sizes.values):\n",
    "    axes[1, 1].text(i, v + 50, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'clustering_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Visualiza√ß√£o principal salva: clustering_visualization.png\")\n",
    "\n",
    "# 9.2 Visualiza√ß√£o 3D (K-Means)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],\n",
    "                     c=df_work['cluster_kmeans'], cmap='tab10', alpha=0.6, s=20)\n",
    "\n",
    "ax.set_title(f'K-Means Clustering 3D (K={optimal_k})', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('PC1', fontsize=11, labelpad=10)\n",
    "ax.set_ylabel('PC2', fontsize=11, labelpad=10)\n",
    "ax.set_zlabel('PC3', fontsize=11, labelpad=10)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster', pad=0.1)\n",
    "plt.savefig(OUTPUT_DIR / 'clustering_3d.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Visualiza√ß√£o 3D salva: clustering_3d.png\")\n",
    "\n",
    "# ============================================================\n",
    "# 10. EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üíæ EXPORTANDO RESULTADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Juntar com dados originais\n",
    "df_clustered = df_work.copy()\n",
    "\n",
    "# Exportar dataset com clusters\n",
    "output_file = OUTPUT_DIR / 'books_clustered.csv'\n",
    "df_clustered.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Dataset com clusters: {output_file}\")\n",
    "print(f\"   {len(df_clustered):,} livros | {len(df_clustered.columns)} colunas\")\n",
    "\n",
    "# Exportar resumo dos clusters\n",
    "cluster_summary = []\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "\n",
    "    summary = {\n",
    "        'cluster': cluster,\n",
    "        'size': len(cluster_data),\n",
    "        'percentage': len(cluster_data) / len(df_work) * 100,\n",
    "        'top_author': cluster_data['author'].value_counts().index[0] if 'author' in cluster_data else 'N/A',\n",
    "        'top_language': cluster_data['language'].value_counts().index[0] if 'language' in cluster_data else 'N/A',\n",
    "        'avg_year': cluster_data['year'].mean() if 'year' in cluster_data else None,\n",
    "        'year_range': f\"{cluster_data['year'].min():.0f}-{cluster_data['year'].max():.0f}\" if 'year' in cluster_data else 'N/A'\n",
    "    }\n",
    "    cluster_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "summary_file = OUTPUT_DIR / 'cluster_summary.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"‚úÖ Resumo dos clusters: {summary_file}\")\n",
    "\n",
    "# Exportar m√©tricas de avalia√ß√£o\n",
    "metrics = {\n",
    "    'Algorithm': ['K-Means', 'DBSCAN', 'Hierarchical'],\n",
    "    'N_Clusters': [optimal_k, n_clusters_dbscan, optimal_k],\n",
    "    'Silhouette_Score': [\n",
    "        silhouette_kmeans,\n",
    "        silhouette_dbscan if n_clusters_dbscan > 1 else np.nan,\n",
    "        silhouette_hier\n",
    "    ],\n",
    "    'Davies_Bouldin_Score': [db_kmeans, np.nan, db_hier],\n",
    "    'Calinski_Harabasz_Score': [ch_kmeans, np.nan, ch_hier]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_file = OUTPUT_DIR / 'clustering_metrics.csv'\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"‚úÖ M√©tricas de avalia√ß√£o: {metrics_file}\")\n",
    "\n",
    "# Exportar exemplos de cada cluster\n",
    "print(f\"\\nüìö Exportando exemplos de cada cluster...\")\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "    sample = cluster_data.sample(min(20, len(cluster_data)), random_state=42)\n",
    "\n",
    "    sample_file = OUTPUT_DIR / f'cluster_{cluster}_examples.csv'\n",
    "    sample.to_csv(sample_file, index=False)\n",
    "    print(f\"   Cluster {cluster}: {len(sample)} exemplos salvos\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ POC CONCLU√çDA COM SUCESSO!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Resultados:\")\n",
    "print(f\"   ‚Ä¢ Melhor algoritmo: K-Means\")\n",
    "print(f\"   ‚Ä¢ N√∫mero de clusters: {optimal_k}\")\n",
    "print(f\"   ‚Ä¢ Silhouette Score: {silhouette_kmeans:.3f}\")\n",
    "print(f\"   ‚Ä¢ Livros clusterizados: {len(df_clustered):,}\")\n",
    "print(f\"\\nüìÅ Todos os arquivos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\nüí° Pr√≥ximos passos:\")\n",
    "print(f\"   1. Analisar os clusters gerados\")\n",
    "print(f\"   2. Refinar os par√¢metros se necess√°rio\")\n",
    "print(f\"   3. Usar os clusters para recomenda√ß√£o de livros\")"
   ],
   "id": "69bdb36efc2c2bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ü§ñ POC: CLUSTERING DE LIVROS SIMILARES\n",
      "======================================================================\n",
      "üìÇ Dataset: exports\\clean_data/book_data_clean.csv\n",
      "üìÅ Sa√≠da: exports\\clustering\n",
      "\n",
      "üìÇ Carregando dataset limpo...\n",
      "‚úÖ Dataset carregado: 55,502 livros\n",
      "   Colunas: 19\n",
      "\n",
      "======================================================================\n",
      "üîß PREPARA√á√ÉO DOS DADOS\n",
      "======================================================================\n",
      "‚úÖ title: 'name'\n",
      "‚úÖ author: 'authors'\n",
      "‚úÖ language: 'language'\n",
      "‚úÖ year: 'publishyear'\n",
      "‚úÖ publisher: 'publisher'\n",
      "\n",
      "üìä Valores nulos antes da limpeza:\n",
      "   language: 36,500 (65.8%)\n",
      "   publisher: 453 (0.8%)\n",
      "\n",
      "‚úÖ Ap√≥s remover nulos em t√≠tulo/autor: 55,502 livros\n",
      "‚úÖ Dataset final para clustering: 55,502 livros\n",
      "\n",
      "======================================================================\n",
      "üî® ENGENHARIA DE FEATURES\n",
      "======================================================================\n",
      "\n",
      "üìù Processando t√≠tulos com TF-IDF...\n",
      "‚úÖ T√≠tulo: 100 features\n",
      "üë§ Processando autores com TF-IDF...\n",
      "‚úÖ Autor: 50 features\n",
      "üåç Processando l√≠nguas...\n",
      "‚úÖ L√≠ngua: 36 features (36 idiomas)\n",
      "üìÖ Processando anos...\n",
      "‚úÖ Ano: normalizado (162 - 2020)\n",
      "üè¢ Processando publishers...\n",
      "‚úÖ Publisher: 30 features\n",
      "\n",
      "üîó Combinando features...\n",
      "‚úÖ Features combinadas: (55502, 217)\n",
      "   Total de features: 217\n",
      "\n",
      "======================================================================\n",
      "üìâ REDU√á√ÉO DE DIMENSIONALIDADE (PCA)\n",
      "======================================================================\n",
      "‚úÖ PCA 2D: vari√¢ncia explicada = 15.09%\n",
      "‚úÖ PCA 3D: vari√¢ncia explicada = 20.04%\n",
      "‚úÖ PCA clustering: 132 componentes (95% vari√¢ncia)\n",
      "\n",
      "======================================================================\n",
      "üìä DETERMINA√á√ÉO DO N√öMERO IDEAL DE CLUSTERS\n",
      "======================================================================\n",
      "\n",
      "üîç Testando diferentes n√∫meros de clusters (K-Means)...\n",
      "   K= 3 | In√©rcia: 409,689 | Silhouette: 0.097\n",
      "   K= 4 | In√©rcia: 393,638 | Silhouette: 0.106\n",
      "   K= 5 | In√©rcia: 386,904 | Silhouette: 0.106\n",
      "   K= 6 | In√©rcia: 371,580 | Silhouette: 0.097\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## An√°lise de Clusters",
   "id": "6f70c54efb8dd2ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Gera tabela interativa para an√°lise r√°pida dos clusters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "INPUT_DIR = Path(\"exports/clustering\")\n",
    "OUTPUT_DIR = Path(\"exports/clustering/analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä TABELA DE CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìÇ Carregando dados...\")\n",
    "clustered_file = INPUT_DIR / \"books_clustered.csv\"\n",
    "\n",
    "if not clustered_file.exists():\n",
    "    print(f\"‚ùå Erro: Arquivo n√£o encontrado: {clustered_file}\")\n",
    "    raise FileNotFoundError(f\"Execute o clustering primeiro!\")\n",
    "\n",
    "df = pd.read_csv(clustered_file)\n",
    "print(f\"‚úÖ {len(df):,} livros carregados\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. CRIAR TABELA RESUMO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüî® Criando tabela resumo...\")\n",
    "\n",
    "cluster_summary = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    # Informa√ß√µes b√°sicas\n",
    "    summary = {\n",
    "        'Cluster': cluster_id,\n",
    "        'Tamanho': len(cluster_data),\n",
    "        '% Total': f\"{len(cluster_data)/len(df)*100:.1f}%\"\n",
    "    }\n",
    "\n",
    "    # Top Autor\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_author = cluster_data['author'].value_counts()\n",
    "        summary['Top Autor'] = top_author.index[0][:40]\n",
    "        summary['Autor (%)'] = f\"{top_author.iloc[0]/len(cluster_data)*100:.0f}%\"\n",
    "        summary['Autores √önicos'] = cluster_data['author'].nunique()\n",
    "\n",
    "    # L√≠ngua dominante\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_lang = cluster_data['language'].value_counts()\n",
    "        summary['L√≠ngua'] = top_lang.index[0]\n",
    "        summary['L√≠ngua (%)'] = f\"{top_lang.iloc[0]/len(cluster_data)*100:.0f}%\"\n",
    "\n",
    "    # Anos\n",
    "    if 'year' in cluster_data.columns:\n",
    "        summary['Ano M√©dio'] = f\"{cluster_data['year'].mean():.0f}\"\n",
    "        summary['Per√≠odo'] = f\"{cluster_data['year'].min():.0f}-{cluster_data['year'].max():.0f}\"\n",
    "\n",
    "    # Publisher\n",
    "    if 'publisher' in cluster_data.columns:\n",
    "        top_pub = cluster_data['publisher'].value_counts()\n",
    "        summary['Top Publisher'] = top_pub.index[0][:30]\n",
    "\n",
    "    # Exemplo de t√≠tulo\n",
    "    if 'title' in cluster_data.columns:\n",
    "        example_title = cluster_data['title'].iloc[0]\n",
    "        summary['Exemplo'] = example_title[:50] + \"...\" if len(example_title) > 50 else example_title\n",
    "\n",
    "    cluster_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "\n",
    "# ============================================================\n",
    "# 4. EXIBIR TABELA NO TERMINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã RESUMO DOS CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Exibir com pandas (formatado)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 5. CRIAR HTML INTERATIVO COM TABELA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìÑ Gerando tabela HTML interativa...\")\n",
    "\n",
    "html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Tabela de Clusters - Goodreads</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1400px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            box-shadow: 0 10px 40px rgba(0,0,0,0.3);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .header h1 {{\n",
    "            margin: 0;\n",
    "            font-size: 2.5em;\n",
    "        }}\n",
    "        .stats {{\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-bottom: 2px solid #667eea;\n",
    "        }}\n",
    "        .stat-box {{\n",
    "            text-align: center;\n",
    "            padding: 15px;\n",
    "        }}\n",
    "        .stat-number {{\n",
    "            font-size: 2em;\n",
    "            font-weight: bold;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "        .stat-label {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "            margin-top: 5px;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            padding: 20px;\n",
    "            overflow-x: auto;\n",
    "        }}\n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        thead {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            position: sticky;\n",
    "            top: 0;\n",
    "            z-index: 10;\n",
    "        }}\n",
    "        th {{\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            font-size: 0.9em;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 0.5px;\n",
    "        }}\n",
    "        td {{\n",
    "            padding: 12px 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }}\n",
    "        tr:hover {{\n",
    "            background-color: #f5f5ff;\n",
    "            cursor: pointer;\n",
    "        }}\n",
    "        .cluster-id {{\n",
    "            font-weight: bold;\n",
    "            font-size: 1.2em;\n",
    "            color: #667eea;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .size {{\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .percentage {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        .author {{\n",
    "            color: #333;\n",
    "            font-weight: 500;\n",
    "        }}\n",
    "        .language {{\n",
    "            display: inline-block;\n",
    "            background: #667eea;\n",
    "            color: white;\n",
    "            padding: 4px 10px;\n",
    "            border-radius: 12px;\n",
    "            font-size: 0.85em;\n",
    "        }}\n",
    "        .year-range {{\n",
    "            color: #666;\n",
    "            font-family: monospace;\n",
    "        }}\n",
    "        .example {{\n",
    "            color: #555;\n",
    "            font-style: italic;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        .filter-box {{\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }}\n",
    "        .filter-box input {{\n",
    "            width: 100%;\n",
    "            padding: 12px;\n",
    "            font-size: 1em;\n",
    "            border: 2px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            transition: border-color 0.3s;\n",
    "        }}\n",
    "        .filter-box input:focus {{\n",
    "            outline: none;\n",
    "            border-color: #667eea;\n",
    "        }}\n",
    "        .badge {{\n",
    "            display: inline-block;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 10px;\n",
    "            font-size: 0.8em;\n",
    "            font-weight: 600;\n",
    "            margin-right: 5px;\n",
    "        }}\n",
    "        .badge-large {{\n",
    "            background: #ff6b6b;\n",
    "            color: white;\n",
    "        }}\n",
    "        .badge-medium {{\n",
    "            background: #ffd93d;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .badge-small {{\n",
    "            background: #6bcf7f;\n",
    "            color: white;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üìö Tabela de Clusters - Goodreads</h1>\n",
    "            <p>An√°lise visual dos clusters gerados</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"stats\">\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{len(df):,}</div>\n",
    "                <div class=\"stat-label\">Total de Livros</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{len(summary_df)}</div>\n",
    "                <div class=\"stat-label\">Clusters</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{int(summary_df['Tamanho'].mean())}</div>\n",
    "                <div class=\"stat-label\">M√©dia por Cluster</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{summary_df['Tamanho'].max()}</div>\n",
    "                <div class=\"stat-label\">Maior Cluster</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"filter-box\">\n",
    "            <input type=\"text\" id=\"searchInput\" placeholder=\"üîç Buscar por autor, l√≠ngua, publisher...\"\n",
    "                   onkeyup=\"filterTable()\">\n",
    "        </div>\n",
    "\n",
    "        <div class=\"table-container\">\n",
    "            <table id=\"clusterTable\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>Cluster</th>\n",
    "                        <th>Tamanho</th>\n",
    "                        <th>Top Autor</th>\n",
    "                        <th>Conc. Autor</th>\n",
    "                        <th>Autores √önicos</th>\n",
    "                        <th>L√≠ngua</th>\n",
    "                        <th>Ano M√©dio</th>\n",
    "                        <th>Per√≠odo</th>\n",
    "                        <th>Top Publisher</th>\n",
    "                        <th>Exemplo de T√≠tulo</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Adicionar linhas da tabela\n",
    "for _, row in summary_df.iterrows():\n",
    "    # Determinar badge de tamanho\n",
    "    size = row['Tamanho']\n",
    "    if size > summary_df['Tamanho'].quantile(0.75):\n",
    "        badge_class = 'badge-large'\n",
    "        badge_text = 'Grande'\n",
    "    elif size > summary_df['Tamanho'].quantile(0.25):\n",
    "        badge_class = 'badge-medium'\n",
    "        badge_text = 'M√©dio'\n",
    "    else:\n",
    "        badge_class = 'badge-small'\n",
    "        badge_text = 'Pequeno'\n",
    "\n",
    "    html += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td class=\"cluster-id\">{row['Cluster']}</td>\n",
    "                        <td class=\"size\">\n",
    "                            {row['Tamanho']:,}\n",
    "                            <span class=\"badge {badge_class}\">{badge_text}</span>\n",
    "                            <br>\n",
    "                            <span class=\"percentage\">{row['% Total']}</span>\n",
    "                        </td>\n",
    "                        <td class=\"author\">{row.get('Top Autor', 'N/A')}</td>\n",
    "                        <td>{row.get('Autor (%)', 'N/A')}</td>\n",
    "                        <td>{row.get('Autores √önicos', 'N/A')}</td>\n",
    "                        <td><span class=\"language\">{row.get('L√≠ngua', 'N/A')}</span> {row.get('L√≠ngua (%)', '')}</td>\n",
    "                        <td class=\"year-range\">{row.get('Ano M√©dio', 'N/A')}</td>\n",
    "                        <td class=\"year-range\">{row.get('Per√≠odo', 'N/A')}</td>\n",
    "                        <td>{row.get('Top Publisher', 'N/A')}</td>\n",
    "                        <td class=\"example\">{row.get('Exemplo', 'N/A')}</td>\n",
    "                    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html += \"\"\"\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function filterTable() {\n",
    "            const input = document.getElementById('searchInput');\n",
    "            const filter = input.value.toUpperCase();\n",
    "            const table = document.getElementById('clusterTable');\n",
    "            const tr = table.getElementsByTagName('tr');\n",
    "\n",
    "            for (let i = 1; i < tr.length; i++) {\n",
    "                const row = tr[i];\n",
    "                const text = row.textContent || row.innerText;\n",
    "\n",
    "                if (text.toUpperCase().indexOf(filter) > -1) {\n",
    "                    row.style.display = '';\n",
    "                } else {\n",
    "                    row.style.display = 'none';\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Adicionar click para destacar linha\n",
    "        const rows = document.querySelectorAll('tbody tr');\n",
    "        rows.forEach(row => {\n",
    "            row.addEventListener('click', function() {\n",
    "                rows.forEach(r => r.style.backgroundColor = '');\n",
    "                this.style.backgroundColor = '#e8eaf6';\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Salvar HTML\n",
    "html_file = OUTPUT_DIR / \"cluster_table.html\"\n",
    "with open(html_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(html)\n",
    "\n",
    "print(f\"‚úÖ Tabela HTML gerada: {html_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. CRIAR TABELA DETALHADA COM EXEMPLOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìã Gerando tabela detalhada com exemplos...\")\n",
    "\n",
    "detailed_data = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    # Pegar 5 exemplos de livros\n",
    "    examples = cluster_data.sample(min(5, len(cluster_data)), random_state=42)\n",
    "\n",
    "    for idx, (_, book) in enumerate(examples.iterrows()):\n",
    "        row = {\n",
    "            'Cluster': cluster_id,\n",
    "            'T√≠tulo': book.get('title', 'N/A'),\n",
    "            'Autor': book.get('author', 'N/A'),\n",
    "            'Ano': int(book.get('year', 0)) if pd.notna(book.get('year')) else 'N/A',\n",
    "            'L√≠ngua': book.get('language', 'N/A'),\n",
    "            'Publisher': book.get('publisher', 'N/A')\n",
    "        }\n",
    "        detailed_data.append(row)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_data)\n",
    "\n",
    "# Salvar CSV detalhado\n",
    "csv_file = OUTPUT_DIR / \"cluster_examples_table.csv\"\n",
    "detailed_df.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Tabela CSV detalhada: {csv_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. CRIAR VISUALIZA√á√ÉO SIMPLIFICADA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìä Gerando visualiza√ß√£o simplificada...\")\n",
    "\n",
    "simple_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Clusters Simplificado</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }}\n",
    "        .cluster-card {{\n",
    "            background: white;\n",
    "            border-radius: 10px;\n",
    "            padding: 20px;\n",
    "            margin-bottom: 15px;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            border-left: 5px solid #667eea;\n",
    "        }}\n",
    "        .cluster-header {{\n",
    "            font-size: 1.5em;\n",
    "            font-weight: bold;\n",
    "            color: #667eea;\n",
    "            margin-bottom: 15px;\n",
    "        }}\n",
    "        .info-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 10px;\n",
    "            margin-bottom: 15px;\n",
    "        }}\n",
    "        .info-item {{\n",
    "            background: #f8f9fa;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "        }}\n",
    "        .info-label {{\n",
    "            font-weight: bold;\n",
    "            color: #555;\n",
    "            font-size: 0.85em;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .info-value {{\n",
    "            color: #333;\n",
    "            font-size: 1.1em;\n",
    "        }}\n",
    "        .books-sample {{\n",
    "            background: #f0f0f0;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        .book-item {{\n",
    "            padding: 8px;\n",
    "            margin: 5px 0;\n",
    "            background: white;\n",
    "            border-radius: 3px;\n",
    "        }}\n",
    "        h1 {{\n",
    "            text-align: center;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>üìö Clusters de Livros - Vis√£o Simplificada</h1>\n",
    "    <p style=\"text-align: center; color: #666;\">Total: {len(df):,} livros em {len(summary_df)} clusters</p>\n",
    "\"\"\"\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    simple_html += f\"\"\"\n",
    "    <div class=\"cluster-card\">\n",
    "        <div class=\"cluster-header\">\n",
    "            Cluster {cluster_id} - {len(cluster_data):,} livros ({len(cluster_data)/len(df)*100:.1f}%)\n",
    "        </div>\n",
    "\n",
    "        <div class=\"info-grid\">\n",
    "    \"\"\"\n",
    "\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_author = cluster_data['author'].value_counts().iloc[0]\n",
    "        top_author_name = cluster_data['author'].value_counts().index[0]\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">Top Autor</div>\n",
    "                <div class=\"info-value\">{top_author_name[:30]}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">({top_author} livros)</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_lang = cluster_data['language'].value_counts()\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">L√≠ngua Principal</div>\n",
    "                <div class=\"info-value\">{top_lang.index[0]}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">({top_lang.iloc[0]/len(cluster_data)*100:.0f}%)</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    if 'year' in cluster_data.columns:\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">Per√≠odo</div>\n",
    "                <div class=\"info-value\">{cluster_data['year'].min():.0f} - {cluster_data['year'].max():.0f}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">M√©dia: {cluster_data['year'].mean():.0f}</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    simple_html += \"\"\"\n",
    "        </div>\n",
    "\n",
    "        <div class=\"books-sample\">\n",
    "            <strong>üìñ Exemplos de livros:</strong>\n",
    "    \"\"\"\n",
    "\n",
    "    samples = cluster_data.sample(min(3, len(cluster_data)), random_state=42)\n",
    "    for _, book in samples.iterrows():\n",
    "        title = str(book.get('title', 'N/A'))[:60]\n",
    "        author = str(book.get('author', 'N/A'))[:30]\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"book-item\">\n",
    "                <strong>{title}</strong> - {author}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    simple_html += \"\"\"\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "simple_html += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "simple_file = OUTPUT_DIR / \"cluster_simple.html\"\n",
    "with open(simple_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(simple_html)\n",
    "\n",
    "print(f\"‚úÖ Visualiza√ß√£o simplificada: {simple_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. SALVAR CSV RESUMO\n",
    "# ============================================================\n",
    "\n",
    "csv_summary = OUTPUT_DIR / \"cluster_summary_table.csv\"\n",
    "summary_df.to_csv(csv_summary, index=False)\n",
    "print(f\"‚úÖ CSV resumo: {csv_summary}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. RESUMO FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TABELAS GERADAS COM SUCESSO!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ Arquivos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\nüìÑ Arquivos gerados:\")\n",
    "print(f\"   1. cluster_table.html              - Tabela interativa completa\")\n",
    "print(f\"   2. cluster_simple.html             - Vis√£o simplificada em cards\")\n",
    "print(f\"   3. cluster_summary_table.csv       - Resumo em CSV\")\n",
    "print(f\"   4. cluster_examples_table.csv      - Exemplos detalhados\")\n",
    "\n",
    "print(f\"\\nüí° Como usar:\")\n",
    "print(f\"   ‚Ä¢ Abra cluster_table.html no navegador\")\n",
    "print(f\"   ‚Ä¢ Use a busca para filtrar clusters\")\n",
    "print(f\"   ‚Ä¢ Clique nas linhas para destacar\")\n",
    "print(f\"   ‚Ä¢ Abra cluster_simple.html para vis√£o r√°pida\")\n",
    "\n",
    "print(f\"\\nüéØ O que observar:\")\n",
    "print(f\"   ‚úì Concentra√ß√£o de autores (clusters de s√©ries?)\")\n",
    "print(f\"   ‚úì Homogeneidade de l√≠ngua (mercados regionais?)\")\n",
    "print(f\"   ‚úì Per√≠odos temporais (tend√™ncias de √©poca?)\")\n",
    "print(f\"   ‚úì Publishers dominantes (nichos editoriais?)\")"
   ],
   "id": "87794f01ecd3ff7f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
