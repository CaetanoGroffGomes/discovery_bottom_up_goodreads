{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8071d5f0",
   "metadata": {},
   "source": [
    "# EDA — `book1-100.csv`\n",
    "\n",
    "Análise exploratória dos dados de `book1-100.csv`."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff5b25fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:03.908590Z",
     "start_time": "2025-10-27T18:45:03.906092Z"
    }
   },
   "source": [
    "# CONFIG\n",
    "import pandas as pd\n",
    "\n",
    "# CAMINHO DO ARQUIVO\n",
    "path = r\"C:\\Users\\USER\\PycharmProjects\\JupyterProject\\data\\book1-100k.csv\""
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "e3bf8834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:04.589450Z",
     "start_time": "2025-10-27T18:45:03.931292Z"
    }
   },
   "source": [
    "# LEITURA ROBUSTA\n",
    "try:\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Arquivo não encontrado em {path}. Ajuste a variável 'path' acima.\") from e\n",
    "\n",
    "# PADRONIZAÇÃO NOMES COLUNAS\n",
    "df.columns = [c.strip().replace(\"  \", \" \").replace(\" \", \"_\").lower() for c in df.columns]\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head(10)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (58292, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id                                               name ratingdist1  \\\n",
       "0   1  Harry Potter and the Half-Blood Prince (Harry ...      1:9896   \n",
       "1   2  Harry Potter and the Order of the Phoenix (Har...     1:12455   \n",
       "2   3  Harry Potter and the Sorcerer's Stone (Harry P...    1:108202   \n",
       "3   4  Harry Potter and the Chamber of Secrets (Harry...     1:11896   \n",
       "4   5  Harry Potter and the Prisoner of Azkaban (Harr...     1:10128   \n",
       "5   6  Harry Potter and the Goblet of Fire (Harry Pot...      1:9419   \n",
       "6   8  Harry Potter Boxed Set, Books 1-5 (Harry Potte...       1:402   \n",
       "7   9  Unauthorized Harry Potter Book Seven News: \"Ha...         1:0   \n",
       "8  10       Harry Potter Collection (Harry Potter, #1-6)       1:257   \n",
       "9  12  The Ultimate Hitchhiker's Guide: Five Complete...      1:3443   \n",
       "\n",
       "   pagesnumber ratingdist4 ratingdisttotal  publishmonth  publishday  \\\n",
       "0          652    4:556485   total:2298124            16           9   \n",
       "1          870    4:604283   total:2358637             1           9   \n",
       "2          309   4:1513191   total:6587388             1          11   \n",
       "3          352    4:706082   total:2560657             1          11   \n",
       "4          435    4:630534   total:2610317             1           5   \n",
       "5          734    4:606800   total:2431085            28           9   \n",
       "6         2690      4:4650     total:43968            13           9   \n",
       "7          152         4:7        total:28            26           4   \n",
       "8         3342      4:4358     total:30313            12           9   \n",
       "9          815     4:75683    total:274268             1          11   \n",
       "\n",
       "         publisher  countsofreview  publishyear language  \\\n",
       "0  Scholastic Inc.           28062         2006      eng   \n",
       "1  Scholastic Inc.           29770         2004      eng   \n",
       "2   Scholastic Inc           75911         2003      eng   \n",
       "3       Scholastic             244         2003      eng   \n",
       "4  Scholastic Inc.           37093         2004      eng   \n",
       "5       Scholastic           31978         2002      eng   \n",
       "6       Scholastic             166         2004      eng   \n",
       "7     Nimble Books               1         2005    en-US   \n",
       "8       Scholastic             809         2005      eng   \n",
       "9   Gramercy Books             255         2005      eng   \n",
       "\n",
       "                  authors  rating ratingdist2 ratingdist5        isbn  \\\n",
       "0            J.K. Rowling    4.57     2:25317   5:1546466         NaN   \n",
       "1            J.K. Rowling    4.50     2:37005   5:1493113  0439358078   \n",
       "2            J.K. Rowling    4.47    2:130310   5:4268227         NaN   \n",
       "3            J.K. Rowling    4.42     2:49353   5:1504505  0439554896   \n",
       "4            J.K. Rowling    4.57     2:24849   5:1749958  043965548X   \n",
       "5            J.K. Rowling    4.56     2:24282   5:1612165         NaN   \n",
       "6            J.K. Rowling    4.78       2:283     5:37432  0439682584   \n",
       "7  W. Frederick Zimmerman    3.79         2:5        5:10  0976540606   \n",
       "8            J.K. Rowling    4.73       2:218     5:24406  0439827604   \n",
       "9           Douglas Adams    4.37      2:7613    5:157499  0517226952   \n",
       "\n",
       "  ratingdist3  \n",
       "0    3:159960  \n",
       "1    3:211781  \n",
       "2    3:567458  \n",
       "3    3:288821  \n",
       "4    3:194848  \n",
       "5    3:178419  \n",
       "6      3:1201  \n",
       "7         3:6  \n",
       "8      3:1074  \n",
       "9     3:30030  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>ratingdist1</th>\n",
       "      <th>pagesnumber</th>\n",
       "      <th>ratingdist4</th>\n",
       "      <th>ratingdisttotal</th>\n",
       "      <th>publishmonth</th>\n",
       "      <th>publishday</th>\n",
       "      <th>publisher</th>\n",
       "      <th>countsofreview</th>\n",
       "      <th>publishyear</th>\n",
       "      <th>language</th>\n",
       "      <th>authors</th>\n",
       "      <th>rating</th>\n",
       "      <th>ratingdist2</th>\n",
       "      <th>ratingdist5</th>\n",
       "      <th>isbn</th>\n",
       "      <th>ratingdist3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>1:9896</td>\n",
       "      <td>652</td>\n",
       "      <td>4:556485</td>\n",
       "      <td>total:2298124</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>28062</td>\n",
       "      <td>2006</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2:25317</td>\n",
       "      <td>5:1546466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:159960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>1:12455</td>\n",
       "      <td>870</td>\n",
       "      <td>4:604283</td>\n",
       "      <td>total:2358637</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>29770</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2:37005</td>\n",
       "      <td>5:1493113</td>\n",
       "      <td>0439358078</td>\n",
       "      <td>3:211781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>1:108202</td>\n",
       "      <td>309</td>\n",
       "      <td>4:1513191</td>\n",
       "      <td>total:6587388</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Scholastic Inc</td>\n",
       "      <td>75911</td>\n",
       "      <td>2003</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.47</td>\n",
       "      <td>2:130310</td>\n",
       "      <td>5:4268227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:567458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>1:11896</td>\n",
       "      <td>352</td>\n",
       "      <td>4:706082</td>\n",
       "      <td>total:2560657</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>244</td>\n",
       "      <td>2003</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.42</td>\n",
       "      <td>2:49353</td>\n",
       "      <td>5:1504505</td>\n",
       "      <td>0439554896</td>\n",
       "      <td>3:288821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>1:10128</td>\n",
       "      <td>435</td>\n",
       "      <td>4:630534</td>\n",
       "      <td>total:2610317</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Scholastic Inc.</td>\n",
       "      <td>37093</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.57</td>\n",
       "      <td>2:24849</td>\n",
       "      <td>5:1749958</td>\n",
       "      <td>043965548X</td>\n",
       "      <td>3:194848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>1:9419</td>\n",
       "      <td>734</td>\n",
       "      <td>4:606800</td>\n",
       "      <td>total:2431085</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>31978</td>\n",
       "      <td>2002</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.56</td>\n",
       "      <td>2:24282</td>\n",
       "      <td>5:1612165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3:178419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>1:402</td>\n",
       "      <td>2690</td>\n",
       "      <td>4:4650</td>\n",
       "      <td>total:43968</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>166</td>\n",
       "      <td>2004</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.78</td>\n",
       "      <td>2:283</td>\n",
       "      <td>5:37432</td>\n",
       "      <td>0439682584</td>\n",
       "      <td>3:1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Unauthorized Harry Potter Book Seven News: \"Ha...</td>\n",
       "      <td>1:0</td>\n",
       "      <td>152</td>\n",
       "      <td>4:7</td>\n",
       "      <td>total:28</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>Nimble Books</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>en-US</td>\n",
       "      <td>W. Frederick Zimmerman</td>\n",
       "      <td>3.79</td>\n",
       "      <td>2:5</td>\n",
       "      <td>5:10</td>\n",
       "      <td>0976540606</td>\n",
       "      <td>3:6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>Harry Potter Collection (Harry Potter, #1-6)</td>\n",
       "      <td>1:257</td>\n",
       "      <td>3342</td>\n",
       "      <td>4:4358</td>\n",
       "      <td>total:30313</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>809</td>\n",
       "      <td>2005</td>\n",
       "      <td>eng</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2:218</td>\n",
       "      <td>5:24406</td>\n",
       "      <td>0439827604</td>\n",
       "      <td>3:1074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>1:3443</td>\n",
       "      <td>815</td>\n",
       "      <td>4:75683</td>\n",
       "      <td>total:274268</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Gramercy Books</td>\n",
       "      <td>255</td>\n",
       "      <td>2005</td>\n",
       "      <td>eng</td>\n",
       "      <td>Douglas Adams</td>\n",
       "      <td>4.37</td>\n",
       "      <td>2:7613</td>\n",
       "      <td>5:157499</td>\n",
       "      <td>0517226952</td>\n",
       "      <td>3:30030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Limpeza do dataset",
   "id": "a4552e4fc9cb2e05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Análise de nulls, uniques e tipo",
   "id": "edbb91cec7eda12e"
  },
  {
   "cell_type": "code",
   "id": "18516917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:05.026357Z",
     "start_time": "2025-10-27T18:45:04.611066Z"
    }
   },
   "source": [
    "profile = []\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    profile.append({\n",
    "        \"column\": c,\n",
    "        \"dtype\": str(s.dtype),\n",
    "        \"missing_%\": round(s.isna().mean()*100, 2),\n",
    "        \"unique_n\": int(s.nunique(dropna=True)),\n",
    "        \"example_values\": \", \".join(map(str, s.dropna().astype(str).unique()[:5]))\n",
    "    })\n",
    "import pandas as pd\n",
    "pd.DataFrame(profile).sort_values(\"missing_%\", ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             column    dtype  missing_%  unique_n  \\\n",
       "11         language   object      65.19        36   \n",
       "16             isbn   object       0.94     57552   \n",
       "8         publisher   object       0.85      7520   \n",
       "0                id    int64       0.00     58097   \n",
       "1              name   object       0.00     57510   \n",
       "2       ratingdist1   object       0.00      2378   \n",
       "5   ratingdisttotal   object       0.00     11464   \n",
       "6      publishmonth    int64       0.00        31   \n",
       "4       ratingdist4   object       0.00      7994   \n",
       "3       pagesnumber    int64       0.00      1340   \n",
       "9    countsofreview    int64       0.00      2163   \n",
       "7        publishday    int64       0.00        12   \n",
       "12          authors   object       0.00     28633   \n",
       "10      publishyear    int64       0.00       105   \n",
       "13           rating  float64       0.00       267   \n",
       "14      ratingdist2   object       0.00      3718   \n",
       "15      ratingdist5   object       0.00      7964   \n",
       "17      ratingdist3   object       0.00      6620   \n",
       "\n",
       "                                       example_values  \n",
       "11                          eng, en-US, fre, spa, mul  \n",
       "16  0439358078, 0439554896, 043965548X, 0439682584...  \n",
       "8   Scholastic Inc., Scholastic Inc, Scholastic, N...  \n",
       "0                                       1, 2, 3, 4, 5  \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...  \n",
       "2         1:9896, 1:12455, 1:108202, 1:11896, 1:10128  \n",
       "5   total:2298124, total:2358637, total:6587388, t...  \n",
       "6                                   16, 1, 28, 13, 26  \n",
       "4   4:556485, 4:604283, 4:1513191, 4:706082, 4:630534  \n",
       "3                             652, 870, 309, 352, 435  \n",
       "9                     28062, 29770, 75911, 244, 37093  \n",
       "7                                      9, 11, 5, 4, 8  \n",
       "12  J.K. Rowling, W. Frederick Zimmerman, Douglas ...  \n",
       "10                       2006, 2004, 2003, 2002, 2005  \n",
       "13          4.57, 4.5, 4.47, 4.42, 4.5600000000000005  \n",
       "14       2:25317, 2:37005, 2:130310, 2:49353, 2:24849  \n",
       "15  5:1546466, 5:1493113, 5:4268227, 5:1504505, 5:...  \n",
       "17   3:159960, 3:211781, 3:567458, 3:288821, 3:194848  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>unique_n</th>\n",
       "      <th>example_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>language</td>\n",
       "      <td>object</td>\n",
       "      <td>65.19</td>\n",
       "      <td>36</td>\n",
       "      <td>eng, en-US, fre, spa, mul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>isbn</td>\n",
       "      <td>object</td>\n",
       "      <td>0.94</td>\n",
       "      <td>57552</td>\n",
       "      <td>0439358078, 0439554896, 043965548X, 0439682584...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>publisher</td>\n",
       "      <td>object</td>\n",
       "      <td>0.85</td>\n",
       "      <td>7520</td>\n",
       "      <td>Scholastic Inc., Scholastic Inc, Scholastic, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58097</td>\n",
       "      <td>1, 2, 3, 4, 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>name</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>57510</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ratingdist1</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2378</td>\n",
       "      <td>1:9896, 1:12455, 1:108202, 1:11896, 1:10128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ratingdisttotal</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11464</td>\n",
       "      <td>total:2298124, total:2358637, total:6587388, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>publishmonth</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31</td>\n",
       "      <td>16, 1, 28, 13, 26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ratingdist4</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7994</td>\n",
       "      <td>4:556485, 4:604283, 4:1513191, 4:706082, 4:630534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pagesnumber</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1340</td>\n",
       "      <td>652, 870, 309, 352, 435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>countsofreview</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2163</td>\n",
       "      <td>28062, 29770, 75911, 244, 37093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>publishday</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>9, 11, 5, 4, 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>authors</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28633</td>\n",
       "      <td>J.K. Rowling, W. Frederick Zimmerman, Douglas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>publishyear</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>105</td>\n",
       "      <td>2006, 2004, 2003, 2002, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rating</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>267</td>\n",
       "      <td>4.57, 4.5, 4.47, 4.42, 4.5600000000000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ratingdist2</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3718</td>\n",
       "      <td>2:25317, 2:37005, 2:130310, 2:49353, 2:24849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ratingdist5</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7964</td>\n",
       "      <td>5:1546466, 5:1493113, 5:4268227, 5:1504505, 5:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ratingdist3</td>\n",
       "      <td>object</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6620</td>\n",
       "      <td>3:159960, 3:211781, 3:567458, 3:288821, 3:194848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Avaliar livros com ano > 2020",
   "id": "85cee9981d6500e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:05.074079Z",
     "start_time": "2025-10-27T18:45:05.035211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DEFINIR NOMES DAS COLUNAS\n",
    "year_col = \"publishyear\"\n",
    "title_col = \"name\"\n",
    "author_col = \"authors\"\n",
    "\n",
    "# FILTRAR > 2020\n",
    "invalidYear = df[df[year_col] > 2020].copy()\n",
    "validYear = df[df[year_col] <= 2020].copy()\n",
    "\n",
    "# MONTAR COLUNAS\n",
    "cols_to_show = [title_col, author_col, year_col]\n",
    "\n",
    "print(f\"Títulos inválidos (> 2020): {len(invalidYear)}\")\n",
    "\n",
    "# EXIBIR INVÁLIDOS\n",
    "if len(invalidYear) > 0:\n",
    "    print(\"\\n⚠️ LIVROS COM ANO INVÁLIDO (> 2020):\")\n",
    "    display(invalidYear[cols_to_show].sort_values(year_col, ascending=False))\n",
    "else:\n",
    "    print(\"✅ Nenhum livro com ano > 2020\")\n",
    "\n",
    "# EXIBIR AMOSTRA\n",
    "print(f\"\\nTítulos válidos (≤ 2020): {len(validYear)}\")"
   ],
   "id": "95fd789fee3862d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Títulos inválidos (> 2020): 1\n",
      "\n",
      "⚠️ LIVROS COM ANO INVÁLIDO (> 2020):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                 name           authors  \\\n",
       "25597  The Water Babies: A Fairy Tale for a Land Baby  Charles Kingsley   \n",
       "\n",
       "       publishyear  \n",
       "25597         3002  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>authors</th>\n",
       "      <th>publishyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25597</th>\n",
       "      <td>The Water Babies: A Fairy Tale for a Land Baby</td>\n",
       "      <td>Charles Kingsley</td>\n",
       "      <td>3002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Títulos válidos (≤ 2020): 58291\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validação e Normalização de ISBN (ISBN-10 / ISBN-13)",
   "id": "a35cf03d356722ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:45:06.353024Z",
     "start_time": "2025-10-27T18:45:05.084058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Valida ISBNs e detecta duplicatas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# 0. CONFIGURAÇÃO E CARREGAMENTO\n",
    "# =====================================================\n",
    "\n",
    "# Caminhos\n",
    "INPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR = Path(\"exports/duplicatas\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"📚 VALIDAÇÃO E ANÁLISE DE ISBN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"📂 Lendo dados de: {INPUT_DIR}\")\n",
    "print(f\"📁 Salvando em: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# Carregar dataset limpo\n",
    "clean_file = INPUT_DIR / \"book_data_clean.csv\"\n",
    "\n",
    "if not clean_file.exists():\n",
    "    print(f\"❌ Erro: Arquivo não encontrado: {clean_file}\")\n",
    "    print(f\"   Execute primeiro o script de limpeza de dados!\")\n",
    "\n",
    "    # Tentar carregar do dataset original\n",
    "    print(f\"\\n⚠️  Tentando carregar dataset original...\")\n",
    "    original_file = Path(\"data/book1-100k.csv\")\n",
    "\n",
    "    if original_file.exists():\n",
    "        print(f\"✅ Dataset original encontrado: {original_file}\")\n",
    "        df = pd.read_csv(original_file, engine=\"python\", on_bad_lines=\"skip\")\n",
    "        # Padronizar nomes de colunas\n",
    "        df.columns = [c.strip().replace(\"  \", \" \").replace(\" \", \"_\").lower() for c in df.columns]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Nenhum dataset encontrado! Execute a limpeza de dados primeiro.\")\n",
    "else:\n",
    "    df = pd.read_csv(clean_file)\n",
    "    print(f\"✅ Dataset limpo carregado: {len(df):,} livros\")\n",
    "\n",
    "# Nome da coluna\n",
    "isbn_col = \"isbn\"\n",
    "\n",
    "# Verificar se coluna existe\n",
    "if isbn_col not in df.columns:\n",
    "    print(f\"❌ Erro: Coluna '{isbn_col}' não encontrada!\")\n",
    "    print(f\"   Colunas disponíveis: {list(df.columns)}\")\n",
    "    raise ValueError(f\"Coluna '{isbn_col}' não encontrada no dataset\")\n",
    "\n",
    "print(f\"✅ Coluna '{isbn_col}' encontrada\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. FUNÇÕES DE VALIDAÇÃO\n",
    "# =====================================================\n",
    "\n",
    "def clean_isbn(s):\n",
    "    \"\"\"Remove caracteres não numéricos do ISBN\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[^0-9X]\", \"\", s)  # Remove tudo exceto números e X\n",
    "    return s if s else None\n",
    "\n",
    "\n",
    "def is_valid_isbn10(isbn):\n",
    "    \"\"\"Valida ISBN-10 usando algoritmo de check digit\"\"\"\n",
    "    if not isbn or len(isbn) != 10:\n",
    "        return False\n",
    "\n",
    "    total = 0\n",
    "    for i, ch in enumerate(isbn[:9]):\n",
    "        if not ch.isdigit():\n",
    "            return False\n",
    "        total += (10 - i) * int(ch)\n",
    "\n",
    "    # Último caractere pode ser X (vale 10)\n",
    "    check = isbn[-1]\n",
    "    if check == 'X':\n",
    "        total += 10\n",
    "    elif check.isdigit():\n",
    "        total += int(check)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    return total % 11 == 0\n",
    "\n",
    "\n",
    "def is_valid_isbn13(isbn):\n",
    "    \"\"\"Valida ISBN-13 usando algoritmo de check digit\"\"\"\n",
    "    if not isbn or len(isbn) != 13 or not isbn.isdigit():\n",
    "        return False\n",
    "\n",
    "    # Calcula check digit\n",
    "    total = 0\n",
    "    for i, ch in enumerate(isbn[:12]):\n",
    "        weight = 1 if i % 2 == 0 else 3\n",
    "        total += int(ch) * weight\n",
    "\n",
    "    check_digit = (10 - (total % 10)) % 10\n",
    "    return check_digit == int(isbn[-1])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. ANÁLISE DO DATASET\n",
    "# =====================================================\n",
    "\n",
    "print(\"🔍 Analisando ISBNs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Limpar ISBNs\n",
    "df['isbn_clean'] = df[isbn_col].apply(clean_isbn)\n",
    "\n",
    "# Detectar tipo de ISBN\n",
    "df['isbn_length'] = df['isbn_clean'].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# Validar\n",
    "df['is_isbn10'] = df['isbn_clean'].apply(\n",
    "    lambda x: is_valid_isbn10(x) if x and len(x) == 10 else False\n",
    ")\n",
    "df['is_isbn13'] = df['isbn_clean'].apply(\n",
    "    lambda x: is_valid_isbn13(x) if x and len(x) == 13 else False\n",
    ")\n",
    "\n",
    "# ISBNs válidos (10 ou 13)\n",
    "df['is_valid'] = df['is_isbn10'] | df['is_isbn13']\n",
    "\n",
    "# =====================================================\n",
    "# 3. ESTATÍSTICAS\n",
    "# =====================================================\n",
    "\n",
    "total = len(df)\n",
    "com_isbn = df['isbn_clean'].notna().sum()\n",
    "isbn10_count = (df['isbn_length'] == 10).sum()\n",
    "isbn13_count = (df['isbn_length'] == 13).sum()\n",
    "isbn10_valid = df['is_isbn10'].sum()\n",
    "isbn13_valid = df['is_isbn13'].sum()\n",
    "validos = df['is_valid'].sum()\n",
    "invalidos = com_isbn - validos\n",
    "\n",
    "print(f\"\\n📊 Resumo:\")\n",
    "print(f\"   Total de livros: {total:,}\")\n",
    "print(f\"   Com ISBN (não nulo): {com_isbn:,} ({com_isbn/total*100:.1f}%)\")\n",
    "print(f\"   Sem ISBN: {total - com_isbn:,} ({(total-com_isbn)/total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📏 Tipos encontrados:\")\n",
    "print(f\"   ISBN-10: {isbn10_count:,} ({isbn10_valid:,} válidos)\")\n",
    "print(f\"   ISBN-13: {isbn13_count:,} ({isbn13_valid:,} válidos)\")\n",
    "print(f\"   Outros tamanhos: {com_isbn - isbn10_count - isbn13_count:,}\")\n",
    "\n",
    "print(f\"\\n✅ Validação:\")\n",
    "print(f\"   ISBNs válidos (10 ou 13): {validos:,} ({validos/com_isbn*100:.1f}%)\")\n",
    "print(f\"   ISBNs inválidos: {invalidos:,} ({invalidos/com_isbn*100:.1f}%)\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. DETECTAR DUPLICATAS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"🔍 ANÁLISE DE DUPLICATAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Contar duplicatas (usando ISBN limpo)\n",
    "df_com_isbn = df[df['isbn_clean'].notna()]\n",
    "duplicatas = df_com_isbn['isbn_clean'].value_counts()\n",
    "duplicatas = duplicatas[duplicatas > 1]\n",
    "\n",
    "print(f\"\\nISBNs duplicados: {len(duplicatas):,}\")\n",
    "print(f\"Total de ocorrências duplicadas: {duplicatas.sum():,}\")\n",
    "\n",
    "if len(duplicatas) > 0:\n",
    "    # Criar DataFrame de duplicatas\n",
    "    df_dupes = df[df['isbn_clean'].isin(duplicatas.index)].copy()\n",
    "    df_dupes = df_dupes.sort_values('isbn_clean')\n",
    "\n",
    "    print(f\"\\n📋 Top 10 ISBNs mais duplicados:\")\n",
    "    for isbn, count in duplicatas.head(10).items():\n",
    "        print(f\"   {isbn}: {count} ocorrências\")\n",
    "\n",
    "    # Exibir tabela de duplicatas\n",
    "    print(f\"\\n📚 Tabela de livros duplicados (primeiros 30):\")\n",
    "    cols_mostrar = ['isbn_clean', 'isbn_length', 'is_valid', 'name', 'authors']\n",
    "    cols_mostrar = [c for c in cols_mostrar if c in df_dupes.columns]\n",
    "    if len(cols_mostrar) > 0:\n",
    "        print(df_dupes[cols_mostrar].head(30).to_string())\n",
    "else:\n",
    "    print(\"\\n✅ Nenhuma duplicata encontrada!\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. AMOSTRAS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"📋 AMOSTRAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ISBNs inválidos\n",
    "invalidos_df = df[df['isbn_clean'].notna() & ~df['is_valid']]\n",
    "if len(invalidos_df) > 0:\n",
    "    print(f\"\\n⚠️  ISBNs inválidos ({len(invalidos_df)} encontrados):\")\n",
    "    cols = ['isbn', 'isbn_clean', 'isbn_length', 'name']\n",
    "    cols = [c for c in cols if c in invalidos_df.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(invalidos_df[cols].head(10).to_string())\n",
    "else:\n",
    "    print(\"\\n✅ Todos os ISBNs são válidos!\")\n",
    "\n",
    "# ISBNs válidos por tipo\n",
    "validos_10 = df[df['is_isbn10']]\n",
    "validos_13 = df[df['is_isbn13']]\n",
    "\n",
    "if len(validos_10) > 0:\n",
    "    print(f\"\\n✅ ISBNs-10 válidos ({len(validos_10)} encontrados):\")\n",
    "    cols = ['isbn_clean', 'name', 'authors']\n",
    "    cols = [c for c in cols if c in validos_10.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(validos_10[cols].head(5).to_string())\n",
    "\n",
    "if len(validos_13) > 0:\n",
    "    print(f\"\\n✅ ISBNs-13 válidos ({len(validos_13)} encontrados):\")\n",
    "    cols = ['isbn_clean', 'name', 'authors']\n",
    "    cols = [c for c in cols if c in validos_13.columns]\n",
    "    if len(cols) > 0:\n",
    "        print(validos_13[cols].head(5).to_string())\n",
    "\n",
    "# =====================================================\n",
    "# 6. EXPORTAR\n",
    "# =====================================================\n",
    "\n",
    "def exportar_resultados():\n",
    "    \"\"\"Exporta relatórios de ISBN para pasta exports/duplicatas\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"💾 EXPORTANDO RESULTADOS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Relatório completo\n",
    "    cols_export = ['id', 'name', 'authors', 'isbn', 'isbn_clean',\n",
    "                   'isbn_length', 'is_isbn10', 'is_isbn13', 'is_valid']\n",
    "    cols_export = [c for c in cols_export if c in df.columns]\n",
    "\n",
    "    complete_file = OUTPUT_DIR / 'isbn_relatorio_completo.csv'\n",
    "    df[cols_export].to_csv(complete_file, index=False)\n",
    "    print(f\"✅ Relatório completo salvo: {complete_file}\")\n",
    "    print(f\"   {len(df):,} registros salvos\")\n",
    "\n",
    "    # Apenas duplicatas\n",
    "    if len(duplicatas) > 0:\n",
    "        dupes_file = OUTPUT_DIR / 'isbn_duplicatas.csv'\n",
    "        df_dupes.to_csv(dupes_file, index=False)\n",
    "        print(f\"✅ Duplicatas salvas: {dupes_file}\")\n",
    "        print(f\"   {len(df_dupes):,} registros duplicados salvos\")\n",
    "\n",
    "    # Apenas inválidos\n",
    "    if len(invalidos_df) > 0:\n",
    "        invalid_file = OUTPUT_DIR / 'isbn_invalidos.csv'\n",
    "        invalidos_df.to_csv(invalid_file, index=False)\n",
    "        print(f\"✅ Inválidos salvos: {invalid_file}\")\n",
    "        print(f\"   {len(invalidos_df):,} registros inválidos salvos\")\n",
    "\n",
    "    # Estatísticas resumidas\n",
    "    stats = {\n",
    "        \"Métrica\": [\n",
    "            \"Total de livros\",\n",
    "            \"Com ISBN (não nulo)\",\n",
    "            \"ISBNs válidos\",\n",
    "            \"ISBNs inválidos\",\n",
    "            \"ISBN-10 válidos\",\n",
    "            \"ISBN-13 válidos\",\n",
    "            \"ISBNs duplicados\",\n",
    "            \"Total de duplicatas\"\n",
    "        ],\n",
    "        \"Valor\": [\n",
    "            len(df),\n",
    "            df['isbn_clean'].notna().sum(),\n",
    "            df['is_valid'].sum(),\n",
    "            (df['isbn_clean'].notna() & ~df['is_valid']).sum(),\n",
    "            df['is_isbn10'].sum(),\n",
    "            df['is_isbn13'].sum(),\n",
    "            len(duplicatas),\n",
    "            duplicatas.sum() if len(duplicatas) > 0 else 0\n",
    "        ]\n",
    "    }\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_file = OUTPUT_DIR / 'isbn_statistics.csv'\n",
    "    stats_df.to_csv(stats_file, index=False)\n",
    "    print(f\"✅ Estatísticas salvas: {stats_file}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"📁 Todos os arquivos salvos em: {OUTPUT_DIR.absolute()}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Executar exportação\n",
    "exportar_resultados()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ Análise concluída!\")\n",
    "print(\"=\" * 70)"
   ],
   "id": "f30b4920f3657677",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📚 VALIDAÇÃO E ANÁLISE DE ISBN\n",
      "======================================================================\n",
      "📂 Lendo dados de: exports\\clean_data\n",
      "📁 Salvando em: exports\\duplicatas\n",
      "\n",
      "❌ Erro: Arquivo não encontrado: exports\\clean_data\\book_data_clean.csv\n",
      "   Execute primeiro o script de limpeza de dados!\n",
      "\n",
      "⚠️  Tentando carregar dataset original...\n",
      "✅ Dataset original encontrado: data\\book1-100k.csv\n",
      "✅ Coluna 'isbn' encontrada\n",
      "\n",
      "🔍 Analisando ISBNs...\n",
      "======================================================================\n",
      "\n",
      "📊 Resumo:\n",
      "   Total de livros: 58,292\n",
      "   Com ISBN (não nulo): 57,746 (99.1%)\n",
      "   Sem ISBN: 546 (0.9%)\n",
      "\n",
      "📏 Tipos encontrados:\n",
      "   ISBN-10: 57,738 (57,730 válidos)\n",
      "   ISBN-13: 0 (0 válidos)\n",
      "   Outros tamanhos: 8\n",
      "\n",
      "✅ Validação:\n",
      "   ISBNs válidos (10 ou 13): 57,730 (100.0%)\n",
      "   ISBNs inválidos: 16 (0.0%)\n",
      "\n",
      "======================================================================\n",
      "🔍 ANÁLISE DE DUPLICATAS\n",
      "======================================================================\n",
      "\n",
      "ISBNs duplicados: 194\n",
      "Total de ocorrências duplicadas: 388\n",
      "\n",
      "📋 Top 10 ISBNs mais duplicados:\n",
      "   038072023X: 2 ocorrências\n",
      "   0380715732: 2 ocorrências\n",
      "   0380725355: 2 ocorrências\n",
      "   006103097X: 2 ocorrências\n",
      "   0380763621: 2 ocorrências\n",
      "   0380720248: 2 ocorrências\n",
      "   0380709937: 2 ocorrências\n",
      "   038076363X: 2 ocorrências\n",
      "   0380715740: 2 ocorrências\n",
      "   0380709945: 2 ocorrências\n",
      "\n",
      "📚 Tabela de livros duplicados (primeiros 30):\n",
      "       isbn_clean  isbn_length  is_valid                                                                                                                                                                     name               authors\n",
      "49794  006053818X           10      True  Everyone Comes to Elaine's: Forty Years of Movie Stars, All-Stars, Literary Lions, Financial Scions, Top Cops, Politicians, and Power Brokers at the Legendary Hot Spot         A.E. Hotchner\n",
      "49877  006053818X           10      True  Everyone Comes to Elaine's: Forty Years of Movie Stars, All-Stars, Literary Lions, Financial Scions, Top Cops, Politicians, and Power Brokers at the Legendary Hot Spot         A.E. Hotchner\n",
      "44115  0060610352           10      True                                                                                   The God We Never Knew: Beyond Dogmatic Religion to a More Authentic Contemporary Faith        Marcus J. Borg\n",
      "44225  0060610352           10      True                                                                                   The God We Never Knew: Beyond Dogmatic Religion to a More Authentic Contemporary Faith        Marcus J. Borg\n",
      "44140  0060611391           10      True                                                                                                                                      Wishful Thinking: A Theological ABC    Frederick Buechner\n",
      "44255  0060611391           10      True                                                                                                                                      Wishful Thinking: A Theological ABC    Frederick Buechner\n",
      "44137  0060611413           10      True                                                                                                                                                       Peculiar Treasures    Frederick Buechner\n",
      "44251  0060611413           10      True                                                                                                                                                       Peculiar Treasures    Frederick Buechner\n",
      "44254  0060611561           10      True                                                                                                         Telling the Truth: The Gospel as Tragedy, Comedy, and Fairy Tale    Frederick Buechner\n",
      "44139  0060611561           10      True                                                                                                         Telling the Truth: The Gospel as Tragedy, Comedy, and Fairy Tale    Frederick Buechner\n",
      "44256  006061160X           10      True                                                                                                                                                        The Wizard's Tide    Frederick Buechner\n",
      "44141  006061160X           10      True                                                                                                                                                        The Wizard's Tide    Frederick Buechner\n",
      "44134  0060611626           10      True                                                                                                                                                                   Godric    Frederick Buechner\n",
      "44248  0060611626           10      True                                                                                                                                                                   Godric    Frederick Buechner\n",
      "44250  006061174X           10      True                                                                                                                                                   The Magnificent Defeat    Frederick Buechner\n",
      "44136  006061174X           10      True                                                                                                                                                   The Magnificent Defeat    Frederick Buechner\n",
      "44249  0060611758           10      True                                                                                                                                                       The Hungering Dark    Frederick Buechner\n",
      "44135  0060611758           10      True                                                                                                                                                       The Hungering Dark    Frederick Buechner\n",
      "44247  0060611782           10      True                                                                                                                                                                  Brendan    Frederick Buechner\n",
      "44133  0060611782           10      True                                                                                                                                                                  Brendan    Frederick Buechner\n",
      "44313  0060616598           10      True                                                                                                                                                The Birth of Christianity  John Dominic Crossan\n",
      "44189  0060616598           10      True                                                                                                                                                The Birth of Christianity  John Dominic Crossan\n",
      "49814  0060724552           10      True                                                                                                                                Party Princess (The Princess Diaries, #7)             Meg Cabot\n",
      "49897  0060724552           10      True                                                                                                                                Party Princess (The Princess Diaries, #7)             Meg Cabot\n",
      "49893  0060724560           10      True                                                                                                                         Princess on the Brink (The Princess Diaries, #8)             Meg Cabot\n",
      "49810  0060724560           10      True                                                                                                                         Princess on the Brink (The Princess Diaries, #8)             Meg Cabot\n",
      "49809  0060898488           10      True                                                                                                                                                        Portrait in Sepia        Isabel Allende\n",
      "49892  0060898488           10      True                                                                                                                                                        Portrait in Sepia        Isabel Allende\n",
      "49790  0060905859           10      True                                                                                                                                          Karl Marx: His Life and Thought        David McLellan\n",
      "49873  0060905859           10      True                                                                                                                                          Karl Marx: His Life and Thought        David McLellan\n",
      "\n",
      "======================================================================\n",
      "📋 AMOSTRAS\n",
      "======================================================================\n",
      "\n",
      "⚠️  ISBNs inválidos (16 encontrados):\n",
      "             isbn  isbn_clean  isbn_length                                                            name\n",
      "2228   0312349486  0312349486           10                              Twelve Sharp (Stephanie Plum, #12)\n",
      "3602    188098510   188098510            9                     Art to Choke Hearts and Pissing in the Gene\n",
      "6952    084386874   084386874            9                                        Rejoice (Redemption, #4)\n",
      "7774   189708210x   189708210            9           Bookclub in a Box Discusses the Novel The Corrections\n",
      "11400  043938950x   043938950            9                           Getting the Girl (Wolfe Brothers, #3)\n",
      "14674  043985623x   043985623            9                                                       Clockwork\n",
      "17659  1591854135  1591854135           10  The Bait Of Satan: Living Free from the Deadly Trap of Offense\n",
      "22269  9781903254  9781903254           10            The Gospel of Filth: A Bible of Decadence & Darkness\n",
      "25177  4490249512  4490249512           10                     The Currents of Space (Galactic Empire, #2)\n",
      "28674  8486478698  8486478698           10             Las Cenizas de Angela (Angela's Ashes): Una Memoria\n",
      "\n",
      "✅ ISBNs-10 válidos (57730 encontrados):\n",
      "   isbn_clean                                                                                     name                 authors\n",
      "1  0439358078                             Harry Potter and the Order of the Phoenix (Harry Potter, #5)            J.K. Rowling\n",
      "3  0439554896                               Harry Potter and the Chamber of Secrets (Harry Potter, #2)            J.K. Rowling\n",
      "4  043965548X                              Harry Potter and the Prisoner of Azkaban (Harry Potter, #3)            J.K. Rowling\n",
      "6  0439682584                                   Harry Potter Boxed Set, Books 1-5 (Harry Potter, #1-5)            J.K. Rowling\n",
      "7  0976540606  Unauthorized Harry Potter Book Seven News: \"Half-Blood Prince\" Analysis and Speculation  W. Frederick Zimmerman\n",
      "\n",
      "======================================================================\n",
      "💾 EXPORTANDO RESULTADOS\n",
      "======================================================================\n",
      "✅ Relatório completo salvo: exports\\duplicatas\\isbn_relatorio_completo.csv\n",
      "   58,292 registros salvos\n",
      "✅ Duplicatas salvas: exports\\duplicatas\\isbn_duplicatas.csv\n",
      "   388 registros duplicados salvos\n",
      "✅ Inválidos salvos: exports\\duplicatas\\isbn_invalidos.csv\n",
      "   16 registros inválidos salvos\n",
      "✅ Estatísticas salvas: exports\\duplicatas\\isbn_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "📁 Todos os arquivos salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\duplicatas\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "✅ Análise concluída!\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detecção de duplicatas de títulos com embeddings simples (TF-IDF n-gramas)",
   "id": "bb64c38064557765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:46:58.302868Z",
     "start_time": "2025-10-27T18:45:06.362169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# 1. CONFIGURAÇÃO\n",
    "# ============================================================\n",
    "\n",
    "import os, re, unicodedata, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Colunas candidatas\n",
    "title_candidates = [\"title\", \"book_title\", \"name\"]\n",
    "id_candidates    = [\"bookid\", \"book_id\", \"id\"]\n",
    "\n",
    "# Criar pasta para exportações\n",
    "OUTPUT_DIR = Path(\"exports/duplicatas\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Arquivo:\", path)\n",
    "print(f\"Pasta de exportação: {OUTPUT_DIR}\")\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 2. LEITURA E PREPARAÇÃO\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Erro ao ler {path}: {e}\")\n",
    "\n",
    "# Padroniza nomes de colunas\n",
    "df.columns = [c.strip().lower().replace(\"  \", \" \").replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "def pick(cands, cols): return next((c for c in cands if c in cols), None)\n",
    "\n",
    "title_col = pick(title_candidates, df.columns)\n",
    "id_col = pick(id_candidates, df.columns)\n",
    "\n",
    "if title_col is None:\n",
    "    raise ValueError(\"Nenhuma coluna de título encontrada.\")\n",
    "if id_col is None:\n",
    "    id_col = \"__row_id__\"\n",
    "    df[id_col] = np.arange(len(df))\n",
    "\n",
    "print(f\"✅ Coluna de título: '{title_col}'\")\n",
    "print(f\"✅ Coluna de ID: '{id_col}'\")\n",
    "print(f\"📊 Total de livros: {len(df):,}\")\n",
    "print()\n",
    "\n",
    "# Normalização de texto\n",
    "def normalize_text(t):\n",
    "    t = str(t).strip().lower()\n",
    "    t = unicodedata.normalize(\"NFKD\", t).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df[\"__title_norm__\"] = df[title_col].astype(str).apply(normalize_text)\n",
    "print(\"📝 Exemplo de títulos normalizados:\")\n",
    "display(df[[title_col, \"__title_norm__\"]].head(10))\n",
    "\n",
    "# ============================================================\n",
    "# 3. VETORIZAÇÃO TF-IDF + SIMILARIDADE COSSENO\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"\\n🔍 Calculando similaridades...\")\n",
    "corpus = df[\"__title_norm__\"].fillna(\"\")\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "n_neighbors = 10 if len(df) <= 20000 else 5\n",
    "nn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "nn.fit(X)\n",
    "\n",
    "distances, indices = nn.kneighbors(X, n_neighbors=n_neighbors, return_distance=True)\n",
    "print(f\"✅ Matriz de vizinhos calculada: {distances.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. MONTAR PARES COM SIMILARIDADE\n",
    "# ============================================================\n",
    "\n",
    "similarity_threshold = 0.15  # distância ≤ 0.15 → similaridade ≥ 0.85\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(df)):\n",
    "    for k in range(1, indices.shape[1]):\n",
    "        j = indices[i, k]\n",
    "        d = distances[i, k]\n",
    "        if np.isfinite(d) and d <= similarity_threshold:\n",
    "            a, b = int(df.iloc[i][id_col]), int(df.iloc[j][id_col])\n",
    "            if a != b:\n",
    "                i_, j_ = sorted([a, b])\n",
    "                pairs.append((i_, j_, float(1 - d)))\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[f\"{id_col}_a\", f\"{id_col}_b\", \"cosine_similarity\"]).drop_duplicates()\n",
    "print(f\"\\n✅ Total de pares similares encontrados: {len(pairs_df):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. MOSTRAR TÍTULOS SEMELHANTES\n",
    "# ============================================================\n",
    "\n",
    "left  = df[[id_col, \"__title_norm__\", title_col]].rename(columns={id_col: f\"{id_col}_a\", \"__title_norm__\":\"title_norm_a\", title_col:\"title_a\"})\n",
    "right = df[[id_col, \"__title_norm__\", title_col]].rename(columns={id_col: f\"{id_col}_b\", \"__title_norm__\":\"title_norm_b\", title_col:\"title_b\"})\n",
    "\n",
    "pairs_details = pairs_df.merge(left, on=f\"{id_col}_a\").merge(right, on=f\"{id_col}_b\")\n",
    "\n",
    "print(\"\\n📋 Amostra de pares similares:\")\n",
    "display(pairs_details.head(30))\n",
    "\n",
    "# ============================================================\n",
    "# 6. CLUSTERS DE DUPLICATAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n🔗 Agrupando duplicatas em clusters...\")\n",
    "\n",
    "parent = {}\n",
    "def find(x):\n",
    "    parent.setdefault(x, x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(a, b):\n",
    "    ra, rb = find(a), find(b)\n",
    "    if ra != rb:\n",
    "        parent[rb] = ra\n",
    "\n",
    "for _, r in pairs_df.iterrows():\n",
    "    union(r[f\"{id_col}_a\"], r[f\"{id_col}_b\"])\n",
    "\n",
    "clusters = {}\n",
    "for node in df[id_col]:\n",
    "    root = find(node)\n",
    "    clusters.setdefault(root, []).append(node)\n",
    "\n",
    "clusters_df = pd.DataFrame(\n",
    "    [{\"cluster_id\": k, \"size\": len(v), \"members\": v} for k,v in clusters.items() if len(v)>1]\n",
    ").sort_values(\"size\", ascending=False)\n",
    "\n",
    "print(f\"✅ Clusters detectados: {len(clusters_df):,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. ESTATÍSTICAS DETALHADAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 ESTATÍSTICAS DE DUPLICATAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estatísticas gerais\n",
    "total_livros = len(df)\n",
    "livros_com_duplicatas = sum([len(members) for members in clusters.values() if len(members) > 1])\n",
    "livros_unicos = total_livros - livros_com_duplicatas\n",
    "\n",
    "print(f\"\\n📚 Visão Geral:\")\n",
    "print(f\"   Total de livros no dataset: {total_livros:,}\")\n",
    "print(f\"   Livros únicos (sem duplicatas): {livros_unicos:,} ({livros_unicos/total_livros*100:.1f}%)\")\n",
    "print(f\"   Livros com duplicatas: {livros_com_duplicatas:,} ({livros_com_duplicatas/total_livros*100:.1f}%)\")\n",
    "print(f\"   Total de clusters de duplicatas: {len(clusters_df):,}\")\n",
    "\n",
    "# Estatísticas de clusters\n",
    "print(f\"\\n🔗 Clusters:\")\n",
    "print(f\"   Maior cluster: {clusters_df['size'].max()} livros\")\n",
    "print(f\"   Menor cluster: {clusters_df['size'].min()} livros\")\n",
    "print(f\"   Média de livros por cluster: {clusters_df['size'].mean():.1f}\")\n",
    "print(f\"   Mediana: {clusters_df['size'].median():.0f}\")\n",
    "\n",
    "# Distribuição de tamanhos\n",
    "print(f\"\\n📊 Distribuição de Tamanhos de Clusters:\")\n",
    "size_dist = clusters_df['size'].value_counts().sort_index()\n",
    "for size, count in size_dist.head(10).items():\n",
    "    print(f\"   {size} duplicatas: {count:,} clusters\")\n",
    "\n",
    "# Top clusters com títulos\n",
    "print(f\"\\n🏆 TOP 10 MAIORES CLUSTERS DE DUPLICATAS:\")\n",
    "print(\"-\" * 70)\n",
    "for idx, row in clusters_df.head(10).iterrows():\n",
    "    cluster_ids = row['members']\n",
    "    cluster_books = df[df[id_col].isin(cluster_ids)]\n",
    "\n",
    "    # Pegar títulos do cluster\n",
    "    titles = cluster_books[title_col].unique()\n",
    "    main_title = titles[0] if len(titles) > 0 else \"N/A\"\n",
    "    if len(main_title) > 60:\n",
    "        main_title = main_title[:60] + \"...\"\n",
    "\n",
    "    print(f\"#{idx+1:2d} | {row['size']:3d} livros | Cluster {row['cluster_id']:6.0f}\")\n",
    "    print(f\"     Título: {main_title}\")\n",
    "\n",
    "    # Mostrar variações de título se houver\n",
    "    if len(titles) > 1:\n",
    "        print(f\"     Variações: {len(titles)} títulos diferentes no cluster\")\n",
    "    print()\n",
    "\n",
    "# Estatísticas de similaridade\n",
    "print(f\"📈 Similaridade dos Pares:\")\n",
    "print(f\"   Total de pares encontrados: {len(pairs_df):,}\")\n",
    "print(f\"   Similaridade média: {pairs_df['cosine_similarity'].mean():.3f}\")\n",
    "print(f\"   Similaridade mínima: {pairs_df['cosine_similarity'].min():.3f}\")\n",
    "print(f\"   Similaridade máxima: {pairs_df['cosine_similarity'].max():.3f}\")\n",
    "\n",
    "# Distribuição de similaridade\n",
    "print(f\"\\n📊 Distribuição de Similaridade:\")\n",
    "bins = [0.85, 0.90, 0.95, 0.98, 1.00]\n",
    "labels = ['0.85-0.90', '0.90-0.95', '0.95-0.98', '0.98-1.00']\n",
    "pairs_df['sim_range'] = pd.cut(pairs_df['cosine_similarity'], bins=bins, labels=labels, include_lowest=True)\n",
    "sim_dist = pairs_df['sim_range'].value_counts().sort_index()\n",
    "for range_label, count in sim_dist.items():\n",
    "    pct = count / len(pairs_df) * 100\n",
    "    print(f\"   {range_label}: {count:,} pares ({pct:.1f}%)\")\n",
    "\n",
    "# Pares mais similares\n",
    "print(f\"\\n🎯 TOP 10 PARES MAIS SIMILARES:\")\n",
    "print(\"-\" * 70)\n",
    "top_similar = pairs_details.nlargest(10, 'cosine_similarity')\n",
    "for i, (idx, row) in enumerate(top_similar.iterrows(), 1):\n",
    "    title_a = row['title_a']\n",
    "    title_b = row['title_b']\n",
    "\n",
    "    # Truncar se muito longo\n",
    "    if len(title_a) > 50:\n",
    "        title_a = title_a[:50] + \"...\"\n",
    "    if len(title_b) > 50:\n",
    "        title_b = title_b[:50] + \"...\"\n",
    "\n",
    "    print(f\"#{i:2d} | Similaridade: {row['cosine_similarity']:.4f}\")\n",
    "    print(f\"     A: {title_a}\")\n",
    "    print(f\"     B: {title_b}\")\n",
    "    print()\n",
    "\n",
    "# Análise de autores em duplicatas (se coluna existir)\n",
    "if 'authors' in df.columns:\n",
    "    print(f\"👤 Análise de Autores em Duplicatas:\")\n",
    "    duplicated_ids = [id for members in clusters.values() if len(members) > 1 for id in members]\n",
    "    duplicated_books = df[df[id_col].isin(duplicated_ids)]\n",
    "\n",
    "    # Autores com mais duplicatas\n",
    "    author_dupes = duplicated_books['authors'].value_counts().head(10)\n",
    "    print(f\"\\n   Top 10 autores com mais duplicatas:\")\n",
    "    for author, count in author_dupes.items():\n",
    "        author_display = author[:40] + \"...\" if len(str(author)) > 40 else author\n",
    "        print(f\"   {author_display}: {count} livros duplicados\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 8. EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💾 EXPORTANDO RESULTADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Exportar pares\n",
    "pairs_file = OUTPUT_DIR / \"title_near_duplicates_pairs.csv\"\n",
    "pairs_details.to_csv(pairs_file, index=False)\n",
    "print(f\"✅ Pares exportados: {pairs_file}\")\n",
    "print(f\"   {len(pairs_details):,} pares salvos\")\n",
    "\n",
    "# Exportar clusters\n",
    "clusters_file = OUTPUT_DIR / \"title_near_duplicates_clusters.csv\"\n",
    "clusters_df.to_csv(clusters_file, index=False)\n",
    "print(f\"✅ Clusters exportados: {clusters_file}\")\n",
    "print(f\"   {len(clusters_df):,} clusters salvos\")\n",
    "\n",
    "# Exportar livros duplicados com detalhes\n",
    "duplicated_ids = [id for members in clusters.values() if len(members) > 1 for id in members]\n",
    "duplicated_books = df[df[id_col].isin(duplicated_ids)].copy()\n",
    "\n",
    "# Adicionar informação do cluster\n",
    "id_to_cluster = {}\n",
    "for cluster_id, members in clusters.items():\n",
    "    if len(members) > 1:\n",
    "        for member_id in members:\n",
    "            id_to_cluster[member_id] = cluster_id\n",
    "\n",
    "duplicated_books['cluster_id'] = duplicated_books[id_col].map(id_to_cluster)\n",
    "duplicated_books['cluster_size'] = duplicated_books['cluster_id'].map(\n",
    "    {k: len(v) for k, v in clusters.items()}\n",
    ")\n",
    "\n",
    "duplicated_books_sorted = duplicated_books.sort_values(['cluster_size', 'cluster_id'], ascending=[False, True])\n",
    "duplicated_file = OUTPUT_DIR / \"books_with_duplicates.csv\"\n",
    "duplicated_books_sorted.to_csv(duplicated_file, index=False)\n",
    "print(f\"✅ Livros duplicados exportados: {duplicated_file}\")\n",
    "print(f\"   {len(duplicated_books_sorted):,} livros com duplicatas salvos\")\n",
    "\n",
    "# Exportar resumo estatístico\n",
    "stats = {\n",
    "    \"Métrica\": [\n",
    "        \"Total de livros\",\n",
    "        \"Livros únicos\",\n",
    "        \"Livros com duplicatas\",\n",
    "        \"% com duplicatas\",\n",
    "        \"Total de clusters\",\n",
    "        \"Maior cluster\",\n",
    "        \"Média de livros por cluster\",\n",
    "        \"Total de pares similares\",\n",
    "        \"Similaridade média\"\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        total_livros,\n",
    "        livros_unicos,\n",
    "        livros_com_duplicatas,\n",
    "        f\"{livros_com_duplicatas/total_livros*100:.1f}%\",\n",
    "        len(clusters_df),\n",
    "        clusters_df['size'].max(),\n",
    "        f\"{clusters_df['size'].mean():.1f}\",\n",
    "        len(pairs_df),\n",
    "        f\"{pairs_df['cosine_similarity'].mean():.3f}\"\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_file = OUTPUT_DIR / \"duplicates_statistics.csv\"\n",
    "stats_df.to_csv(stats_file, index=False)\n",
    "print(f\"✅ Estatísticas exportadas: {stats_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ ANÁLISE CONCLUÍDA!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n📁 Todos os arquivos foram salvos em: {OUTPUT_DIR.absolute()}\")"
   ],
   "id": "8fe428b6af003ee7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\data\\book1-100k.csv\n",
      "Pasta de exportação: exports\\duplicatas\n",
      "\n",
      "✅ Coluna de título: 'name'\n",
      "✅ Coluna de ID: 'id'\n",
      "📊 Total de livros: 58,292\n",
      "\n",
      "📝 Exemplo de títulos normalizados:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                name  \\\n",
       "0  Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1  Harry Potter and the Order of the Phoenix (Har...   \n",
       "2  Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "3  Harry Potter and the Chamber of Secrets (Harry...   \n",
       "4  Harry Potter and the Prisoner of Azkaban (Harr...   \n",
       "5  Harry Potter and the Goblet of Fire (Harry Pot...   \n",
       "6  Harry Potter Boxed Set, Books 1-5 (Harry Potte...   \n",
       "7  Unauthorized Harry Potter Book Seven News: \"Ha...   \n",
       "8       Harry Potter Collection (Harry Potter, #1-6)   \n",
       "9  The Ultimate Hitchhiker's Guide: Five Complete...   \n",
       "\n",
       "                                      __title_norm__  \n",
       "0  harry potter and the half blood prince harry p...  \n",
       "1  harry potter and the order of the phoenix harr...  \n",
       "2  harry potter and the sorcerer s stone harry po...  \n",
       "3  harry potter and the chamber of secrets harry ...  \n",
       "4  harry potter and the prisoner of azkaban harry...  \n",
       "5  harry potter and the goblet of fire harry pott...  \n",
       "6  harry potter boxed set books 1 5 harry potter 1 5  \n",
       "7  unauthorized harry potter book seven news half...  \n",
       "8           harry potter collection harry potter 1 6  \n",
       "9  the ultimate hitchhiker s guide five complete ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>__title_norm__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>harry potter and the sorcerer s stone harry po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets (Harry...</td>\n",
       "      <td>harry potter and the chamber of secrets harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban (Harr...</td>\n",
       "      <td>harry potter and the prisoner of azkaban harry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Goblet of Fire (Harry Pot...</td>\n",
       "      <td>harry potter and the goblet of fire harry pott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter Boxed Set, Books 1-5 (Harry Potte...</td>\n",
       "      <td>harry potter boxed set books 1 5 harry potter 1 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Unauthorized Harry Potter Book Seven News: \"Ha...</td>\n",
       "      <td>unauthorized harry potter book seven news half...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Harry Potter Collection (Harry Potter, #1-6)</td>\n",
       "      <td>harry potter collection harry potter 1 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>the ultimate hitchhiker s guide five complete ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Calculando similaridades...\n",
      "✅ Matriz de vizinhos calculada: (58292, 5)\n",
      "\n",
      "✅ Total de pares similares encontrados: 3,074\n",
      "\n",
      "📋 Amostra de pares similares:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    id_a   id_b  cosine_similarity  \\\n",
       "0      1  70367           1.000000   \n",
       "1      1  93124           1.000000   \n",
       "2      2  77522           1.000000   \n",
       "3      2  70356           0.875292   \n",
       "4      3  77523           0.902182   \n",
       "5     12     18           0.863741   \n",
       "6     13     14           0.943693   \n",
       "7     13     18           0.930903   \n",
       "8     13  79260           0.886023   \n",
       "9     13  79259           0.886023   \n",
       "10    14  17059           0.940345   \n",
       "11    14  79259           0.940345   \n",
       "12    14  79260           0.940345   \n",
       "13    18  17059           0.867549   \n",
       "14    18  79260           0.867549   \n",
       "15    18  79259           0.867549   \n",
       "16    28  10540           1.000000   \n",
       "17    31  15247           0.910844   \n",
       "18    34  92671           1.000000   \n",
       "19    34    119           0.908473   \n",
       "20    53  52539           1.000000   \n",
       "21    53  69941           0.978680   \n",
       "22    58  28807           0.901075   \n",
       "23    59  77354           1.000000   \n",
       "24    61  28807           0.873253   \n",
       "25    74     82           0.932489   \n",
       "26    82  19895           0.931149   \n",
       "27    98   6930           0.908762   \n",
       "28   103  42432           1.000000   \n",
       "29   105  53767           1.000000   \n",
       "\n",
       "                                         title_norm_a  \\\n",
       "0   harry potter and the half blood prince harry p...   \n",
       "1   harry potter and the half blood prince harry p...   \n",
       "2   harry potter and the order of the phoenix harr...   \n",
       "3   harry potter and the order of the phoenix harr...   \n",
       "4   harry potter and the sorcerer s stone harry po...   \n",
       "5   the ultimate hitchhiker s guide five complete ...   \n",
       "6   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "7   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "8   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "9   the ultimate hitchhiker s guide to the galaxy ...   \n",
       "10  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "11  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "12  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "13  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "14  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "15  the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "16                          notes from a small island   \n",
       "17    the lord of the rings the lord of the rings 1 3   \n",
       "18  the fellowship of the ring the lord of the rin...   \n",
       "19  the fellowship of the ring the lord of the rin...   \n",
       "20  guts the true stories behind hatchet and the b...   \n",
       "21  guts the true stories behind hatchet and the b...   \n",
       "22                            changeling changeling 1   \n",
       "23                                 the changeling sea   \n",
       "24                                     the changeling   \n",
       "25        the john mcphee reader john mcphee reader 1   \n",
       "26  the second john mcphee reader john mcphee read...   \n",
       "27       what to expect the first year what to expect   \n",
       "28              god emperor of dune dune chronicles 4   \n",
       "29                chapterhouse dune dune chronicles 6   \n",
       "\n",
       "                                              title_a  \\\n",
       "0   Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...   \n",
       "2   Harry Potter and the Order of the Phoenix (Har...   \n",
       "3   Harry Potter and the Order of the Phoenix (Har...   \n",
       "4   Harry Potter and the Sorcerer's Stone (Harry P...   \n",
       "5   The Ultimate Hitchhiker's Guide: Five Complete...   \n",
       "6   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "7   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "8   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "9   The Ultimate Hitchhiker's Guide to the Galaxy ...   \n",
       "10  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "11  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "12  The Hitchhiker's Guide to the Galaxy (Hitchhik...   \n",
       "13  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "14  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "15  The Ultimate Hitchhiker's Guide (Hitchhiker's ...   \n",
       "16                          Notes from a Small Island   \n",
       "17  The Lord of the Rings (The Lord of the Rings, ...   \n",
       "18  The Fellowship of the Ring (The Lord of the Ri...   \n",
       "19  The Fellowship of the Ring (The Lord of the Ri...   \n",
       "20  Guts: The True Stories behind Hatchet and the ...   \n",
       "21  Guts: The True Stories behind Hatchet and the ...   \n",
       "22                        Changeling (Changeling, #1)   \n",
       "23                                 The Changeling Sea   \n",
       "24                                     The Changeling   \n",
       "25    The John McPhee Reader (John McPhee Reader, #1)   \n",
       "26  The Second John McPhee Reader (John McPhee Rea...   \n",
       "27     What to Expect the First Year (What to Expect)   \n",
       "28          God Emperor of Dune (Dune Chronicles, #4)   \n",
       "29            Chapterhouse: Dune (Dune Chronicles #6)   \n",
       "\n",
       "                                         title_norm_b  \\\n",
       "0   harry potter and the half blood prince harry p...   \n",
       "1   harry potter and the half blood prince harry p...   \n",
       "2   harry potter and the order of the phoenix harr...   \n",
       "3           harry potter and the order of the phoenix   \n",
       "4               harry potter and the sorcerer s stone   \n",
       "5   the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "6   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "7   the ultimate hitchhiker s guide hitchhiker s g...   \n",
       "8   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "9   the hitchhiker s guide to the galaxy hitchhike...   \n",
       "10  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "11  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "12  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "13  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "14  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "15  the hitchhiker s guide to the galaxy hitchhike...   \n",
       "16                          notes from a small island   \n",
       "17                              the lord of the rings   \n",
       "18  the fellowship of the ring the lord of the rin...   \n",
       "19  the lord of the rings the art of the fellowshi...   \n",
       "20  guts the true stories behind hatchet and the b...   \n",
       "21  guts the true stories behind hatchet and the b...   \n",
       "22                                         changeling   \n",
       "23                                 the changeling sea   \n",
       "24                                         changeling   \n",
       "25  the second john mcphee reader john mcphee read...   \n",
       "26                      the second john mcphee reader   \n",
       "27                      what to expect the first year   \n",
       "28              god emperor of dune dune chronicles 4   \n",
       "29                chapterhouse dune dune chronicles 6   \n",
       "\n",
       "                                              title_b  \n",
       "0   Harry Potter and the Half-blood Prince (Harry ...  \n",
       "1   Harry Potter and the Half-Blood Prince (Harry ...  \n",
       "2   Harry Potter and the Order of the Phoenix (Har...  \n",
       "3           Harry Potter and the Order of the Phoenix  \n",
       "4               Harry Potter and the Sorcerer's Stone  \n",
       "5   The Ultimate Hitchhiker's Guide (Hitchhiker's ...  \n",
       "6   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "7   The Ultimate Hitchhiker's Guide (Hitchhiker's ...  \n",
       "8   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "9   The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "10  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "11  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "12  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "13  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "14  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "15  The Hitchhiker's Guide to the Galaxy (Hitchhik...  \n",
       "16                          Notes From A Small Island  \n",
       "17                              The Lord of the Rings  \n",
       "18  The Fellowship of the Ring (The Lord of the Ri...  \n",
       "19  The Lord of the Rings: The Art of the Fellowsh...  \n",
       "20  Guts: The True Stories Behind Hatchet and the ...  \n",
       "21  Guts: The True Stories Behind Hatchet and the ...  \n",
       "22                                         Changeling  \n",
       "23                                 The Changeling Sea  \n",
       "24                                         Changeling  \n",
       "25  The Second John McPhee Reader (John McPhee Rea...  \n",
       "26                      The Second John McPhee Reader  \n",
       "27                      What to Expect the First Year  \n",
       "28           God Emperor of Dune (Dune Chronicles #4)  \n",
       "29            Chapterhouse Dune (Dune Chronicles, #6)  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>title_norm_a</th>\n",
       "      <th>title_a</th>\n",
       "      <th>title_norm_b</th>\n",
       "      <th>title_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>70367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>93124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "      <td>harry potter and the half blood prince harry p...</td>\n",
       "      <td>Harry Potter and the Half-Blood Prince (Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>77522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>70356</td>\n",
       "      <td>0.875292</td>\n",
       "      <td>harry potter and the order of the phoenix harr...</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix (Har...</td>\n",
       "      <td>harry potter and the order of the phoenix</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>77523</td>\n",
       "      <td>0.902182</td>\n",
       "      <td>harry potter and the sorcerer s stone harry po...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone (Harry P...</td>\n",
       "      <td>harry potter and the sorcerer s stone</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0.863741</td>\n",
       "      <td>the ultimate hitchhiker s guide five complete ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide: Five Complete...</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0.943693</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0.930903</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.886023</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.886023</td>\n",
       "      <td>the ultimate hitchhiker s guide to the galaxy ...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide to the Galaxy ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>17059</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>17059</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>79260</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>79259</td>\n",
       "      <td>0.867549</td>\n",
       "      <td>the ultimate hitchhiker s guide hitchhiker s g...</td>\n",
       "      <td>The Ultimate Hitchhiker's Guide (Hitchhiker's ...</td>\n",
       "      <td>the hitchhiker s guide to the galaxy hitchhike...</td>\n",
       "      <td>The Hitchhiker's Guide to the Galaxy (Hitchhik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28</td>\n",
       "      <td>10540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>notes from a small island</td>\n",
       "      <td>Notes from a Small Island</td>\n",
       "      <td>notes from a small island</td>\n",
       "      <td>Notes From A Small Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>31</td>\n",
       "      <td>15247</td>\n",
       "      <td>0.910844</td>\n",
       "      <td>the lord of the rings the lord of the rings 1 3</td>\n",
       "      <td>The Lord of the Rings (The Lord of the Rings, ...</td>\n",
       "      <td>the lord of the rings</td>\n",
       "      <td>The Lord of the Rings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34</td>\n",
       "      <td>92671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34</td>\n",
       "      <td>119</td>\n",
       "      <td>0.908473</td>\n",
       "      <td>the fellowship of the ring the lord of the rin...</td>\n",
       "      <td>The Fellowship of the Ring (The Lord of the Ri...</td>\n",
       "      <td>the lord of the rings the art of the fellowshi...</td>\n",
       "      <td>The Lord of the Rings: The Art of the Fellowsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>53</td>\n",
       "      <td>52539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories behind Hatchet and the ...</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories Behind Hatchet and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53</td>\n",
       "      <td>69941</td>\n",
       "      <td>0.978680</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories behind Hatchet and the ...</td>\n",
       "      <td>guts the true stories behind hatchet and the b...</td>\n",
       "      <td>Guts: The True Stories Behind Hatchet and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>58</td>\n",
       "      <td>28807</td>\n",
       "      <td>0.901075</td>\n",
       "      <td>changeling changeling 1</td>\n",
       "      <td>Changeling (Changeling, #1)</td>\n",
       "      <td>changeling</td>\n",
       "      <td>Changeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>59</td>\n",
       "      <td>77354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>the changeling sea</td>\n",
       "      <td>The Changeling Sea</td>\n",
       "      <td>the changeling sea</td>\n",
       "      <td>The Changeling Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61</td>\n",
       "      <td>28807</td>\n",
       "      <td>0.873253</td>\n",
       "      <td>the changeling</td>\n",
       "      <td>The Changeling</td>\n",
       "      <td>changeling</td>\n",
       "      <td>Changeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>74</td>\n",
       "      <td>82</td>\n",
       "      <td>0.932489</td>\n",
       "      <td>the john mcphee reader john mcphee reader 1</td>\n",
       "      <td>The John McPhee Reader (John McPhee Reader, #1)</td>\n",
       "      <td>the second john mcphee reader john mcphee read...</td>\n",
       "      <td>The Second John McPhee Reader (John McPhee Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82</td>\n",
       "      <td>19895</td>\n",
       "      <td>0.931149</td>\n",
       "      <td>the second john mcphee reader john mcphee read...</td>\n",
       "      <td>The Second John McPhee Reader (John McPhee Rea...</td>\n",
       "      <td>the second john mcphee reader</td>\n",
       "      <td>The Second John McPhee Reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>98</td>\n",
       "      <td>6930</td>\n",
       "      <td>0.908762</td>\n",
       "      <td>what to expect the first year what to expect</td>\n",
       "      <td>What to Expect the First Year (What to Expect)</td>\n",
       "      <td>what to expect the first year</td>\n",
       "      <td>What to Expect the First Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>103</td>\n",
       "      <td>42432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>god emperor of dune dune chronicles 4</td>\n",
       "      <td>God Emperor of Dune (Dune Chronicles, #4)</td>\n",
       "      <td>god emperor of dune dune chronicles 4</td>\n",
       "      <td>God Emperor of Dune (Dune Chronicles #4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>105</td>\n",
       "      <td>53767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>chapterhouse dune dune chronicles 6</td>\n",
       "      <td>Chapterhouse: Dune (Dune Chronicles #6)</td>\n",
       "      <td>chapterhouse dune dune chronicles 6</td>\n",
       "      <td>Chapterhouse Dune (Dune Chronicles, #6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 Agrupando duplicatas em clusters...\n",
      "✅ Clusters detectados: 2,060\n",
      "\n",
      "======================================================================\n",
      "📊 ESTATÍSTICAS DE DUPLICATAS\n",
      "======================================================================\n",
      "\n",
      "📚 Visão Geral:\n",
      "   Total de livros no dataset: 58,292\n",
      "   Livros únicos (sem duplicatas): 53,629 (92.0%)\n",
      "   Livros com duplicatas: 4,663 (8.0%)\n",
      "   Total de clusters de duplicatas: 2,060\n",
      "\n",
      "🔗 Clusters:\n",
      "   Maior cluster: 19 livros\n",
      "   Menor cluster: 2 livros\n",
      "   Média de livros por cluster: 2.3\n",
      "   Mediana: 2\n",
      "\n",
      "📊 Distribuição de Tamanhos de Clusters:\n",
      "   2 duplicatas: 1,753 clusters\n",
      "   3 duplicatas: 208 clusters\n",
      "   4 duplicatas: 52 clusters\n",
      "   5 duplicatas: 19 clusters\n",
      "   6 duplicatas: 9 clusters\n",
      "   7 duplicatas: 5 clusters\n",
      "   8 duplicatas: 5 clusters\n",
      "   9 duplicatas: 4 clusters\n",
      "   10 duplicatas: 2 clusters\n",
      "   11 duplicatas: 1 clusters\n",
      "\n",
      "🏆 TOP 10 MAIORES CLUSTERS DE DUPLICATAS:\n",
      "----------------------------------------------------------------------\n",
      "#65 |  19 livros | Cluster    866\n",
      "     Título: Fullmetal Alchemist, Vol. 9 (Fullmetal Alchemist, #9)\n",
      "     Variações: 19 títulos diferentes no cluster\n",
      "\n",
      "#1658 |  15 livros | Cluster  74949\n",
      "     Título: Angel Sanctuary, Vol. 14\n",
      "     Variações: 15 títulos diferentes no cluster\n",
      "\n",
      "#854 |  11 livros | Cluster  23708\n",
      "     Título: Maison Ikkoku, Volume 1 (Maison Ikkoku, #1)\n",
      "     Variações: 11 títulos diferentes no cluster\n",
      "\n",
      "#565 |  10 livros | Cluster  13026\n",
      "     Título: Alice In Wonderland: Including Alice's Adventures In Wonderl...\n",
      "     Variações: 8 títulos diferentes no cluster\n",
      "\n",
      "#151 |  10 livros | Cluster  13731\n",
      "     Título: Bleach, Volume 15\n",
      "     Variações: 10 títulos diferentes no cluster\n",
      "\n",
      "#584 |   9 livros | Cluster  13568\n",
      "     Título: Tsubasa: RESERVoir CHRoNiCLE, Vol. 11\n",
      "     Variações: 9 títulos diferentes no cluster\n",
      "\n",
      "#1283 |   9 livros | Cluster  69547\n",
      "     Título: The Best American Poetry 2006\n",
      "     Variações: 9 títulos diferentes no cluster\n",
      "\n",
      "#653 |   9 livros | Cluster  15592\n",
      "     Título: McSweeney's #12\n",
      "     Variações: 9 títulos diferentes no cluster\n",
      "\n",
      "# 4 |   9 livros | Cluster  24956\n",
      "     Título: The Ultimate Hitchhiker's Guide: Five Complete Novels and On...\n",
      "     Variações: 8 títulos diferentes no cluster\n",
      "\n",
      "#705 |   8 livros | Cluster  17434\n",
      "     Título: Physics for Scientists and Engineers with Modern Physics: Vo...\n",
      "     Variações: 8 títulos diferentes no cluster\n",
      "\n",
      "📈 Similaridade dos Pares:\n",
      "   Total de pares encontrados: 3,074\n",
      "   Similaridade média: 0.940\n",
      "   Similaridade mínima: 0.850\n",
      "   Similaridade máxima: 1.000\n",
      "\n",
      "📊 Distribuição de Similaridade:\n",
      "   0.85-0.90: 860 pares (28.0%)\n",
      "   0.90-0.95: 838 pares (27.3%)\n",
      "   0.95-0.98: 269 pares (8.8%)\n",
      "   0.98-1.00: 1,107 pares (36.0%)\n",
      "\n",
      "🎯 TOP 10 PARES MAIS SIMILARES:\n",
      "----------------------------------------------------------------------\n",
      "# 1 | Similaridade: 1.0000\n",
      "     A: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "     B: Harry Potter and the Half-blood Prince (Harry Pott...\n",
      "\n",
      "# 2 | Similaridade: 1.0000\n",
      "     A: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "     B: Harry Potter and the Half-Blood Prince (Harry Pott...\n",
      "\n",
      "# 3 | Similaridade: 1.0000\n",
      "     A: Notes from a Small Island\n",
      "     B: Notes From A Small Island\n",
      "\n",
      "# 4 | Similaridade: 1.0000\n",
      "     A: The Fellowship of the Ring (The Lord of the Rings,...\n",
      "     B: The Fellowship of the Ring (The Lord of the Rings,...\n",
      "\n",
      "# 5 | Similaridade: 1.0000\n",
      "     A: The Changeling Sea\n",
      "     B: The Changeling Sea\n",
      "\n",
      "# 6 | Similaridade: 1.0000\n",
      "     A: God Emperor of Dune (Dune Chronicles, #4)\n",
      "     B: God Emperor of Dune (Dune Chronicles #4)\n",
      "\n",
      "# 7 | Similaridade: 1.0000\n",
      "     A: Heretics of Dune (Dune Chronicles, #5)\n",
      "     B: Heretics of Dune (Dune Chronicles #5)\n",
      "\n",
      "# 8 | Similaridade: 1.0000\n",
      "     A: Treasure Island\n",
      "     B: Treasure Island\n",
      "\n",
      "# 9 | Similaridade: 1.0000\n",
      "     A: Stranger in a Strange Land\n",
      "     B: Stranger in a Strange Land\n",
      "\n",
      "#10 | Similaridade: 1.0000\n",
      "     A: Time Enough for Love\n",
      "     B: Time Enough for Love\n",
      "\n",
      "👤 Análise de Autores em Duplicatas:\n",
      "\n",
      "   Top 10 autores com mais duplicatas:\n",
      "   Rumiko Takahashi: 44 livros duplicados\n",
      "   J.R.R. Tolkien: 42 livros duplicados\n",
      "   William Shakespeare: 36 livros duplicados\n",
      "   John Grisham: 34 livros duplicados\n",
      "   Lawrence Block: 28 livros duplicados\n",
      "   James Patterson: 27 livros duplicados\n",
      "   Orson Scott Card: 26 livros duplicados\n",
      "   Alexander McCall Smith: 23 livros duplicados\n",
      "   Dave Eggers: 23 livros duplicados\n",
      "   Colin Dexter: 22 livros duplicados\n",
      "\n",
      "======================================================================\n",
      "======================================================================\n",
      "💾 EXPORTANDO RESULTADOS\n",
      "======================================================================\n",
      "✅ Pares exportados: exports\\duplicatas\\title_near_duplicates_pairs.csv\n",
      "   3,094 pares salvos\n",
      "✅ Clusters exportados: exports\\duplicatas\\title_near_duplicates_clusters.csv\n",
      "   2,060 clusters salvos\n",
      "✅ Livros duplicados exportados: exports\\duplicatas\\books_with_duplicates.csv\n",
      "   4,663 livros com duplicatas salvos\n",
      "✅ Estatísticas exportadas: exports\\duplicatas\\duplicates_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "✅ ANÁLISE CONCLUÍDA!\n",
      "======================================================================\n",
      "\n",
      "📁 Todos os arquivos foram salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\duplicatas\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Criação de Dataset Limpo",
   "id": "f02fd67d4bc05111"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:47:02.246242Z",
     "start_time": "2025-10-27T18:46:58.341797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Remove duplicatas de ISBN, títulos e anos inválidos\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURAÇÃO\n",
    "# ============================================================\n",
    "\n",
    "# Criar pasta para datasets limpos\n",
    "OUTPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🧹 LIMPEZA DE DATASET - REMOÇÃO DE DUPLICATAS E ANOS INVÁLIDOS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"📁 Pasta de saída: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS ORIGINAIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"📂 Carregando dataset original...\")\n",
    "# O DataFrame df já deve estar carregado no notebook\n",
    "# Se não estiver, descomente a linha abaixo:\n",
    "# df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "df_original = df.copy()  # Fazer cópia para preservar original\n",
    "print(f\"✅ Dataset carregado: {len(df_original):,} livros\")\n",
    "print(f\"   Colunas: {list(df_original.columns)}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. IDENTIFICAR REGISTROS A REMOVER\n",
    "# ============================================================\n",
    "\n",
    "print(\"🔍 Identificando registros para remoção...\\n\")\n",
    "\n",
    "# Inicializar máscaras\n",
    "ids_to_remove = set()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.1. ANOS INVÁLIDOS (> 2020)\n",
    "# -----------------------------\n",
    "year_col = \"publishyear\"\n",
    "\n",
    "if year_col in df_original.columns:\n",
    "    invalid_years_mask = df_original[year_col] > 2020\n",
    "    invalid_years_count = invalid_years_mask.sum()\n",
    "    invalid_years_ids = df_original[invalid_years_mask]['id'].tolist()\n",
    "    ids_to_remove.update(invalid_years_ids)\n",
    "\n",
    "    print(f\"📅 Anos Inválidos (> 2020):\")\n",
    "    print(f\"   Encontrados: {invalid_years_count:,} livros\")\n",
    "\n",
    "    if invalid_years_count > 0:\n",
    "        years_dist = df_original[invalid_years_mask][year_col].value_counts().sort_index(ascending=False)\n",
    "        print(f\"   Distribuição:\")\n",
    "        for year, count in years_dist.head(5).items():\n",
    "            print(f\"      {int(year)}: {count:,} livros\")\n",
    "else:\n",
    "    print(f\"⚠️  Coluna '{year_col}' não encontrada - pulando validação de anos\")\n",
    "    invalid_years_count = 0\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.2. DUPLICATAS DE ISBN\n",
    "# -----------------------------\n",
    "isbn_col = \"isbn\"\n",
    "\n",
    "if isbn_col in df_original.columns:\n",
    "    # Considerar apenas ISBNs não nulos\n",
    "    df_with_isbn = df_original[df_original[isbn_col].notna()]\n",
    "\n",
    "    # Encontrar ISBNs duplicados\n",
    "    isbn_counts = df_with_isbn[isbn_col].value_counts()\n",
    "    duplicate_isbns = isbn_counts[isbn_counts > 1].index.tolist()\n",
    "\n",
    "    # Para cada ISBN duplicado, manter apenas o primeiro registro\n",
    "    isbn_duplicate_ids = []\n",
    "    for isbn in duplicate_isbns:\n",
    "        duplicate_records = df_original[df_original[isbn_col] == isbn]\n",
    "        # Manter o primeiro, remover os demais\n",
    "        ids_to_keep_one = duplicate_records['id'].iloc[0]\n",
    "        ids_to_remove_from_isbn = duplicate_records['id'].iloc[1:].tolist()\n",
    "        isbn_duplicate_ids.extend(ids_to_remove_from_isbn)\n",
    "\n",
    "    ids_to_remove.update(isbn_duplicate_ids)\n",
    "\n",
    "    print(f\"📚 Duplicatas de ISBN:\")\n",
    "    print(f\"   ISBNs duplicados: {len(duplicate_isbns):,}\")\n",
    "    print(f\"   Registros a remover: {len(isbn_duplicate_ids):,}\")\n",
    "    print(f\"   Registros únicos mantidos: {len(duplicate_isbns):,}\")\n",
    "\n",
    "    if len(duplicate_isbns) > 0:\n",
    "        print(f\"   Top 5 ISBNs mais duplicados:\")\n",
    "        for isbn, count in isbn_counts.head(5).items():\n",
    "            if count > 1:\n",
    "                print(f\"      {isbn}: {count} ocorrências\")\n",
    "else:\n",
    "    print(f\"⚠️  Coluna '{isbn_col}' não encontrada - pulando validação de ISBN\")\n",
    "    len_isbn_duplicate_ids = 0\n",
    "\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# 3.3. DUPLICATAS DE TÍTULO\n",
    "# -----------------------------\n",
    "# Nota: Assumindo que você já rodou a análise de duplicatas de título\n",
    "# e tem os clusters salvos. Vamos carregar se existir.\n",
    "\n",
    "title_clusters_file = Path(\"exports/duplicatas/title_near_duplicates_clusters.csv\")\n",
    "\n",
    "if title_clusters_file.exists():\n",
    "    print(f\"📖 Duplicatas de Título:\")\n",
    "    clusters_df = pd.read_csv(title_clusters_file)\n",
    "\n",
    "    # Para cada cluster, manter apenas o primeiro ID\n",
    "    title_duplicate_ids = []\n",
    "    for _, row in clusters_df.iterrows():\n",
    "        members = eval(row['members'])  # Converter string de lista para lista\n",
    "        # Manter o primeiro, remover os demais\n",
    "        title_duplicate_ids.extend(members[1:])\n",
    "\n",
    "    ids_to_remove.update(title_duplicate_ids)\n",
    "\n",
    "    print(f\"   Clusters encontrados: {len(clusters_df):,}\")\n",
    "    print(f\"   Registros a remover: {len(title_duplicate_ids):,}\")\n",
    "    print(f\"   Registros únicos mantidos: {len(clusters_df):,}\")\n",
    "else:\n",
    "    print(f\"⚠️  Arquivo de clusters não encontrado: {title_clusters_file}\")\n",
    "    print(f\"   Execute a análise de duplicatas de título primeiro\")\n",
    "    print(f\"   Pulando remoção de duplicatas de título...\")\n",
    "    title_duplicate_ids = []\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 4. REMOVER REGISTROS E GERAR DATASET LIMPO\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🧹 GERANDO DATASET LIMPO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Criar máscara de IDs a manter\n",
    "df_clean = df_original[~df_original['id'].isin(ids_to_remove)].copy()\n",
    "\n",
    "print(f\"\\n📊 Resumo da Limpeza:\")\n",
    "print(f\"   Dataset original: {len(df_original):,} livros\")\n",
    "print(f\"   \")\n",
    "print(f\"   Removidos por:\")\n",
    "print(f\"      Anos inválidos: {invalid_years_count:,}\")\n",
    "print(f\"      Duplicatas de ISBN: {len(isbn_duplicate_ids) if 'isbn_duplicate_ids' in locals() else 0:,}\")\n",
    "print(f\"      Duplicatas de título: {len(title_duplicate_ids):,}\")\n",
    "print(f\"   \")\n",
    "print(f\"   Total de IDs únicos a remover: {len(ids_to_remove):,}\")\n",
    "print(f\"   Dataset limpo: {len(df_clean):,} livros\")\n",
    "print(f\"   \")\n",
    "print(f\"   Taxa de remoção: {len(ids_to_remove)/len(df_original)*100:.2f}%\")\n",
    "print(f\"   Taxa de retenção: {len(df_clean)/len(df_original)*100:.2f}%\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. VALIDAÇÕES DO DATASET LIMPO\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n✅ Validações do Dataset Limpo:\")\n",
    "\n",
    "# Validar anos\n",
    "if year_col in df_clean.columns:\n",
    "    max_year = df_clean[year_col].max()\n",
    "    min_year = df_clean[year_col].min()\n",
    "    print(f\"   Anos: {min_year:.0f} - {max_year:.0f}\")\n",
    "    invalid_after = (df_clean[year_col] > 2020).sum()\n",
    "    print(f\"   Anos > 2020: {invalid_after} (deve ser 0)\")\n",
    "\n",
    "# Validar ISBNs duplicados\n",
    "if isbn_col in df_clean.columns:\n",
    "    isbn_dupes_after = df_clean[isbn_col].duplicated().sum()\n",
    "    print(f\"   ISBNs duplicados: {isbn_dupes_after} (deve ser 0)\")\n",
    "\n",
    "# Validar IDs únicos\n",
    "id_dupes = df_clean['id'].duplicated().sum()\n",
    "print(f\"   IDs duplicados: {id_dupes} (deve ser 0)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# 6. EXPORTAR DATASETS\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"💾 EXPORTANDO DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dataset limpo principal\n",
    "clean_file = OUTPUT_DIR / \"book_data_clean.csv\"\n",
    "df_clean.to_csv(clean_file, index=False)\n",
    "print(f\"\\n✅ Dataset limpo salvo: {clean_file}\")\n",
    "print(f\"   {len(df_clean):,} livros | {len(df_clean.columns)} colunas\")\n",
    "\n",
    "# Dataset de registros removidos (para auditoria)\n",
    "df_removed = df_original[df_original['id'].isin(ids_to_remove)].copy()\n",
    "\n",
    "# Adicionar coluna indicando o motivo da remoção\n",
    "df_removed['removal_reason'] = ''\n",
    "\n",
    "for idx, row in df_removed.iterrows():\n",
    "    reasons = []\n",
    "\n",
    "    if year_col in df_removed.columns and row[year_col] > 2020:\n",
    "        reasons.append('invalid_year')\n",
    "\n",
    "    if isbn_col in df_removed.columns and 'isbn_duplicate_ids' in locals():\n",
    "        if row['id'] in isbn_duplicate_ids:\n",
    "            reasons.append('duplicate_isbn')\n",
    "\n",
    "    if row['id'] in title_duplicate_ids:\n",
    "        reasons.append('duplicate_title')\n",
    "\n",
    "    df_removed.at[idx, 'removal_reason'] = '; '.join(reasons)\n",
    "\n",
    "removed_file = OUTPUT_DIR / \"book_data_removed.csv\"\n",
    "df_removed.to_csv(removed_file, index=False)\n",
    "print(f\"✅ Registros removidos salvos: {removed_file}\")\n",
    "print(f\"   {len(df_removed):,} livros | Para auditoria\")\n",
    "\n",
    "# Estatísticas da limpeza\n",
    "stats = {\n",
    "    \"Métrica\": [\n",
    "        \"Dataset Original\",\n",
    "        \"Anos Inválidos Removidos\",\n",
    "        \"Duplicatas ISBN Removidas\",\n",
    "        \"Duplicatas Título Removidas\",\n",
    "        \"Total Removido (IDs únicos)\",\n",
    "        \"Dataset Limpo\",\n",
    "        \"Taxa de Retenção (%)\"\n",
    "    ],\n",
    "    \"Valor\": [\n",
    "        len(df_original),\n",
    "        invalid_years_count,\n",
    "        len(isbn_duplicate_ids) if 'isbn_duplicate_ids' in locals() else 0,\n",
    "        len(title_duplicate_ids),\n",
    "        len(ids_to_remove),\n",
    "        len(df_clean),\n",
    "        f\"{len(df_clean)/len(df_original)*100:.2f}\"\n",
    "    ]\n",
    "}\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_file = OUTPUT_DIR / \"cleaning_statistics.csv\"\n",
    "stats_df.to_csv(stats_file, index=False)\n",
    "print(f\"✅ Estatísticas salvas: {stats_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. COMPARAÇÃO ANTES/DEPOIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 COMPARAÇÃO ANTES vs DEPOIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n📈 Estatísticas Gerais:\")\n",
    "print(f\"   {'Métrica':<30} {'Antes':>12} {'Depois':>12} {'Diferença':>12}\")\n",
    "print(f\"   {'-'*30} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "\n",
    "metrics = [\n",
    "    ('Total de livros', len(df_original), len(df_clean)),\n",
    "    ('Colunas', len(df_original.columns), len(df_clean.columns)),\n",
    "]\n",
    "\n",
    "if year_col in df_original.columns:\n",
    "    metrics.append(('Ano médio', df_original[year_col].mean(), df_clean[year_col].mean()))\n",
    "    metrics.append(('Ano mínimo', df_original[year_col].min(), df_clean[year_col].min()))\n",
    "    metrics.append(('Ano máximo', df_original[year_col].max(), df_clean[year_col].max()))\n",
    "\n",
    "if isbn_col in df_original.columns:\n",
    "    metrics.append(('ISBNs únicos', df_original[isbn_col].nunique(), df_clean[isbn_col].nunique()))\n",
    "    metrics.append(('ISBNs nulos', df_original[isbn_col].isna().sum(), df_clean[isbn_col].isna().sum()))\n",
    "\n",
    "for metric, before, after in metrics:\n",
    "    diff = after - before\n",
    "    if isinstance(before, float) and isinstance(after, float):\n",
    "        print(f\"   {metric:<30} {before:>12.1f} {after:>12.1f} {diff:>+12.1f}\")\n",
    "    else:\n",
    "        print(f\"   {metric:<30} {before:>12,} {after:>12,} {diff:>+12,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ LIMPEZA CONCLUÍDA!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n📁 Arquivos salvos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\nVocê pode usar o dataset limpo com:\")\n",
    "print(f\"   df_clean = pd.read_csv('{clean_file}')\")"
   ],
   "id": "56be59727f1e099a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🧹 LIMPEZA DE DATASET - REMOÇÃO DE DUPLICATAS E ANOS INVÁLIDOS\n",
      "======================================================================\n",
      "📁 Pasta de saída: exports\\clean_data\n",
      "\n",
      "📂 Carregando dataset original...\n",
      "✅ Dataset carregado: 58,292 livros\n",
      "   Colunas: ['id', 'name', 'ratingdist1', 'pagesnumber', 'ratingdist4', 'ratingdisttotal', 'publishmonth', 'publishday', 'publisher', 'countsofreview', 'publishyear', 'language', 'authors', 'rating', 'ratingdist2', 'ratingdist5', 'isbn', 'ratingdist3', '__title_norm__']\n",
      "\n",
      "🔍 Identificando registros para remoção...\n",
      "\n",
      "📅 Anos Inválidos (> 2020):\n",
      "   Encontrados: 1 livros\n",
      "   Distribuição:\n",
      "      3002: 1 livros\n",
      "\n",
      "📚 Duplicatas de ISBN:\n",
      "   ISBNs duplicados: 194\n",
      "   Registros a remover: 194\n",
      "   Registros únicos mantidos: 194\n",
      "   Top 5 ISBNs mais duplicados:\n",
      "      038072023X: 2 ocorrências\n",
      "      0380715732: 2 ocorrências\n",
      "      0380725355: 2 ocorrências\n",
      "      006103097X: 2 ocorrências\n",
      "      0380763621: 2 ocorrências\n",
      "\n",
      "📖 Duplicatas de Título:\n",
      "   Clusters encontrados: 2,060\n",
      "   Registros a remover: 2,603\n",
      "   Registros únicos mantidos: 2,060\n",
      "\n",
      "======================================================================\n",
      "🧹 GERANDO DATASET LIMPO\n",
      "======================================================================\n",
      "\n",
      "📊 Resumo da Limpeza:\n",
      "   Dataset original: 58,292 livros\n",
      "   \n",
      "   Removidos por:\n",
      "      Anos inválidos: 1\n",
      "      Duplicatas de ISBN: 194\n",
      "      Duplicatas de título: 2,603\n",
      "   \n",
      "   Total de IDs únicos a remover: 2,595\n",
      "   Dataset limpo: 55,502 livros\n",
      "   \n",
      "   Taxa de remoção: 4.45%\n",
      "   Taxa de retenção: 95.21%\n",
      "\n",
      "✅ Validações do Dataset Limpo:\n",
      "   Anos: 162 - 2020\n",
      "   Anos > 2020: 0 (deve ser 0)\n",
      "   ISBNs duplicados: 500 (deve ser 0)\n",
      "   IDs duplicados: 0 (deve ser 0)\n",
      "\n",
      "======================================================================\n",
      "💾 EXPORTANDO DATASETS\n",
      "======================================================================\n",
      "\n",
      "✅ Dataset limpo salvo: exports\\clean_data\\book_data_clean.csv\n",
      "   55,502 livros | 19 colunas\n",
      "✅ Registros removidos salvos: exports\\clean_data\\book_data_removed.csv\n",
      "   2,790 livros | Para auditoria\n",
      "✅ Estatísticas salvas: exports\\clean_data\\cleaning_statistics.csv\n",
      "\n",
      "======================================================================\n",
      "📊 COMPARAÇÃO ANTES vs DEPOIS\n",
      "======================================================================\n",
      "\n",
      "📈 Estatísticas Gerais:\n",
      "   Métrica                               Antes       Depois    Diferença\n",
      "   ------------------------------ ------------ ------------ ------------\n",
      "   Total de livros                      58,292       55,502       -2,790\n",
      "   Colunas                                  19           19           +0\n",
      "   Ano médio                            1999.5       1999.6         +0.0\n",
      "   Ano mínimo                              162          162           +0\n",
      "   Ano máximo                            3,002        2,020         -982\n",
      "   ISBNs únicos                         57,552       55,001       -2,551\n",
      "   ISBNs nulos                             546          501          -45\n",
      "\n",
      "======================================================================\n",
      "✅ LIMPEZA CONCLUÍDA!\n",
      "======================================================================\n",
      "\n",
      "📁 Arquivos salvos em: C:\\Users\\USER\\PycharmProjects\\JupyterProject\\exports\\clean_data\n",
      "\n",
      "Você pode usar o dataset limpo com:\n",
      "   df_clean = pd.read_csv('exports\\clean_data\\book_data_clean.csv')\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## POC de cluster por similaridade de Título, Autor, Ano de publicação, Editora e língua",
   "id": "2bdff6122d787e05"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-27T18:47:02.252651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Agrupa livros similares baseado em: título, autor, língua, ano, publisher\n",
    "Usa K-Means, DBSCAN e Hierarchical Clustering\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar visualização\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURAÇÃO\n",
    "# ============================================================\n",
    "\n",
    "# Pastas\n",
    "INPUT_DIR = Path(\"exports/clean_data\")\n",
    "OUTPUT_DIR = Path(\"exports/clustering\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"🤖 POC: CLUSTERING DE LIVROS SIMILARES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"📂 Dataset: {INPUT_DIR}/book_data_clean.csv\")\n",
    "print(f\"📁 Saída: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS LIMPOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"📂 Carregando dataset limpo...\")\n",
    "clean_file = INPUT_DIR / \"book_data_clean.csv\"\n",
    "\n",
    "if not clean_file.exists():\n",
    "    print(f\"❌ Erro: Arquivo não encontrado: {clean_file}\")\n",
    "    print(f\"   Execute primeiro o script de limpeza de dados!\")\n",
    "    raise FileNotFoundError(f\"Dataset limpo não encontrado: {clean_file}\")\n",
    "\n",
    "df = pd.read_csv(clean_file)\n",
    "print(f\"✅ Dataset carregado: {len(df):,} livros\")\n",
    "print(f\"   Colunas: {len(df.columns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. PREPARAÇÃO DOS DADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔧 PREPARAÇÃO DOS DADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Selecionar colunas relevantes\n",
    "features_cols = {\n",
    "    'title': 'name',\n",
    "    'author': 'authors',\n",
    "    'language': 'language',\n",
    "    'year': 'publishyear',\n",
    "    'publisher': 'publisher'\n",
    "}\n",
    "\n",
    "# Verificar quais colunas existem\n",
    "available_cols = {}\n",
    "for key, col in features_cols.items():\n",
    "    if col in df.columns:\n",
    "        available_cols[key] = col\n",
    "        print(f\"✅ {key}: '{col}'\")\n",
    "    else:\n",
    "        print(f\"⚠️  {key}: '{col}' não encontrada\")\n",
    "\n",
    "# Criar DataFrame de trabalho com colunas disponíveis\n",
    "df_work = df[list(available_cols.values()) + ['id']].copy()\n",
    "df_work = df_work.rename(columns={v: k for k, v in available_cols.items()})\n",
    "\n",
    "# Remover linhas com muitos valores nulos\n",
    "print(f\"\\n📊 Valores nulos antes da limpeza:\")\n",
    "null_counts = df_work.isnull().sum()\n",
    "for col, count in null_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {col}: {count:,} ({count/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "# Remover linhas onde título ou autor estão nulos\n",
    "df_work = df_work.dropna(subset=['title', 'author'])\n",
    "print(f\"\\n✅ Após remover nulos em título/autor: {len(df_work):,} livros\")\n",
    "\n",
    "# Preencher valores nulos restantes\n",
    "if 'language' in df_work.columns:\n",
    "    df_work['language'] = df_work['language'].fillna('unknown')\n",
    "if 'year' in df_work.columns:\n",
    "    df_work['year'] = df_work['year'].fillna(df_work['year'].median())\n",
    "if 'publisher' in df_work.columns:\n",
    "    df_work['publisher'] = df_work['publisher'].fillna('unknown')\n",
    "\n",
    "print(f\"✅ Dataset final para clustering: {len(df_work):,} livros\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. ENGENHARIA DE FEATURES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔨 ENGENHARIA DE FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 4.1 TÍTULO - TF-IDF\n",
    "print(\"\\n📝 Processando títulos com TF-IDF...\")\n",
    "title_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    lowercase=True,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "title_features = title_vectorizer.fit_transform(df_work['title'].astype(str))\n",
    "print(f\"✅ Título: {title_features.shape[1]} features\")\n",
    "\n",
    "# 4.2 AUTOR - TF-IDF\n",
    "print(\"👤 Processando autores com TF-IDF...\")\n",
    "author_vectorizer = TfidfVectorizer(\n",
    "    max_features=50,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=2,\n",
    "    lowercase=True\n",
    ")\n",
    "author_features = author_vectorizer.fit_transform(df_work['author'].astype(str))\n",
    "print(f\"✅ Autor: {author_features.shape[1]} features\")\n",
    "\n",
    "# 4.3 LÍNGUA - One-Hot Encoding\n",
    "if 'language' in df_work.columns:\n",
    "    print(\"🌍 Processando línguas...\")\n",
    "    le_lang = LabelEncoder()\n",
    "    df_work['language_encoded'] = le_lang.fit_transform(df_work['language'].astype(str))\n",
    "    language_features = pd.get_dummies(df_work['language'], prefix='lang').values\n",
    "    print(f\"✅ Língua: {language_features.shape[1]} features ({len(le_lang.classes_)} idiomas)\")\n",
    "else:\n",
    "    language_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.4 ANO - Normalizado\n",
    "if 'year' in df_work.columns:\n",
    "    print(\"📅 Processando anos...\")\n",
    "    scaler_year = StandardScaler()\n",
    "    year_features = scaler_year.fit_transform(df_work[['year']])\n",
    "    print(f\"✅ Ano: normalizado ({df_work['year'].min():.0f} - {df_work['year'].max():.0f})\")\n",
    "else:\n",
    "    year_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.5 PUBLISHER - TF-IDF\n",
    "if 'publisher' in df_work.columns:\n",
    "    print(\"🏢 Processando publishers...\")\n",
    "    publisher_vectorizer = TfidfVectorizer(\n",
    "        max_features=30,\n",
    "        min_df=3,\n",
    "        lowercase=True\n",
    "    )\n",
    "    publisher_features = publisher_vectorizer.fit_transform(df_work['publisher'].astype(str))\n",
    "    print(f\"✅ Publisher: {publisher_features.shape[1]} features\")\n",
    "else:\n",
    "    publisher_features = np.zeros((len(df_work), 1))\n",
    "\n",
    "# 4.6 COMBINAR TODAS AS FEATURES\n",
    "print(\"\\n🔗 Combinando features...\")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Converter arrays densos para sparse\n",
    "language_sparse = csr_matrix(language_features)\n",
    "year_sparse = csr_matrix(year_features)\n",
    "\n",
    "# Combinar com pesos\n",
    "features_combined = hstack([\n",
    "    title_features * 3.0,        # Peso maior para título\n",
    "    author_features * 2.0,       # Peso médio-alto para autor\n",
    "    language_sparse * 1.0,       # Peso normal para língua\n",
    "    year_sparse * 0.5,           # Peso menor para ano\n",
    "    publisher_features * 0.5     # Peso menor para publisher\n",
    "])\n",
    "\n",
    "print(f\"✅ Features combinadas: {features_combined.shape}\")\n",
    "print(f\"   Total de features: {features_combined.shape[1]}\")\n",
    "\n",
    "# Converter para array denso para clustering\n",
    "X = features_combined.toarray()\n",
    "\n",
    "# ============================================================\n",
    "# 5. REDUÇÃO DE DIMENSIONALIDADE (PCA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📉 REDUÇÃO DE DIMENSIONALIDADE (PCA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# PCA para visualização (2D e 3D)\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X)\n",
    "print(f\"✅ PCA 2D: variância explicada = {pca_2d.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "print(f\"✅ PCA 3D: variância explicada = {pca_3d.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# PCA para clustering (manter 95% da variância)\n",
    "pca_full = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca_full.fit_transform(X)\n",
    "print(f\"✅ PCA clustering: {X_pca.shape[1]} componentes (95% variância)\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. DETERMINAÇÃO DO NÚMERO IDEAL DE CLUSTERS (ELBOW METHOD)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 DETERMINAÇÃO DO NÚMERO IDEAL DE CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n🔍 Testando diferentes números de clusters (K-Means)...\")\n",
    "k_range = range(3, 15)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "db_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_pca, labels))\n",
    "    db_scores.append(davies_bouldin_score(X_pca, labels))\n",
    "\n",
    "    print(f\"   K={k:2d} | Inércia: {kmeans.inertia_:,.0f} | Silhouette: {silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "# Plotar curvas de avaliação\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Número de Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inércia', fontsize=12)\n",
    "axes[0].set_title('Método do Cotovelo (Elbow Method)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score\n",
    "axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Número de Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score por K', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Threshold 0.5')\n",
    "axes[1].legend()\n",
    "\n",
    "# Davies-Bouldin score (menor é melhor)\n",
    "axes[2].plot(k_range, db_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Número de Clusters (K)', fontsize=12)\n",
    "axes[2].set_ylabel('Davies-Bouldin Score', fontsize=12)\n",
    "axes[2].set_title('Davies-Bouldin Score por K', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✅ Gráfico salvo: {OUTPUT_DIR / 'elbow_analysis.png'}\")\n",
    "\n",
    "# Sugerir melhor K\n",
    "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "best_k_db = k_range[np.argmin(db_scores)]\n",
    "print(f\"\\n💡 Sugestões de K:\")\n",
    "print(f\"   Melhor Silhouette Score: K={best_k_silhouette}\")\n",
    "print(f\"   Melhor Davies-Bouldin: K={best_k_db}\")\n",
    "\n",
    "# Usar K sugerido\n",
    "optimal_k = best_k_silhouette\n",
    "print(f\"\\n✅ K escolhido para análise: {optimal_k}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. APLICAR ALGORITMOS DE CLUSTERING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🤖 APLICANDO ALGORITMOS DE CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 7.1 K-MEANS\n",
    "print(f\"\\n1️⃣ K-Means (K={optimal_k})...\")\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "df_work['cluster_kmeans'] = kmeans.fit_predict(X_pca)\n",
    "silhouette_kmeans = silhouette_score(X_pca, df_work['cluster_kmeans'])\n",
    "db_kmeans = davies_bouldin_score(X_pca, df_work['cluster_kmeans'])\n",
    "ch_kmeans = calinski_harabasz_score(X_pca, df_work['cluster_kmeans'])\n",
    "\n",
    "print(f\"   ✅ Silhouette Score: {silhouette_kmeans:.3f}\")\n",
    "print(f\"   ✅ Davies-Bouldin Score: {db_kmeans:.3f}\")\n",
    "print(f\"   ✅ Calinski-Harabasz Score: {ch_kmeans:.1f}\")\n",
    "\n",
    "# 7.2 DBSCAN\n",
    "print(f\"\\n2️⃣ DBSCAN...\")\n",
    "dbscan = DBSCAN(eps=2.0, min_samples=5, metric='euclidean')\n",
    "df_work['cluster_dbscan'] = dbscan.fit_predict(X_pca)\n",
    "n_clusters_dbscan = len(set(df_work['cluster_dbscan'])) - (1 if -1 in df_work['cluster_dbscan'] else 0)\n",
    "n_noise_dbscan = list(df_work['cluster_dbscan']).count(-1)\n",
    "\n",
    "print(f\"   ✅ Clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"   ✅ Ruído (outliers): {n_noise_dbscan} livros ({n_noise_dbscan/len(df_work)*100:.1f}%)\")\n",
    "\n",
    "if n_clusters_dbscan > 1:\n",
    "    mask = df_work['cluster_dbscan'] != -1\n",
    "    silhouette_dbscan = silhouette_score(X_pca[mask], df_work[mask]['cluster_dbscan'])\n",
    "    print(f\"   ✅ Silhouette Score: {silhouette_dbscan:.3f}\")\n",
    "\n",
    "# 7.3 HIERARCHICAL CLUSTERING\n",
    "print(f\"\\n3️⃣ Hierarchical Clustering (K={optimal_k})...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "df_work['cluster_hierarchical'] = hierarchical.fit_predict(X_pca)\n",
    "silhouette_hier = silhouette_score(X_pca, df_work['cluster_hierarchical'])\n",
    "db_hier = davies_bouldin_score(X_pca, df_work['cluster_hierarchical'])\n",
    "ch_hier = calinski_harabasz_score(X_pca, df_work['cluster_hierarchical'])\n",
    "\n",
    "print(f\"   ✅ Silhouette Score: {silhouette_hier:.3f}\")\n",
    "print(f\"   ✅ Davies-Bouldin Score: {db_hier:.3f}\")\n",
    "print(f\"   ✅ Calinski-Harabasz Score: {ch_hier:.1f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. ANÁLISE DOS CLUSTERS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 ANÁLISE DOS CLUSTERS (K-MEANS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Estatísticas por cluster\n",
    "print(f\"\\n📈 Distribuição de livros por cluster:\")\n",
    "cluster_counts = df_work['cluster_kmeans'].value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    pct = count / len(df_work) * 100\n",
    "    print(f\"   Cluster {cluster}: {count:,} livros ({pct:.1f}%)\")\n",
    "\n",
    "# Características de cada cluster\n",
    "print(f\"\\n🔍 Características dos Clusters:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "\n",
    "    print(f\"\\n🏷️  CLUSTER {cluster} ({len(cluster_data):,} livros)\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "\n",
    "    # Top autores\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_authors = cluster_data['author'].value_counts().head(3)\n",
    "        print(f\"   📚 Top autores:\")\n",
    "        for author, count in top_authors.items():\n",
    "            author_display = author[:40] + \"...\" if len(str(author)) > 40 else author\n",
    "            print(f\"      • {author_display}: {count} livros\")\n",
    "\n",
    "    # Línguas\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_langs = cluster_data['language'].value_counts().head(3)\n",
    "        print(f\"   🌍 Línguas:\")\n",
    "        for lang, count in top_langs.items():\n",
    "            print(f\"      • {lang}: {count} livros ({count/len(cluster_data)*100:.1f}%)\")\n",
    "\n",
    "    # Anos\n",
    "    if 'year' in cluster_data.columns:\n",
    "        print(f\"   📅 Anos: {cluster_data['year'].min():.0f} - {cluster_data['year'].max():.0f}\")\n",
    "        print(f\"      Média: {cluster_data['year'].mean():.1f}\")\n",
    "\n",
    "    # Top publishers\n",
    "    if 'publisher' in cluster_data.columns:\n",
    "        top_pubs = cluster_data['publisher'].value_counts().head(3)\n",
    "        print(f\"   🏢 Top publishers:\")\n",
    "        for pub, count in top_pubs.items():\n",
    "            pub_display = pub[:40] + \"...\" if len(str(pub)) > 40 else pub\n",
    "            print(f\"      • {pub_display}: {count} livros\")\n",
    "\n",
    "    # Exemplos de títulos\n",
    "    print(f\"   📖 Exemplos de títulos:\")\n",
    "    sample_titles = cluster_data['title'].sample(min(3, len(cluster_data)), random_state=42)\n",
    "    for title in sample_titles:\n",
    "        title_display = title[:60] + \"...\" if len(str(title)) > 60 else title\n",
    "        print(f\"      • {title_display}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. VISUALIZAÇÕES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📊 GERANDO VISUALIZAÇÕES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 9.1 Scatter plot 2D - K-Means\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_kmeans'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 0].set_title(f'K-Means Clustering (K={optimal_k})', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster')\n",
    "\n",
    "# DBSCAN\n",
    "scatter2 = axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_dbscan'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 1].set_title(f'DBSCAN Clustering ({n_clusters_dbscan} clusters)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster')\n",
    "\n",
    "# Hierarchical\n",
    "scatter3 = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],\n",
    "                               c=df_work['cluster_hierarchical'],\n",
    "                               cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1, 0].set_title(f'Hierarchical Clustering (K={optimal_k})', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Componente Principal 1', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Componente Principal 2', fontsize=11)\n",
    "plt.colorbar(scatter3, ax=axes[1, 0], label='Cluster')\n",
    "\n",
    "# Distribuição de tamanhos dos clusters\n",
    "cluster_sizes = df_work['cluster_kmeans'].value_counts().sort_index()\n",
    "axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values, color='steelblue', alpha=0.7)\n",
    "axes[1, 1].set_title('Distribuição de Livros por Cluster (K-Means)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Cluster', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Número de Livros', fontsize=11)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i, v in enumerate(cluster_sizes.values):\n",
    "    axes[1, 1].text(i, v + 50, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'clustering_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✅ Visualização principal salva: clustering_visualization.png\")\n",
    "\n",
    "# 9.2 Visualização 3D (K-Means)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],\n",
    "                     c=df_work['cluster_kmeans'], cmap='tab10', alpha=0.6, s=20)\n",
    "\n",
    "ax.set_title(f'K-Means Clustering 3D (K={optimal_k})', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('PC1', fontsize=11, labelpad=10)\n",
    "ax.set_ylabel('PC2', fontsize=11, labelpad=10)\n",
    "ax.set_zlabel('PC3', fontsize=11, labelpad=10)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster', pad=0.1)\n",
    "plt.savefig(OUTPUT_DIR / 'clustering_3d.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✅ Visualização 3D salva: clustering_3d.png\")\n",
    "\n",
    "# ============================================================\n",
    "# 10. EXPORTAR RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"💾 EXPORTANDO RESULTADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Juntar com dados originais\n",
    "df_clustered = df_work.copy()\n",
    "\n",
    "# Exportar dataset com clusters\n",
    "output_file = OUTPUT_DIR / 'books_clustered.csv'\n",
    "df_clustered.to_csv(output_file, index=False)\n",
    "print(f\"\\n✅ Dataset com clusters: {output_file}\")\n",
    "print(f\"   {len(df_clustered):,} livros | {len(df_clustered.columns)} colunas\")\n",
    "\n",
    "# Exportar resumo dos clusters\n",
    "cluster_summary = []\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "\n",
    "    summary = {\n",
    "        'cluster': cluster,\n",
    "        'size': len(cluster_data),\n",
    "        'percentage': len(cluster_data) / len(df_work) * 100,\n",
    "        'top_author': cluster_data['author'].value_counts().index[0] if 'author' in cluster_data else 'N/A',\n",
    "        'top_language': cluster_data['language'].value_counts().index[0] if 'language' in cluster_data else 'N/A',\n",
    "        'avg_year': cluster_data['year'].mean() if 'year' in cluster_data else None,\n",
    "        'year_range': f\"{cluster_data['year'].min():.0f}-{cluster_data['year'].max():.0f}\" if 'year' in cluster_data else 'N/A'\n",
    "    }\n",
    "    cluster_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "summary_file = OUTPUT_DIR / 'cluster_summary.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"✅ Resumo dos clusters: {summary_file}\")\n",
    "\n",
    "# Exportar métricas de avaliação\n",
    "metrics = {\n",
    "    'Algorithm': ['K-Means', 'DBSCAN', 'Hierarchical'],\n",
    "    'N_Clusters': [optimal_k, n_clusters_dbscan, optimal_k],\n",
    "    'Silhouette_Score': [\n",
    "        silhouette_kmeans,\n",
    "        silhouette_dbscan if n_clusters_dbscan > 1 else np.nan,\n",
    "        silhouette_hier\n",
    "    ],\n",
    "    'Davies_Bouldin_Score': [db_kmeans, np.nan, db_hier],\n",
    "    'Calinski_Harabasz_Score': [ch_kmeans, np.nan, ch_hier]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_file = OUTPUT_DIR / 'clustering_metrics.csv'\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"✅ Métricas de avaliação: {metrics_file}\")\n",
    "\n",
    "# Exportar exemplos de cada cluster\n",
    "print(f\"\\n📚 Exportando exemplos de cada cluster...\")\n",
    "for cluster in sorted(df_work['cluster_kmeans'].unique()):\n",
    "    cluster_data = df_work[df_work['cluster_kmeans'] == cluster]\n",
    "    sample = cluster_data.sample(min(20, len(cluster_data)), random_state=42)\n",
    "\n",
    "    sample_file = OUTPUT_DIR / f'cluster_{cluster}_examples.csv'\n",
    "    sample.to_csv(sample_file, index=False)\n",
    "    print(f\"   Cluster {cluster}: {len(sample)} exemplos salvos\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ POC CONCLUÍDA COM SUCESSO!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n📊 Resultados:\")\n",
    "print(f\"   • Melhor algoritmo: K-Means\")\n",
    "print(f\"   • Número de clusters: {optimal_k}\")\n",
    "print(f\"   • Silhouette Score: {silhouette_kmeans:.3f}\")\n",
    "print(f\"   • Livros clusterizados: {len(df_clustered):,}\")\n",
    "print(f\"\\n📁 Todos os arquivos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\n💡 Próximos passos:\")\n",
    "print(f\"   1. Analisar os clusters gerados\")\n",
    "print(f\"   2. Refinar os parâmetros se necessário\")\n",
    "print(f\"   3. Usar os clusters para recomendação de livros\")"
   ],
   "id": "69bdb36efc2c2bfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🤖 POC: CLUSTERING DE LIVROS SIMILARES\n",
      "======================================================================\n",
      "📂 Dataset: exports\\clean_data/book_data_clean.csv\n",
      "📁 Saída: exports\\clustering\n",
      "\n",
      "📂 Carregando dataset limpo...\n",
      "✅ Dataset carregado: 55,502 livros\n",
      "   Colunas: 19\n",
      "\n",
      "======================================================================\n",
      "🔧 PREPARAÇÃO DOS DADOS\n",
      "======================================================================\n",
      "✅ title: 'name'\n",
      "✅ author: 'authors'\n",
      "✅ language: 'language'\n",
      "✅ year: 'publishyear'\n",
      "✅ publisher: 'publisher'\n",
      "\n",
      "📊 Valores nulos antes da limpeza:\n",
      "   language: 36,500 (65.8%)\n",
      "   publisher: 453 (0.8%)\n",
      "\n",
      "✅ Após remover nulos em título/autor: 55,502 livros\n",
      "✅ Dataset final para clustering: 55,502 livros\n",
      "\n",
      "======================================================================\n",
      "🔨 ENGENHARIA DE FEATURES\n",
      "======================================================================\n",
      "\n",
      "📝 Processando títulos com TF-IDF...\n",
      "✅ Título: 100 features\n",
      "👤 Processando autores com TF-IDF...\n",
      "✅ Autor: 50 features\n",
      "🌍 Processando línguas...\n",
      "✅ Língua: 36 features (36 idiomas)\n",
      "📅 Processando anos...\n",
      "✅ Ano: normalizado (162 - 2020)\n",
      "🏢 Processando publishers...\n",
      "✅ Publisher: 30 features\n",
      "\n",
      "🔗 Combinando features...\n",
      "✅ Features combinadas: (55502, 217)\n",
      "   Total de features: 217\n",
      "\n",
      "======================================================================\n",
      "📉 REDUÇÃO DE DIMENSIONALIDADE (PCA)\n",
      "======================================================================\n",
      "✅ PCA 2D: variância explicada = 15.09%\n",
      "✅ PCA 3D: variância explicada = 20.04%\n",
      "✅ PCA clustering: 132 componentes (95% variância)\n",
      "\n",
      "======================================================================\n",
      "📊 DETERMINAÇÃO DO NÚMERO IDEAL DE CLUSTERS\n",
      "======================================================================\n",
      "\n",
      "🔍 Testando diferentes números de clusters (K-Means)...\n",
      "   K= 3 | Inércia: 409,689 | Silhouette: 0.097\n",
      "   K= 4 | Inércia: 393,638 | Silhouette: 0.106\n",
      "   K= 5 | Inércia: 386,904 | Silhouette: 0.106\n",
      "   K= 6 | Inércia: 371,580 | Silhouette: 0.097\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Análise de Clusters",
   "id": "6f70c54efb8dd2ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Gera tabela interativa para análise rápida dos clusters\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURAÇÃO\n",
    "# ============================================================\n",
    "\n",
    "INPUT_DIR = Path(\"exports/clustering\")\n",
    "OUTPUT_DIR = Path(\"exports/clustering/analysis\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"📊 TABELA DE CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================\n",
    "# 2. CARREGAR DADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📂 Carregando dados...\")\n",
    "clustered_file = INPUT_DIR / \"books_clustered.csv\"\n",
    "\n",
    "if not clustered_file.exists():\n",
    "    print(f\"❌ Erro: Arquivo não encontrado: {clustered_file}\")\n",
    "    raise FileNotFoundError(f\"Execute o clustering primeiro!\")\n",
    "\n",
    "df = pd.read_csv(clustered_file)\n",
    "print(f\"✅ {len(df):,} livros carregados\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. CRIAR TABELA RESUMO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n🔨 Criando tabela resumo...\")\n",
    "\n",
    "cluster_summary = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    # Informações básicas\n",
    "    summary = {\n",
    "        'Cluster': cluster_id,\n",
    "        'Tamanho': len(cluster_data),\n",
    "        '% Total': f\"{len(cluster_data)/len(df)*100:.1f}%\"\n",
    "    }\n",
    "\n",
    "    # Top Autor\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_author = cluster_data['author'].value_counts()\n",
    "        summary['Top Autor'] = top_author.index[0][:40]\n",
    "        summary['Autor (%)'] = f\"{top_author.iloc[0]/len(cluster_data)*100:.0f}%\"\n",
    "        summary['Autores Únicos'] = cluster_data['author'].nunique()\n",
    "\n",
    "    # Língua dominante\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_lang = cluster_data['language'].value_counts()\n",
    "        summary['Língua'] = top_lang.index[0]\n",
    "        summary['Língua (%)'] = f\"{top_lang.iloc[0]/len(cluster_data)*100:.0f}%\"\n",
    "\n",
    "    # Anos\n",
    "    if 'year' in cluster_data.columns:\n",
    "        summary['Ano Médio'] = f\"{cluster_data['year'].mean():.0f}\"\n",
    "        summary['Período'] = f\"{cluster_data['year'].min():.0f}-{cluster_data['year'].max():.0f}\"\n",
    "\n",
    "    # Publisher\n",
    "    if 'publisher' in cluster_data.columns:\n",
    "        top_pub = cluster_data['publisher'].value_counts()\n",
    "        summary['Top Publisher'] = top_pub.index[0][:30]\n",
    "\n",
    "    # Exemplo de título\n",
    "    if 'title' in cluster_data.columns:\n",
    "        example_title = cluster_data['title'].iloc[0]\n",
    "        summary['Exemplo'] = example_title[:50] + \"...\" if len(example_title) > 50 else example_title\n",
    "\n",
    "    cluster_summary.append(summary)\n",
    "\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "\n",
    "# ============================================================\n",
    "# 4. EXIBIR TABELA NO TERMINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"📋 RESUMO DOS CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Exibir com pandas (formatado)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# ============================================================\n",
    "# 5. CRIAR HTML INTERATIVO COM TABELA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📄 Gerando tabela HTML interativa...\")\n",
    "\n",
    "html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Tabela de Clusters - Goodreads</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 20px;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 1400px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            border-radius: 15px;\n",
    "            box-shadow: 0 10px 40px rgba(0,0,0,0.3);\n",
    "            overflow: hidden;\n",
    "        }}\n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .header h1 {{\n",
    "            margin: 0;\n",
    "            font-size: 2.5em;\n",
    "        }}\n",
    "        .stats {{\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-bottom: 2px solid #667eea;\n",
    "        }}\n",
    "        .stat-box {{\n",
    "            text-align: center;\n",
    "            padding: 15px;\n",
    "        }}\n",
    "        .stat-number {{\n",
    "            font-size: 2em;\n",
    "            font-weight: bold;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "        .stat-label {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "            margin-top: 5px;\n",
    "        }}\n",
    "        .table-container {{\n",
    "            padding: 20px;\n",
    "            overflow-x: auto;\n",
    "        }}\n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        thead {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            position: sticky;\n",
    "            top: 0;\n",
    "            z-index: 10;\n",
    "        }}\n",
    "        th {{\n",
    "            padding: 15px;\n",
    "            text-align: left;\n",
    "            font-weight: 600;\n",
    "            font-size: 0.9em;\n",
    "            text-transform: uppercase;\n",
    "            letter-spacing: 0.5px;\n",
    "        }}\n",
    "        td {{\n",
    "            padding: 12px 15px;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }}\n",
    "        tr:hover {{\n",
    "            background-color: #f5f5ff;\n",
    "            cursor: pointer;\n",
    "        }}\n",
    "        .cluster-id {{\n",
    "            font-weight: bold;\n",
    "            font-size: 1.2em;\n",
    "            color: #667eea;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .size {{\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .percentage {{\n",
    "            color: #666;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        .author {{\n",
    "            color: #333;\n",
    "            font-weight: 500;\n",
    "        }}\n",
    "        .language {{\n",
    "            display: inline-block;\n",
    "            background: #667eea;\n",
    "            color: white;\n",
    "            padding: 4px 10px;\n",
    "            border-radius: 12px;\n",
    "            font-size: 0.85em;\n",
    "        }}\n",
    "        .year-range {{\n",
    "            color: #666;\n",
    "            font-family: monospace;\n",
    "        }}\n",
    "        .example {{\n",
    "            color: #555;\n",
    "            font-style: italic;\n",
    "            font-size: 0.9em;\n",
    "        }}\n",
    "        .filter-box {{\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-bottom: 1px solid #e0e0e0;\n",
    "        }}\n",
    "        .filter-box input {{\n",
    "            width: 100%;\n",
    "            padding: 12px;\n",
    "            font-size: 1em;\n",
    "            border: 2px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            transition: border-color 0.3s;\n",
    "        }}\n",
    "        .filter-box input:focus {{\n",
    "            outline: none;\n",
    "            border-color: #667eea;\n",
    "        }}\n",
    "        .badge {{\n",
    "            display: inline-block;\n",
    "            padding: 3px 8px;\n",
    "            border-radius: 10px;\n",
    "            font-size: 0.8em;\n",
    "            font-weight: 600;\n",
    "            margin-right: 5px;\n",
    "        }}\n",
    "        .badge-large {{\n",
    "            background: #ff6b6b;\n",
    "            color: white;\n",
    "        }}\n",
    "        .badge-medium {{\n",
    "            background: #ffd93d;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .badge-small {{\n",
    "            background: #6bcf7f;\n",
    "            color: white;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>📚 Tabela de Clusters - Goodreads</h1>\n",
    "            <p>Análise visual dos clusters gerados</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"stats\">\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{len(df):,}</div>\n",
    "                <div class=\"stat-label\">Total de Livros</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{len(summary_df)}</div>\n",
    "                <div class=\"stat-label\">Clusters</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{int(summary_df['Tamanho'].mean())}</div>\n",
    "                <div class=\"stat-label\">Média por Cluster</div>\n",
    "            </div>\n",
    "            <div class=\"stat-box\">\n",
    "                <div class=\"stat-number\">{summary_df['Tamanho'].max()}</div>\n",
    "                <div class=\"stat-label\">Maior Cluster</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"filter-box\">\n",
    "            <input type=\"text\" id=\"searchInput\" placeholder=\"🔍 Buscar por autor, língua, publisher...\"\n",
    "                   onkeyup=\"filterTable()\">\n",
    "        </div>\n",
    "\n",
    "        <div class=\"table-container\">\n",
    "            <table id=\"clusterTable\">\n",
    "                <thead>\n",
    "                    <tr>\n",
    "                        <th>Cluster</th>\n",
    "                        <th>Tamanho</th>\n",
    "                        <th>Top Autor</th>\n",
    "                        <th>Conc. Autor</th>\n",
    "                        <th>Autores Únicos</th>\n",
    "                        <th>Língua</th>\n",
    "                        <th>Ano Médio</th>\n",
    "                        <th>Período</th>\n",
    "                        <th>Top Publisher</th>\n",
    "                        <th>Exemplo de Título</th>\n",
    "                    </tr>\n",
    "                </thead>\n",
    "                <tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Adicionar linhas da tabela\n",
    "for _, row in summary_df.iterrows():\n",
    "    # Determinar badge de tamanho\n",
    "    size = row['Tamanho']\n",
    "    if size > summary_df['Tamanho'].quantile(0.75):\n",
    "        badge_class = 'badge-large'\n",
    "        badge_text = 'Grande'\n",
    "    elif size > summary_df['Tamanho'].quantile(0.25):\n",
    "        badge_class = 'badge-medium'\n",
    "        badge_text = 'Médio'\n",
    "    else:\n",
    "        badge_class = 'badge-small'\n",
    "        badge_text = 'Pequeno'\n",
    "\n",
    "    html += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td class=\"cluster-id\">{row['Cluster']}</td>\n",
    "                        <td class=\"size\">\n",
    "                            {row['Tamanho']:,}\n",
    "                            <span class=\"badge {badge_class}\">{badge_text}</span>\n",
    "                            <br>\n",
    "                            <span class=\"percentage\">{row['% Total']}</span>\n",
    "                        </td>\n",
    "                        <td class=\"author\">{row.get('Top Autor', 'N/A')}</td>\n",
    "                        <td>{row.get('Autor (%)', 'N/A')}</td>\n",
    "                        <td>{row.get('Autores Únicos', 'N/A')}</td>\n",
    "                        <td><span class=\"language\">{row.get('Língua', 'N/A')}</span> {row.get('Língua (%)', '')}</td>\n",
    "                        <td class=\"year-range\">{row.get('Ano Médio', 'N/A')}</td>\n",
    "                        <td class=\"year-range\">{row.get('Período', 'N/A')}</td>\n",
    "                        <td>{row.get('Top Publisher', 'N/A')}</td>\n",
    "                        <td class=\"example\">{row.get('Exemplo', 'N/A')}</td>\n",
    "                    </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html += \"\"\"\n",
    "                </tbody>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function filterTable() {\n",
    "            const input = document.getElementById('searchInput');\n",
    "            const filter = input.value.toUpperCase();\n",
    "            const table = document.getElementById('clusterTable');\n",
    "            const tr = table.getElementsByTagName('tr');\n",
    "\n",
    "            for (let i = 1; i < tr.length; i++) {\n",
    "                const row = tr[i];\n",
    "                const text = row.textContent || row.innerText;\n",
    "\n",
    "                if (text.toUpperCase().indexOf(filter) > -1) {\n",
    "                    row.style.display = '';\n",
    "                } else {\n",
    "                    row.style.display = 'none';\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Adicionar click para destacar linha\n",
    "        const rows = document.querySelectorAll('tbody tr');\n",
    "        rows.forEach(row => {\n",
    "            row.addEventListener('click', function() {\n",
    "                rows.forEach(r => r.style.backgroundColor = '');\n",
    "                this.style.backgroundColor = '#e8eaf6';\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Salvar HTML\n",
    "html_file = OUTPUT_DIR / \"cluster_table.html\"\n",
    "with open(html_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(html)\n",
    "\n",
    "print(f\"✅ Tabela HTML gerada: {html_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. CRIAR TABELA DETALHADA COM EXEMPLOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📋 Gerando tabela detalhada com exemplos...\")\n",
    "\n",
    "detailed_data = []\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    # Pegar 5 exemplos de livros\n",
    "    examples = cluster_data.sample(min(5, len(cluster_data)), random_state=42)\n",
    "\n",
    "    for idx, (_, book) in enumerate(examples.iterrows()):\n",
    "        row = {\n",
    "            'Cluster': cluster_id,\n",
    "            'Título': book.get('title', 'N/A'),\n",
    "            'Autor': book.get('author', 'N/A'),\n",
    "            'Ano': int(book.get('year', 0)) if pd.notna(book.get('year')) else 'N/A',\n",
    "            'Língua': book.get('language', 'N/A'),\n",
    "            'Publisher': book.get('publisher', 'N/A')\n",
    "        }\n",
    "        detailed_data.append(row)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_data)\n",
    "\n",
    "# Salvar CSV detalhado\n",
    "csv_file = OUTPUT_DIR / \"cluster_examples_table.csv\"\n",
    "detailed_df.to_csv(csv_file, index=False)\n",
    "print(f\"✅ Tabela CSV detalhada: {csv_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7. CRIAR VISUALIZAÇÃO SIMPLIFICADA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n📊 Gerando visualização simplificada...\")\n",
    "\n",
    "simple_html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Clusters Simplificado</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: Arial, sans-serif;\n",
    "            margin: 20px;\n",
    "            background: #f5f5f5;\n",
    "        }}\n",
    "        .cluster-card {{\n",
    "            background: white;\n",
    "            border-radius: 10px;\n",
    "            padding: 20px;\n",
    "            margin-bottom: 15px;\n",
    "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
    "            border-left: 5px solid #667eea;\n",
    "        }}\n",
    "        .cluster-header {{\n",
    "            font-size: 1.5em;\n",
    "            font-weight: bold;\n",
    "            color: #667eea;\n",
    "            margin-bottom: 15px;\n",
    "        }}\n",
    "        .info-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 10px;\n",
    "            margin-bottom: 15px;\n",
    "        }}\n",
    "        .info-item {{\n",
    "            background: #f8f9fa;\n",
    "            padding: 10px;\n",
    "            border-radius: 5px;\n",
    "        }}\n",
    "        .info-label {{\n",
    "            font-weight: bold;\n",
    "            color: #555;\n",
    "            font-size: 0.85em;\n",
    "            margin-bottom: 5px;\n",
    "        }}\n",
    "        .info-value {{\n",
    "            color: #333;\n",
    "            font-size: 1.1em;\n",
    "        }}\n",
    "        .books-sample {{\n",
    "            background: #f0f0f0;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin-top: 10px;\n",
    "        }}\n",
    "        .book-item {{\n",
    "            padding: 8px;\n",
    "            margin: 5px 0;\n",
    "            background: white;\n",
    "            border-radius: 3px;\n",
    "        }}\n",
    "        h1 {{\n",
    "            text-align: center;\n",
    "            color: #667eea;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>📚 Clusters de Livros - Visão Simplificada</h1>\n",
    "    <p style=\"text-align: center; color: #666;\">Total: {len(df):,} livros em {len(summary_df)} clusters</p>\n",
    "\"\"\"\n",
    "\n",
    "for cluster_id in sorted(df['cluster_kmeans'].unique()):\n",
    "    cluster_data = df[df['cluster_kmeans'] == cluster_id]\n",
    "\n",
    "    simple_html += f\"\"\"\n",
    "    <div class=\"cluster-card\">\n",
    "        <div class=\"cluster-header\">\n",
    "            Cluster {cluster_id} - {len(cluster_data):,} livros ({len(cluster_data)/len(df)*100:.1f}%)\n",
    "        </div>\n",
    "\n",
    "        <div class=\"info-grid\">\n",
    "    \"\"\"\n",
    "\n",
    "    if 'author' in cluster_data.columns:\n",
    "        top_author = cluster_data['author'].value_counts().iloc[0]\n",
    "        top_author_name = cluster_data['author'].value_counts().index[0]\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">Top Autor</div>\n",
    "                <div class=\"info-value\">{top_author_name[:30]}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">({top_author} livros)</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    if 'language' in cluster_data.columns:\n",
    "        top_lang = cluster_data['language'].value_counts()\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">Língua Principal</div>\n",
    "                <div class=\"info-value\">{top_lang.index[0]}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">({top_lang.iloc[0]/len(cluster_data)*100:.0f}%)</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    if 'year' in cluster_data.columns:\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"info-item\">\n",
    "                <div class=\"info-label\">Período</div>\n",
    "                <div class=\"info-value\">{cluster_data['year'].min():.0f} - {cluster_data['year'].max():.0f}</div>\n",
    "                <div style=\"font-size: 0.85em; color: #666;\">Média: {cluster_data['year'].mean():.0f}</div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    simple_html += \"\"\"\n",
    "        </div>\n",
    "\n",
    "        <div class=\"books-sample\">\n",
    "            <strong>📖 Exemplos de livros:</strong>\n",
    "    \"\"\"\n",
    "\n",
    "    samples = cluster_data.sample(min(3, len(cluster_data)), random_state=42)\n",
    "    for _, book in samples.iterrows():\n",
    "        title = str(book.get('title', 'N/A'))[:60]\n",
    "        author = str(book.get('author', 'N/A'))[:30]\n",
    "        simple_html += f\"\"\"\n",
    "            <div class=\"book-item\">\n",
    "                <strong>{title}</strong> - {author}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "\n",
    "    simple_html += \"\"\"\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "simple_html += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "simple_file = OUTPUT_DIR / \"cluster_simple.html\"\n",
    "with open(simple_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(simple_html)\n",
    "\n",
    "print(f\"✅ Visualização simplificada: {simple_file}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8. SALVAR CSV RESUMO\n",
    "# ============================================================\n",
    "\n",
    "csv_summary = OUTPUT_DIR / \"cluster_summary_table.csv\"\n",
    "summary_df.to_csv(csv_summary, index=False)\n",
    "print(f\"✅ CSV resumo: {csv_summary}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9. RESUMO FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✅ TABELAS GERADAS COM SUCESSO!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n📁 Arquivos em: {OUTPUT_DIR.absolute()}\")\n",
    "print(f\"\\n📄 Arquivos gerados:\")\n",
    "print(f\"   1. cluster_table.html              - Tabela interativa completa\")\n",
    "print(f\"   2. cluster_simple.html             - Visão simplificada em cards\")\n",
    "print(f\"   3. cluster_summary_table.csv       - Resumo em CSV\")\n",
    "print(f\"   4. cluster_examples_table.csv      - Exemplos detalhados\")\n",
    "\n",
    "print(f\"\\n💡 Como usar:\")\n",
    "print(f\"   • Abra cluster_table.html no navegador\")\n",
    "print(f\"   • Use a busca para filtrar clusters\")\n",
    "print(f\"   • Clique nas linhas para destacar\")\n",
    "print(f\"   • Abra cluster_simple.html para visão rápida\")\n",
    "\n",
    "print(f\"\\n🎯 O que observar:\")\n",
    "print(f\"   ✓ Concentração de autores (clusters de séries?)\")\n",
    "print(f\"   ✓ Homogeneidade de língua (mercados regionais?)\")\n",
    "print(f\"   ✓ Períodos temporais (tendências de época?)\")\n",
    "print(f\"   ✓ Publishers dominantes (nichos editoriais?)\")"
   ],
   "id": "87794f01ecd3ff7f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
